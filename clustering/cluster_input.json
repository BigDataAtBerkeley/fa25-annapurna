{
  "region": "us-east-1",
  "endpoint": "search-research-papers-uv3fxq76j5bkxq3bgp3nyfdtnm.us-east-1.es.amazonaws.com",
  "index": "research-papers-v2",
  "cluster_health": {
    "cluster_name": "478852001205:research-papers",
    "status": "yellow",
    "timed_out": false,
    "number_of_nodes": 1,
    "number_of_data_nodes": 1,
    "discovered_master": true,
    "discovered_cluster_manager": true,
    "active_primary_shards": 15,
    "active_shards": 15,
    "relocating_shards": 0,
    "initializing_shards": 0,
    "unassigned_shards": 2,
    "delayed_unassigned_shards": 0,
    "number_of_pending_tasks": 0,
    "number_of_in_flight_fetch": 0,
    "task_max_waiting_in_queue_millis": 0,
    "active_shards_percent_as_number": 88.23529411764706
  },
  "indices": [
    {
      "health": "green",
      "status": "open",
      "index": ".plugins-ml-config",
      "uuid": "h3d7FB3RS9WqMZRr0ZufbA",
      "pri": "5",
      "rep": "0",
      "docs.count": "1",
      "docs.deleted": "0",
      "store.size": "4.7kb",
      "pri.store.size": "4.7kb"
    },
    {
      "health": "green",
      "status": "open",
      "index": ".opensearch-observability",
      "uuid": "9wJ0EgcbRv2P1ewJEKu1HA",
      "pri": "1",
      "rep": "0",
      "docs.count": "0",
      "docs.deleted": "0",
      "store.size": "208b",
      "pri.store.size": "208b"
    },
    {
      "health": "yellow",
      "status": "open",
      "index": "research-papers-v2",
      "uuid": "4fuYvVkCTruVC3UY4tbZoA",
      "pri": "1",
      "rep": "1",
      "docs.count": "114",
      "docs.deleted": "0",
      "store.size": "417.4kb",
      "pri.store.size": "417.4kb"
    },
    {
      "health": "green",
      "status": "open",
      "index": ".kibana_1",
      "uuid": "SqZ2iGLfRgO8ReHDMuzcxQ",
      "pri": "1",
      "rep": "0",
      "docs.count": "1",
      "docs.deleted": "0",
      "store.size": "5.2kb",
      "pri.store.size": "5.2kb"
    },
    {
      "health": "green",
      "status": "open",
      "index": ".opendistro_security",
      "uuid": "rmSWdJLkSQqlSTjzeIU4hQ",
      "pri": "1",
      "rep": "0",
      "docs.count": "10",
      "docs.deleted": "1",
      "store.size": "59.5kb",
      "pri.store.size": "59.5kb"
    },
    {
      "health": "yellow",
      "status": "open",
      "index": "research-papers",
      "uuid": "3WpdaCAuQ1CuizI7Bn_Diw",
      "pri": "1",
      "rep": "1",
      "docs.count": "2",
      "docs.deleted": "0",
      "store.size": "21.9kb",
      "pri.store.size": "21.9kb"
    }
  ],
  "documents": [
    {
      "_id": "5-j63JkBP8oloYi_6iI5",
      "title": "A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers",
      "title_normalized": "a_bringyourownmodel_approach_for_mldriven_storage_placement_in_warehousescale_computers",
      "authors": [
        "Chenxi Yang",
        "Yan Li",
        "Martin Maas",
        "Mustafa Uysal",
        "Ubaid Ullah Hafeez",
        "Arif Merchant",
        "Richard McDougall"
      ],
      "abstract": "Storage systems account for a major portion of the total cost of ownership (TCO) of warehouse-scale computers, and thus have a major impact on the overall system's efficiency. Machine learning (ML)-based methods for solving key problems in storage system efficiency, such as data placement, have shown significant promise. However, there are few known practical deployments of such methods. Studying this problem in the context of real-world hyperscale data centers at Google, we identify a number of challenges that we believe cause this lack of practical adoption. Specifically, prior work assumes a monolithic model that resides entirely within the storage layer, an unrealistic assumption in real-world deployments with frequently changing workloads.\nTo address this problem, we introduce a cross-layer approach where workloads instead “bring their own model”. This strategy moves ML out of the storage system and instead allows each workload to train its own lightweight model at the application layer, capturing the workload's specific characteristics. These small, interpretable models generate predictions that guide a co-designed scheduling heuristic at the storage layer, enabling adaptation to diverse online environments. We build a proof-of-concept of this approach in a production distributed computation framework at Google. Evaluations in a test deployment and large-scale simulation studies using production traces show improvements of as much as 3.47$\\times$ in TCO savings compared to state-of-the-art baselines.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "A_Bring-Your-Own-Model_Approach_for_ML-Driven_Storage_Placement_in_Warehouse-Scale_Computers.pdf",
      "sha_abstract": "bf9511f19e5a6fd931c96c1ffff997bb304f069df8a0cefef37222b88c715547",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349055467
    },
    {
      "_id": "6uj63JkBP8oloYi_7yKD",
      "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking",
      "title_normalized": "efficient_llm_inference_using_dynamic_input_pruning_and_cacheaware_masking",
      "authors": [
        "Marco Federici",
        "Davide Belli",
        "Mart Van Baalen",
        "Amir Jalalirad",
        "Andrii Skliar",
        "Bence Major",
        "Markus Nagel",
        "Paul Whatmough"
      ],
      "abstract": "While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower.\nThis is unfortunate for large language model (LLM) token generation, which is heavily memory-bound.\nPrevious work has proposed to leverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce effective DRAM bandwidth per token.\nHowever, more recent LLMs use SwiGLU instead of ReLU, which results in little inherent sparsity. While SwiGLU activations can be pruned based on magnitude, the resulting sparsity patterns are difficult to predict, rendering previous approaches ineffective.\nTo circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a predictor-free dynamic sparsification approach,\nwhich preserves accuracy with minimal fine-tuning.\nDIP can further use lightweight LoRA adapters to regain some performance lost during sparsification. \nLastly, we describe a novel cache-aware masking strategy, which considers the cache state and activation magnitude to further increase cache hit rate, improving LLM token rate on mobile devices.\nDIP outperforms other methods in terms of accuracy, memory and throughput trade-offs across simulated hardware settings. \nOn Phi-3-Medium, DIP achieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$ 0.1 loss in perplexity when compared to streaming the dense model from Flash.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Efficient_LLM_Inference_using_Dynamic_Input_Pruning_and_Cache-Aware_Masking.pdf",
      "sha_abstract": "881c1d27cfc3142e037681e3bfc71978c3e58425e0f83139e25e4493b2239a0d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349056841
    },
    {
      "_id": "4ujn3JkBP8oloYi_ySIt",
      "title": "Universal Image Restoration Pre-training via Degradation Classification",
      "title_normalized": "universal_image_restoration_pretraining_via_degradation_classification",
      "authors": [
        "JiaKui Hu",
        "Lujia Jin",
        "Zhengjian Yao",
        "Yanye Lu"
      ],
      "abstract": "This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal image restoration pre-training. Unlike the existing self-supervised pre-training methods, DCPT utilizes the degradation type of the input image as an extremely weak supervision, which can be effortlessly obtained, even intrinsic in all image restoration datasets. DCPT comprises two primary stages. Initially, image features are extracted from the encoder. Subsequently, a lightweight decoder, such as ResNet18, is leveraged to classify the degradation type of the input image solely based on the features extracted in the first stage, without utilizing the input image. The encoder is pre-trained with a straightforward yet potent DCPT, which is used to address universal image restoration and achieve outstanding performance. Following DCPT, both convolutional neural networks (CNNs) and transformers demonstrate performance improvements, with gains of up to 2.55 dB in the 10D all-in-one restoration task and 6.53 dB in the mixed degradation scenarios. Moreover, previous self-supervised pretraining methods, such as masked image modeling, discard the decoder after pre-training, while our DCPT utilizes the pre-trained parameters more effectively. This superiority arises from the degradation classifier acquired during DCPT, which facilitates transfer learning between models of identical architecture trained on diverse degradation types. Source code and models are available at \\url{https://github.com/MILab-PKU/dcpt}.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Universal_Image_Restoration_Pre-training_via_Degradation_Classification.pdf",
      "sha_abstract": "0ffa4f4e1c77c9dfa3cd11b517bf448061d610b00c411635b5103a0d6b234c86",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760347801489
    },
    {
      "_id": "7Oj63JkBP8oloYi_8yJ4",
      "title": "Youmu: Efficient Columnar Data Pipeline for LLM Training",
      "title_normalized": "youmu_efficient_columnar_data_pipeline_for_llm_training",
      "authors": [
        "Tianle Zhong",
        "Jiechen Zhao",
        "Qiang Su",
        "Geoffrey Fox"
      ],
      "abstract": "Large language models (LLMs) training is extremely data-intensive, often involving over trillion-level tokens. \nAlthough LLM datasets are usually ingested and stored in columnar formats, they often need to be converted into another format for training, which incurs significant storage and maintenance costs due to extra data copies. \nWhile eliminating the conversion would save tens of terabytes of space in costly high performance storage, this work identifies challenges that drive us to re-think the entire data pipeline. \nWithout conversion, we find that fine-grained random access patterns incur hundreds of times efficiency drops.\nSpecifically, the existing data pipelines have two fundamental drawbacks: \n(1) They cannot efficiently support directly digesting data in columnar format due to default coarse-grained I/O; \n(2) Solutions to the first drawback sacrifice memory footprint to cache datasets. \nIn this paper, we present Youmu, a new data pipeline that directly feeds fine-grained columnar data into GPUs, enabling cost-efficient LLM training. \nMeanwhile, Youmu maintains high training accuracy, whose perplexity outperforms widely adopted local shuffle by reducing 0.3-0.7 for pretraining. \nCompared to performance-optimal state-of-the-art, distributed memory-based pipelines, Youmu achieves comparable throughput with $\\sim$80\\% less memory footprint.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Youmu__Efficient_Columnar_Data_Pipeline_for_LLM_Training.pdf",
      "sha_abstract": "19f1ca9a849fedd415423d4e220bae1b9dcce30ad7633cdcebba66b67ec6dfaa",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349057878
    },
    {
      "_id": "5ujn3JkBP8oloYi_2CLD",
      "title": "Framer: Interactive Frame Interpolation",
      "title_normalized": "framer_interactive_frame_interpolation",
      "authors": [
        "Wen Wang",
        "Qiuyu Wang",
        "Kecheng Zheng",
        "Hao OUYANG",
        "Zhekai Chen",
        "Biao Gong",
        "Hao Chen",
        "Yujun Shen",
        "Chunhua Shen"
      ],
      "abstract": "We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an \"autopilot\" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, model, and interface are publicly accessible at https://github.com/aim-uofa/Framer.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Framer__Interactive_Frame_Interpolation.pdf",
      "sha_abstract": "79f7cda9d3e369c5f40cc1f1859cffb263308a12d1f02167e673f07e665edb37",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760347805863
    },
    {
      "_id": "5Ojn3JkBP8oloYi_1yJI",
      "title": "Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks",
      "title_normalized": "porthamiltonian_architectural_bias_for_longrange_propagation_in_deep_graph_networks",
      "authors": [
        "Simon Heilig",
        "Alessio Gravina",
        "Alessandro Trenta",
        "Claudio Gallicchio",
        "Davide Bacciu"
      ],
      "abstract": "The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce port-Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Port-Hamiltonian_Architectural_Bias_for_Long-Range_Propagation_in_Deep_Graph_Networks.pdf",
      "sha_abstract": "d9b47761e54c4140622ea15eb058b1db477fc309e9ef0d92b3597e3d7e1f5b0d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760347805468
    },
    {
      "_id": "4-jn3JkBP8oloYi_1SKS",
      "title": "On the Benefits of Attribute-Driven Graph Domain Adaptation",
      "title_normalized": "on_the_benefits_of_attributedriven_graph_domain_adaptation",
      "authors": [
        "Ruiyi Fang",
        "Bingheng Li",
        "zhao kang",
        "Qiuhao Zeng",
        "Nima Hosseini Dashtbayaz",
        "Ruizhi Pu",
        "Boyu Wang",
        "Charles Ling"
      ],
      "abstract": "Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. \nSpecifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "On_the_Benefits_of_Attribute-Driven_Graph_Domain_Adaptation.pdf",
      "sha_abstract": "32639fb627fececb4968bd2121248d5f9edebfae77eceb9bca62788b1b5a51ef",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760347804934
    },
    {
      "_id": "6ej63JkBP8oloYi_7SI_",
      "title": "LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
      "title_normalized": "leanattention_hardwareaware_scalable_attention_mechanism_for_the_decodephase_of_transformers",
      "authors": [
        "Rya Sanovar",
        "Srikant Bharadwaj",
        "Renée St. Amant",
        "Victor Rühle",
        "Saravan Rajmohan"
      ],
      "abstract": "Transformer-based large language models are memory hungry and incur significant inference latencies even\non cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and output tokens.\n\n\nTo that end, we propose LeanAttention, a scalable, hardware-efficient, “exact” attention acceleration mechanism\nfor the decode-phase of transformer-based models. LeanAttention enables scaling the attention mechanism for the\nchallenging case of long context lengths by re-designing the attention execution flow for the decode-phase. As a\nresult, we achieve an average of 1.73x speedup in attention execution compared to FlashDecoding, with up to\n2.18x speedup for 256k context length.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "LeanAttention__Hardware-Aware_Scalable_Attention_Mechanism_for_the_Decode-Phase_of_Transformers.pdf",
      "sha_abstract": "bce8f527099e9a9f82c6832c295d59a1e25171275fff81d03620e1bd343b28aa",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349056283
    },
    {
      "_id": "6-j63JkBP8oloYi_8CJH",
      "title": "TurboAttention: Efficient attention approximation for high throughputs llm",
      "title_normalized": "turboattention_efficient_attention_approximation_for_high_throughputs_llm",
      "authors": [
        "Hao Kang",
        "Srikant Bharadwaj",
        "James Hensman",
        "Tushar Krishna",
        "Victor Rühle",
        "Saravan Rajmohan"
      ],
      "abstract": "Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanisms. While techniques, such as quantization, and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operations.\n\nWe present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "TurboAttention__Efficient_attention_approximation_for_high_throughputs_llm.pdf",
      "sha_abstract": "447ea6f576d309fbb7f10acdf3b3d9dcb6e353ac72d4acb2696878824c89b50f",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349057067,
      "code_generated": true,
      "code_metadata_s3_key": "6-j63JkBP8oloYi_8CJH/metadata.json",
      "code_s3_key": "6-j63JkBP8oloYi_8CJH/code.py",
      "code_s3_bucket": "papers-code-artifacts",
      "code_generated_at": "2025-10-20T04:26:13.335164",
      "test_results": {
        "error_message": "Unknown error",
        "tested_at": "2025-10-20T04:26:13.753372",
        "stdout_s3_key": "6-j63JkBP8oloYi_8CJH/outputs/stdout.log",
        "tested": true,
        "success": false,
        "error_type": "unknown",
        "stdout_s3_bucket": "papers-test-outputs",
        "return_code": 1,
        "artifacts_s3_prefix": "6-j63JkBP8oloYi_8CJH/outputs/",
        "timeout": false,
        "execution_time": 0.061788,
        "stderr_s3_key": "6-j63JkBP8oloYi_8CJH/outputs/stderr.log"
      }
    },
    {
      "_id": "5ejn3JkBP8oloYi_2CIm",
      "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal",
      "title_normalized": "sorrybench_systematically_evaluating_large_language_model_safety_refusal",
      "authors": [
        "Tinghao Xie",
        "Xiangyu Qi",
        "Yi Zeng",
        "Yangsibo Huang",
        "Udari Madhushani Sehwag",
        "Kaixuan Huang",
        "Luxi He",
        "Boyi Wei",
        "Dacheng Li",
        "Ying Sheng",
        "Ruoxi Jia",
        "Bo Li",
        "Kai Li",
        "Danqi Chen",
        "Peter Henderson",
        "Prateek Mittal"
      ],
      "abstract": "Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more --- which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through [https://sorry-bench.github.io](https://sorry-bench.github.io).",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "SORRY-Bench__Systematically_Evaluating_Large_Language_Model_Safety_Refusal.pdf",
      "sha_abstract": "1132bdc951c96fa4b830c6af27985f86b76112fa1a30d90fe1911cc4c157e5ad",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760347805699,
      "code_generated": true,
      "code_metadata_s3_key": "5ejn3JkBP8oloYi_2CIm/metadata.json",
      "code_s3_key": "5ejn3JkBP8oloYi_2CIm/code.py",
      "code_s3_bucket": "papers-code-artifacts",
      "code_generated_at": "2025-10-20T06:46:45.183935"
    },
    {
      "_id": "6Oj63JkBP8oloYi_7CKj",
      "title": "Scaling Deep Learning Training with MPMD Pipeline Parallelism",
      "title_normalized": "scaling_deep_learning_training_with_mpmd_pipeline_parallelism",
      "authors": [
        "Anxhelo Xhebraj",
        "Sean Lee",
        "Hanfeng Chen",
        "Vinod Grover"
      ],
      "abstract": "We present JaxPP, a system for efficiently scaling the training of large deep learning\nmodels with flexible pipeline parallelism.\nWe introduce a seamless programming model that allows implementing user-defined pipeline\nschedules for gradient accumulation.\nJaxPP automatically distributes tasks, corresponding to pipeline stages, over\na cluster of nodes and automatically infers the communication among them.\nWe implement a MPMD runtime for asynchronous execution of SPMD tasks.\nThe pipeline parallelism implementation of JaxPP improves hardware utilization by up\nto $1.16\\times$ with respect to the best performing SPMD configuration.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Scaling_Deep_Learning_Training_with_MPMD_Pipeline_Parallelism.pdf",
      "sha_abstract": "eec5d2528108c4622c0104fdfc4742906eedd22e95ca57e905a52aabe55f375d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349056122,
      "code_generated": true,
      "code_metadata_s3_key": "6Oj63JkBP8oloYi_7CKj/metadata.json",
      "code_s3_key": "6Oj63JkBP8oloYi_7CKj/code.py",
      "code_s3_bucket": "papers-code-artifacts",
      "code_generated_at": "2025-10-20T06:53:03.654567"
    },
    {
      "_id": "7ej63JkBP8oloYi_9iKE",
      "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
      "title_normalized": "selfdata_distillation_for_recovering_quality_in_pruned_large_language_models",
      "authors": [
        "Vithursan Thangarasa",
        "Ganesh Venkatesh",
        "Mike Lasby",
        "Nish Sinnadurai",
        "Sean Lie"
      ],
      "abstract": "Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.30%. Furthermore, combining\nself-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Self-Data_Distillation_for_Recovering_Quality_in_Pruned_Large_Language_Models.pdf",
      "sha_abstract": "2370ca0c910e3bafb58ba2dd5e1a760fda1ea07ca040951f66aabd1547975ced",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349058668
    },
    {
      "_id": "8Oj73JkBP8oloYi_AiJV",
      "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine",
      "title_normalized": "adaparse_an_adaptive_parallel_pdf_parsing_and_resource_scaling_engine",
      "authors": [
        "Carlo Siebenschuh",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Arham Mushtaq Khan",
        "Khalid Hossain",
        "Yadu Babuji",
        "Nicholas Chia",
        "Venkatram Vishwanath",
        "Arvind Ramanathan",
        "Rick L. Stevens",
        "Ian Foster",
        "Robert Underwood"
      ],
      "abstract": "Language models for scientific tasks are trained on text from scientific publications---most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML‑driven systems (for complex or degraded ones). The choice of the ``best'' parser for a particular document depends on 1) its computational cost and 2) the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and (aligned) predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by 17$\\times$ while still achieving comparable accuracy (actually, 0.2\\% better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at \\url{https://github.com/7shoe/AdaParse/}.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "AdaParse__An_Adaptive_Parallel_PDF_Parsing_and_Resource_Scaling_Engine.pdf",
      "sha_abstract": "ed9925d65ae5198fd29febb4c3948661944222120467d53e05d377de17b18f0d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349061643
    },
    {
      "_id": "7-j63JkBP8oloYi__SJW",
      "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
      "title_normalized": "flstore_efficient_federated_learning_storage_for_nontraining_workloads",
      "authors": [
        "Ahmad Faraz Khan",
        "Samuel Fountain",
        "Ahmed M. Abdelmoniem",
        "Ali R. Butt",
        "Ali Anwar"
      ],
      "abstract": "Federated Learning (FL) is an approach for privacy-preserving Machine Learning (ML), enabling model training across multiple clients without centralized data collection. With an aggregator server coordinating training, aggregating model updates, and storing metadata across rounds. In addition to training, a substantial part of FL systems are the non-training workloads such as scheduling, personalization, clustering, debugging, and incentivization. Most existing systems rely on the aggregator to handle non-training workloads and use cloud services for data storage. This results in high latency and increased costs as non-training workloads rely on large volumes of metadata, including weight parameters from client updates, hyperparameters, and aggregated updates across rounds, making the situation even worse. \nWe propose FLStore, a serverless framework for efficient FL non-training workloads and storage. FLStore unifies the data and compute planes on a serverless cache, enabling locality-aware execution via tailored caching policies to reduce latency and costs. Per our evaluations, compared to cloud object store based aggregator server FLStore reduces per request average latency by $71$% and costs by $92.45$%, with peak improvements of $99.7$% and $98.8$%, respectively. Compared to an in-memory cloud cache based aggregator server, FLStore reduces average latency by $64.6$% and costs by $98.83$%, with peak improvements of $98.8$% and $99.6$%, respectively. FLStore integrates seamlessly with existing FL frameworks with minimal modifications, while also being fault-tolerant and highly scalable.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "FLStore__Efficient_Federated_Learning_Storage_for_non-training_workloads.pdf",
      "sha_abstract": "e68e9525dbb8f27ea5f0da576db7691fc2066ee7856a06183b232f6edeb5173a",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349060396
    },
    {
      "_id": "EeiaD5oBclM7MZc3c4-d",
      "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
      "title_normalized": "aligned_better_listen_better_for_audiovisual_large_language_models",
      "authors": [
        "Yuxin Guo",
        "Shuailei Ma",
        "Shijie Ma",
        "Xiaoyi Bao",
        "Chen-Wei Xie",
        "Kecheng Zheng",
        "Tingyu Weng",
        "Siyang Sun",
        "Yun Zheng",
        "Wei Zou"
      ],
      "abstract": "Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption \\& instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Aligned_Better__Listen_Better_for_Audio-Visual_Large_Language_Models.pdf",
      "sha_abstract": "aea160d31c518a004e227ae3e8b13321f328b817bd3feeb19d57c6b24724e83c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198371712
    },
    {
      "_id": "7uj63JkBP8oloYi_-CJp",
      "title": "Efficient On-Device Machine Learning with a Biologically-Plausible Forward-Only Algorithm",
      "title_normalized": "efficient_ondevice_machine_learning_with_a_biologicallyplausible_forwardonly_algorithm",
      "authors": [
        "Baichuan Huang",
        "Amir Aminifar"
      ],
      "abstract": "The training of the state-of-the-art Deep Neural Networks (DNNs) consumes massive amounts of energy, while the human brain learns new tasks with remarkable efficiency. Currently, the training of DNNs relies almost exclusively on Backpropagation (BP). However, BP faces criticism due to its biologically implausible nature, underscoring the significant disparity in performance and energy efficiency between DNNs and the human brain. Forward-only algorithms are proposed to be the biologically plausible alternatives to BP, to better mimic the learning process of the human brain and enhance energy efficiency. In this paper, we propose a biologically-plausible forward-only algorithm (Bio-FO), not only targeting the biological-implausibility issues associated with BP, but also outperforming the state-of-the-art forward-only algorithms. We extensively evaluate our proposed Bio-FO against other forward-only algorithms and demonstrate its performance across diverse datasets, including two real-world medical applications on wearable devices with limited resources and relatively large-scale datasets such as mini-ImageNet. At the same time, we implement our proposed on-device learning algorithm on the NVIDIA Jetson Nano and demonstrate its efficiency compared to other state-of-the-art forward-only algorithms. The code is available at https://github.com/whubaichuan/Bio-FO.",
      "date": "2025-05-13",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Efficient_On-Device_Machine_Learning_with_a_Biologically-Plausible_Forward-Only_Algorithm.pdf",
      "sha_abstract": "10abdb4683382d0511f8612c87b923727ad29216b074ba58a86b9166b23cd3df",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1760349059137
    },
    {
      "_id": "EOiaD5oBclM7MZc3bo84",
      "title": "CTSyn: A Foundation Model for Cross Tabular Data Generation",
      "title_normalized": "ctsyn_a_foundation_model_for_cross_tabular_data_generation",
      "authors": [
        "Xiaofeng Lin",
        "Chenheng Xu",
        "Matthew Yang",
        "Guang Cheng"
      ],
      "abstract": "Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity.  These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "CTSyn__A_Foundation_Model_for_Cross_Tabular_Data_Generation.pdf",
      "sha_abstract": "0e3f1b904ae346acd8fce51a3ae0d89546340f4927c6fa95bf92b06fe5b012d3",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198370330
    },
    {
      "_id": "MeikD5oBclM7MZc3Zo_A",
      "title": "Bridging the Data Provenance Gap Across Text, Speech, and Video",
      "title_normalized": "bridging_the_data_provenance_gap_across_text_speech_and_video",
      "authors": [
        "Shayne Longpre",
        "Nikhil Singh",
        "Manuel Cherep",
        "Kushagra Tiwary",
        "Joanna Materzynska",
        "William Brannon",
        "Robert Mahari",
        "Naana Obeng-Marnu",
        "Manan Dey",
        "Mohammed Hamdy",
        "Nayan Saxena",
        "Ahmad Mustafa Anis",
        "Emad A. Alghamdi",
        "Vu Minh Chien",
        "Da Yin",
        "Kun Qian",
        "Yizhi LI",
        "Minnie Liang",
        "An Dinh",
        "Shrestha Mohanty",
        "Deividas Mataciunas",
        "Tobin South",
        "Jianguo Zhang",
        "Ariel N. Lee",
        "Campbell S. Lund",
        "Christopher Klamm",
        "Damien Sileo",
        "Diganta Misra",
        "Enrico Shippole",
        "Kevin Klyman",
        "Lester James Validad Miranda",
        "Niklas Muennighoff",
        "Seonghyeon Ye",
        "Seungone Kim",
        "Vipul Gupta",
        "Vivek Sharma",
        "Xuhui Zhou",
        "Caiming Xiong",
        "Luis Villa",
        "Stella Biderman",
        "Alex Pentland",
        "Sara Hooker",
        "Jad Kabbara"
      ],
      "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities --- popular text, speech, and video datasets --- from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Bridging_the_Data_Provenance_Gap_Across_Text__Speech__and_Video.pdf",
      "sha_abstract": "5b99f498a593207ff7578c6e1ca895af9d6fde5f1b679116650aa44c357d16f4",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199023791
    },
    {
      "_id": "LeikD5oBclM7MZc3Uo8Y",
      "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation",
      "title_normalized": "large_language_models_meet_symbolic_provers_for_logical_reasoning_evaluation",
      "authors": [
        "Chengwen Qi",
        "Ren Ma",
        "Bowen Li",
        "He Du",
        "Binyuan Hui",
        "Jinwang Wu",
        "Yuanjun Laili",
        "Conghui He"
      ],
      "abstract": "First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework.\nThe finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: \\url{https://github.com/opendatalab/ProverGen}",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Large_Language_Models_Meet_Symbolic_Provers_for_Logical_Reasoning_Evaluation.pdf",
      "sha_abstract": "b644c34c9c0e4766fc8f7b20535638636e876085f3ad3e0f2db94e88c30a7b7e",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199018510
    },
    {
      "_id": "NeikD5oBclM7MZc3e49F",
      "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
      "title_normalized": "agenttrek_agent_trajectory_synthesis_via_guiding_replay_with_web_tutorials",
      "authors": [
        "Yiheng Xu",
        "Dunjie Lu",
        "Zhennan Shen",
        "Junli Wang",
        "Zekun Wang",
        "Yuchen Mao",
        "Caiming Xiong",
        "Tao Yu"
      ],
      "abstract": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "AgentTrek__Agent_Trajectory_Synthesis_via_Guiding_Replay_with_Web_Tutorials.pdf",
      "sha_abstract": "56be0f23c7ad987c4a2e633f8d54477e9ccb8f319e4c95ddbd6721f34c5ca9f3",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199029031
    },
    {
      "_id": "MOikD5oBclM7MZc3Yo88",
      "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs",
      "title_normalized": "codemmlu_a_multitask_benchmark_for_assessing_code_understanding__reasoning_capabilities_of_codellms",
      "authors": [
        "Dung Manh Nguyen",
        "Thang Chau Phan",
        "Nam Le Hai",
        "Tien-Thong Doan",
        "Nam V. Nguyen",
        "Quang Pham",
        "Nghi D. Q. Bui"
      ],
      "abstract": "Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding & reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth of software and code comprehension in LLMs. CodeMMLU includes nearly 20,000 questions spanning diverse domains, including code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks that emphasize code generation, CodeMMLU assesses a model’s ability to reason about programs across a wide-range of tasks such as code repair, execution reasoning, and fill-in-the-blank challenges. Our extensive evaluation reveals that even state-of-the-art models struggle with CodeMMLU, highlighting significant gaps in comprehension beyond generation. By emphasizing the essential connection between code understanding and effective AI-assisted development, CodeMMLU provides a critical resource for advancing more reliable and capable coding assistants.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "CodeMMLU__A_Multi-Task_Benchmark_for_Assessing_Code_Understanding___Reasoning_Capabilities_of_CodeLLMs.pdf",
      "sha_abstract": "7c84a259afc65efa679d9499ff6a900f1586d356dd47469613a886e159221043",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199022632
    },
    {
      "_id": "L-ikD5oBclM7MZc3XY-7",
      "title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors",
      "title_normalized": "dittotts_diffusion_transformers_for_scalable_texttospeech_without_domainspecific_factors",
      "authors": [
        "Keon Lee",
        "Dong Won Kim",
        "Jaehyeon Kim",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "abstract": "Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors, performance remains suboptimal. In this work, we introduce DiTTo-TTS, a Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based TTS can achieve state-of-the-art performance without domain-specific factors. Through rigorous analysis and empirical exploration, we find that (1) DiT with minimal modifications outperforms U-Net, (2) variable-length modeling with a speech length predictor significantly improves results over fixed-length approaches, and (3) conditions like semantic alignment in speech latent representations are key to further enhancement. By scaling our training data to 82K hours and the model size to 790M parameters, we achieve superior or comparable zero-shot performance to state-of-the-art TTS models in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific factors. Speech samples are available at https://ditto-tts.github.io.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "DiTTo-TTS__Diffusion_Transformers_for_Scalable_Text-to-Speech_without_Domain-Specific_Factors.pdf",
      "sha_abstract": "73fd9e3269f52eca5ac6eca6d24e70d4e8dec5781900c25a45bf24bc58a4a45d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199021417
    },
    {
      "_id": "LuikD5oBclM7MZc3Vo9h",
      "title": "Learning mirror maps in policy mirror descent",
      "title_normalized": "learning_mirror_maps_in_policy_mirror_descent",
      "authors": [
        "Carlo Alfano",
        "Sebastian Rene Towers",
        "Silvia Sapora",
        "Chris Lu",
        "Patrick Rebeschini"
      ],
      "abstract": "Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e.\\ Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Learning_mirror_maps_in_policy_mirror_descent.pdf",
      "sha_abstract": "46b3d71542c2cb31f9802e0047593be6abe7594059d25028a51bca0493ee2e29",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199019587
    },
    {
      "_id": "M-ikD5oBclM7MZc3cY-R",
      "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness",
      "title_normalized": "improved_diffusionbased_generative_model_with_better_adversarial_robustness",
      "authors": [
        "Zekun Wang",
        "Mingyang Yi",
        "Shuchen Xue",
        "Zhenguo Li",
        "Ming Liu",
        "Bing Qin",
        "Zhi-Ming Ma"
      ],
      "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Improved_Diffusion-based_Generative_Model_with_Better_Adversarial_Robustness.pdf",
      "sha_abstract": "ea82003bd7cecc49176a65db9f5b12ca2995cbc9664ce440bf8c5aee0ea62660",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199026352
    },
    {
      "_id": "NOikD5oBclM7MZc3do-f",
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "title_normalized": "generative_classifiers_avoid_shortcut_solutions",
      "authors": [
        "Alexander Cong Li",
        "Ananya Kumar",
        "Deepak Pathak"
      ],
      "abstract": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Generative_Classifiers_Avoid_Shortcut_Solutions.pdf",
      "sha_abstract": "03648b7031675eca7d21544ee7e7ad61790aa0104fc114dda073cb5fb81638ad",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199027844
    },
    {
      "_id": "MuikD5oBclM7MZc3bI8S",
      "title": "Decoupled Subgraph Federated Learning",
      "title_normalized": "decoupled_subgraph_federated_learning",
      "authors": [
        "Javad Aliakbari",
        "Johan Östman",
        "Alexandre Graell i Amat"
      ],
      "abstract": "We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Decoupled_Subgraph_Federated_Learning.pdf",
      "sha_abstract": "e0204095f49e5e036d3c2ef9ffc0589a9eb5e57700f223442f462211d7494a0c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199025141
    },
    {
      "_id": "RuikD5oBclM7MZc3z4_z",
      "title": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View",
      "title_normalized": "understanding_warmupstabledecay_learning_rates_a_river_valley_loss_landscape_view",
      "authors": [
        "Kaiyue Wen",
        "Zhiyuan Li",
        "Jason S. Wang",
        "David Leo Wright Hall",
        "Percy Liang",
        "Tengyu Ma"
      ],
      "abstract": "Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a constant learning rate to produce a main branch of iterates that can in principle continue indefinitely without a pre-specified compute budget. Then, given any compute budget, one can branch out from the main branch at a proper time with a rapidly decaying learning rate to produce a strong model. Empirically, WSD generates an intriguing, non-traditional loss curve: the loss remains elevated during the stable phase but sharply declines during the decay phase. Towards explaining this phenomenon, we conjecture that pretraining loss exhibits a river valley landscape, which resembles a deep valley with a river at its bottom. Under this assumption, we show that during the stable phase, the iterate undergoes large oscillations due to the high learning rate, yet it progresses swiftly along the river. During the decay phase, the rapidly dropping learning rate minimizes the iterate’s oscillations, moving it closer to the river and revealing true optimization progress. Therefore, the sustained high learning rate phase and fast decaying phase are responsible for progress in the river and the mountain directions, respectively, and are both critical. Our analysis predicts phenomenons consistent with empirical observations and shows that this landscape can naturally emerge from pretraining on a simple bi-gram dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that reuses previous checkpoints’ decay phases and keeps only one main branch, where we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and Cyclic-Cosine in obtaining multiple pretrained language model checkpoints across various compute budgets in a single run for parameters scaling from 0.1B to 1.2B.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Understanding_Warmup-Stable-Decay_Learning_Rates__A_River_Valley_Loss_Landscape_View.pdf",
      "sha_abstract": "b2804e5eb0d77fef57d645fb89c3e16a56fa916cc6ca278d712859f3142691b3",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199050730
    },
    {
      "_id": "Q-ikD5oBclM7MZc3wo-_",
      "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization",
      "title_normalized": "achieving_dimensionfree_communication_in_federated_learning_via_zerothorder_optimization",
      "authors": [
        "Zhe Li",
        "Bicheng Ying",
        "Zidong Liu",
        "Chaosheng Dong",
        "Haibo Yang"
      ],
      "abstract": "Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. \nHowever, the substantial communication costs associated with FL significantly challenge its efficiency. \nSpecifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. \nDespite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations.\nThis paper proposes a novel dimension-free communication algorithm - DeComFL, which leverages the zeroth-order optimization techniques and reduces the communication cost from $\\mathcal{O}(d)$ to $\\mathcal{O}(1)$ by transmitting only a constant number of scalar values between clients and the server in each round, regardless of the dimension $d$ of the model parameters.\nTheoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions. With additional low effective rank assumption, we can further show that the convergence rate is independent of the model dimension $d$ as well.\nEmpirical evaluations, encompassing both classic deep learning training and large language model fine-tuning, demonstrate significant reductions in communication overhead. \nNotably, DeComFL achieves this by transmitting only around 1MB of data in total between the server and a client to fine-tune a model with billions of parameters. \nThe code is available at https://github.com/ZidongLiu/DeComFL.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Achieving_Dimension-Free_Communication_in_Federated_Learning_via_Zeroth-Order_Optimization.pdf",
      "sha_abstract": "5ecd56638a7d57fc993d0d2bce6f664dd7917b67b630c7f120846ae99fcc9cd1",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199047350
    },
    {
      "_id": "QOikD5oBclM7MZc3sY-1",
      "title": "Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping",
      "title_normalized": "nonconvex_stochastic_optimization_under_heavytailed_noises_optimal_convergence_without_gradient_clipping",
      "authors": [
        "Zijian Liu",
        "Zhengyuan Zhou"
      ],
      "abstract": "Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and true gradient) is considered to have only a finite $\\mathfrak{p}$-th moment where $\\mathfrak{p}\\in\\left(1,2\\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different algorithms and proved them to converge at an optimal $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{3\\mathfrak{p}-2}})$ rate for smooth objectives after $T$ iterations. Notably, all these new-designed algorithms are based on the same technique – gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, we prove that Batched NSGDM can achieve the optimal $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{3\\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{2\\mathfrak{p}}})$ convergence rate in the case where the tail index $\\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Nonconvex_Stochastic_Optimization_under_Heavy-Tailed_Noises__Optimal_Convergence_without_Gradient_Clipping.pdf",
      "sha_abstract": "ef8781698909539c5f457d98666ea16eaffd8d7ba8f0993016ee1423589ace66",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199042992
    },
    {
      "_id": "ReikD5oBclM7MZc3y49p",
      "title": "Towards Learning High-Precision Least Squares Algorithms with Sequence Models",
      "title_normalized": "towards_learning_highprecision_least_squares_algorithms_with_sequence_models",
      "authors": [
        "Jerry Weihong Liu",
        "Jessica Grogan",
        "Owen M Dugan",
        "Ashish Rao",
        "Simran Arora",
        "Atri Rudra",
        "Christopher Re"
      ],
      "abstract": "This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain solutions that are accurate to near floating point error, and (2) numerical generality, i.e. we want them to apply broadly across problem instances. We find that prior approaches using Transformers fail to meet these criteria, and identify limitations present in existing architectures and training procedures. First, we show that softmax Transformers struggle to perform high-precision multiplications, which prevents them from precisely learning numerical algorithms. Second, we identify an alternate class of architectures, comprised entirely of polynomials, that can efficiently represent high-precision gradient descent iterates. Finally, we investigate precision bottlenecks during training and address them via a high-precision training recipe that reduces stochastic gradient noise. Our recipe enables us to train two polynomial architectures, gated convolutions and linear attention, to perform gradient descent iterates on least squares problems. For the first time, we demonstrate the ability to train to near machine precision. Applied iteratively, our models obtain $100,000\\times$ lower MSE than standard Transformers trained end-to-end and they incur a $10,000\\times$ smaller generalization gap on out-of-distribution problems. We make progress towards end-to-end learning of numerical algorithms for least squares.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Towards_Learning_High-Precision_Least_Squares_Algorithms_with_Sequence_Models.pdf",
      "sha_abstract": "a0aacaa6150ff4c3864629affca9d59c7d3fcd0f37d4a48bf46efd5ef51d0945",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199049570
    },
    {
      "_id": "QeikD5oBclM7MZc3to-e",
      "title": "Large (Vision) Language Models are Unsupervised In-Context Learners",
      "title_normalized": "large_vision_language_models_are_unsupervised_incontext_learners",
      "authors": [
        "Artyom Gadetsky",
        "Andrei Atanov",
        "Yulun Jiang",
        "Zhitong Gao",
        "Ghazal Hosseini Mighan",
        "Amir Zamir",
        "Maria Brbic"
      ],
      "abstract": "Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various adaptation techniques such as prompt engineering, In-Context Learning (ICL), and supervised fine-tuning can further enhance the model’s performance on a downstream task, but they require substantial manual effort to construct effective prompts or labeled examples. In this work, we introduce a joint inference framework for fully unsupervised adaptation, eliminating the need for manual prompt engineering and labeled examples. Unlike zero-shot inference, which makes independent predictions, the joint inference makes predictions simultaneously for all inputs in a given task. Since direct joint inference involves computationally expensive optimization, we develop efficient approximation techniques, leading to two unsupervised adaptation methods: unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness of our methods across diverse tasks and models, including language-only Llama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math on grade school math problems, vision-language OpenFlamingo on vision tasks, and the API-only access GPT-4o model on massive multi-discipline tasks. Our experiments demonstrate substantial improvements over the standard zero-shot approach, including 39% absolute improvement on the challenging GSM8K math reasoning dataset. Remarkably, despite being fully unsupervised, our framework often performs on par with supervised approaches that rely on ground truth labels.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Large__Vision__Language_Models_are_Unsupervised_In-Context_Learners.pdf",
      "sha_abstract": "0f73661c7f6c590e011562660724046231dfa94d1e37e37cddc93dacf4b4526f",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199044236
    },
    {
      "_id": "ROikD5oBclM7MZc3xo-n",
      "title": "Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable",
      "title_normalized": "chainofthought_provably_enables_learning_the_otherwise_unlearnable",
      "authors": [
        "Chenxiao Yang",
        "Zhiyuan Li",
        "David Wipf"
      ],
      "abstract": "Modern language models have demonstrated remarkable reasoning capabilities by using chain-of-thought (CoT). One hypothesis about the inner workings of CoT is that it breaks down originally complex tasks into smaller subtasks that are more amenable to learning. We formalize this notion by showing possibility and impossibility results of learning from in-context demonstrations with and without CoT. In particular, with CoT, we examine a family of learning algorithms that learn a task step-by-step, capable of composing simpler functions from individual reasoning steps to form an overall complex function. This process reduces the difficulty of learning a task to that of the hardest reasoning step in the chain. Moreover, we prove Transformers can express this algorithm and thus they can efficiently in-context learn arbitrary tasks as long as these tasks can be decomposed into a finite number of subtasks, each of which are efficiently learnable. In contrast, without CoT, we demonstrate that there exist tasks that are inherently unlearnable by the same algorithm. Overall, our results suggest several provably effective ways for decomposing target problems to instantiate CoT. Empirically, we demonstrate our proposed CoT construction significantly enhances the reasoning capabilities of real-world LLMs in solving challenging arithmetic reasoning tasks, including learning polynomials and Boolean formulas.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Chain-of-Thought_Provably_Enables_Learning_the__Otherwise__Unlearnable.pdf",
      "sha_abstract": "bfd8f4faa8a534eac651d564ba4e8c06e23f674cc628d719487541d41f4e6e89",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199048350
    },
    {
      "_id": "QuikD5oBclM7MZc3vo9L",
      "title": "ThunderKittens: Simple, Fast, and $\\textit{Adorable}$ Kernels",
      "title_normalized": "thunderkittens_simple_fast_and_textitadorable_kernels",
      "authors": [
        "Benjamin Frederick Spector",
        "Simran Arora",
        "Aaryan Singhal",
        "Arjun Parthasarathy",
        "Daniel Y Fu",
        "Christopher Re"
      ],
      "abstract": "The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse capabilities of GPUs suggests we might we need a wide variety of techniques to achieve high performance. However, our work explores if a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like operations, (2) at the thread-block level, we provide templates for asynchronously overlapping operations, and (3) at the grid-level, TK helps hide block launch, tear-down, and memory costs. We show the value of TK by providing simple & diverse kernels that match or outperform prior art. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40$\\% on attention backwards, $8\\times$ on state space models, and $14\\times$ on linear attention.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "ThunderKittens__Simple__Fast__and___textit_Adorable___Kernels.pdf",
      "sha_abstract": "b51d3b3a548104606f5960672c19f35f3003ca696c500df8c77d196f1fed3abf",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199046210
    },
    {
      "_id": "P-ikD5oBclM7MZc3rY9_",
      "title": "Do LLMs estimate uncertainty well in instruction-following?",
      "title_normalized": "do_llms_estimate_uncertainty_well_in_instructionfollowing",
      "authors": [
        "Juyeon Heo",
        "Miao Xiong",
        "Christina Heinze-Deml",
        "Jaya Narain"
      ],
      "abstract": "Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. \nAccurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. \nOur study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models.\nTo address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions.\nOur findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. \nThe insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Do_LLMs_estimate_uncertainty_well_in_instruction-following_.pdf",
      "sha_abstract": "b76bd4af926c2820594d770badbcca82d202a958509c80a74e267c99e086fe7c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199041910
    },
    {
      "_id": "R-ikD5oBclM7MZc31I9k",
      "title": "nGPT: Normalized Transformer with Representation Learning on the Hypersphere",
      "title_normalized": "ngpt_normalized_transformer_with_representation_learning_on_the_hypersphere",
      "authors": [
        "Ilya Loshchilov",
        "Cheng-Ping Hsieh",
        "Simeng Sun",
        "Boris Ginsburg"
      ],
      "abstract": "We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "nGPT__Normalized_Transformer_with_Representation_Learning_on_the_Hypersphere.pdf",
      "sha_abstract": "9c456b374f8847aab6ebe1d4ef3e2027b4eee0cdf458dc29c93701a128c4c5d9",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199051870
    },
    {
      "_id": "T-ikD5oBclM7MZc3_o9s",
      "title": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars",
      "title_normalized": "surfhead_affine_rig_blending_for_geometrically_accurate_2d_gaussian_surfel_head_avatars",
      "authors": [
        "Jaeseong Lee",
        "Taewoong Kang",
        "Marcel Buehler",
        "Min-Jung Kim",
        "Sungwon Hwang",
        "Junha Hyung",
        "Hyojin Jang",
        "Jaegul Choo"
      ],
      "abstract": "Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "SurFhead__Affine_Rig_Blending_for_Geometrically_Accurate_2D_Gaussian_Surfel_Head_Avatars.pdf",
      "sha_abstract": "35510a1106bed9af79c5d43b4c845d70c609807d591f74717aa0757c90b1dc20",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199062630
    },
    {
      "_id": "SuikD5oBclM7MZc35I-p",
      "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
      "title_normalized": "reasoning_with_latent_thoughts_on_the_power_of_looped_transformers",
      "authors": [
        "Nikunj Saunshi",
        "Nishanth Dikkala",
        "Zhiyuan Li",
        "Sanjiv Kumar",
        "Sashank J. Reddi"
      ],
      "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Reasoning_with_Latent_Thoughts__On_the_Power_of_Looped_Transformers.pdf",
      "sha_abstract": "117b4a8d098d1d18e19701927e217ce52282f3b874faf73c968c79e3a9335147",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199056010
    },
    {
      "_id": "TuikD5oBclM7MZc3-Y-8",
      "title": "InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation",
      "title_normalized": "instantportrait_onestep_portrait_editing_via_diffusion_multiobjective_distillation",
      "authors": [
        "Zhixin Lai",
        "Keqiang Sun",
        "Fu-Yun Wang",
        "Dhritiman Sagar",
        "Erli Ding"
      ],
      "abstract": "Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc. However, real-time portrait editing presents three significant challenges: identity preservation, fidelity to editing instructions, and fast model inference. Given that these aspects often present a trade-off, concurrently addressing them poses an even greater challenge. While diffusion-based image editing methods have shown promising capabilities in personalized image editing in recent years, they lack a dedicated focus on portrait editing and thus suffer from the aforementioned problems as well. To address the gap, this paper introduces an Instant-Portrait Network (IPNet), the first one-step diffusion-based model for portrait editing. We train the network in two stages. We first employ an annealing identity loss to train an Identity Enhancement Network (IDE-Net), to ensure robust identity preservation. We then train the IPNet using a novel diffusion Multi-Objective Distillation approach that integrates adversarial loss, identity distillation loss, and a novel Facial-Style Enhancing loss. The Diffusion Multi-Objective Distillation approach efficiently reduces inference steps, ensures identity consistency, and enhances the precision of instruction-based editing. Extensive comparison with prior models demonstrates IPNet as a superior model in terms of identity preservation, text fidelity, and inference speed.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "InstantPortrait__One-Step_Portrait_Editing_via_Diffusion_Multi-Objective_Distillation.pdf",
      "sha_abstract": "a96a8d7ce5169f7c148e3963356c95e372419b327c7c667c395ace49f4ed295b",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199061430
    },
    {
      "_id": "UOilD5oBclM7MZc3BI92",
      "title": "MAP: Multi-Human-Value Alignment Palette",
      "title_normalized": "map_multihumanvalue_alignment_palette",
      "authors": [
        "Xinran Wang",
        "Qi Le",
        "Ammar Ahmed",
        "Enmao Diao",
        "Yi Zhou",
        "Nathalie Baracaldo",
        "Jie Ding",
        "Ali Anwar"
      ],
      "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "MAP__Multi-Human-Value_Alignment_Palette.pdf",
      "sha_abstract": "a65c1b15145123c0b86f1dbd80ffb6423b7733a839681a30f91c9c5deaebdce8",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199064170
    },
    {
      "_id": "S-ikD5oBclM7MZc36Y9Y",
      "title": "PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark",
      "title_normalized": "physpde_rethinking_pde_discovery_and_a_physical_hypothesis_selection_benchmark",
      "authors": [
        "Mingquan Feng",
        "Yixin Huang",
        "Yizhou Liu",
        "Bofang Jiang",
        "Junchi Yan"
      ],
      "abstract": "Despite extensive research, recovering PDE expressions from experimental observations often involves symbolic regression. This method generally lacks the incorporation of meaningful physical insights, resulting in outcomes lacking clear physical interpretations. Recognizing that the primary interest of Machine Learning for Science (ML4Sci) often lies in understanding the underlying physical mechanisms or even discovering new physical laws rather than simply obtaining mathematical expressions, this paper introduces a novel ML4Sci task paradigm. This paradigm focuses on interpreting experimental data within the framework of prior physical hypotheses and theories, thereby guiding and constraining the discovery of PDE expressions. We have formulated this approach as a nonlinear mixed-integer programming (MIP) problem, addressed through an efficient search scheme developed for this purpose. Our experiments on newly designed Fluid Mechanics and Laser Fusion datasets demonstrate the interpretability and feasibility of this method.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "PhysPDE__Rethinking_PDE_Discovery_and_a_Physical_HYpothesis_Selection_Benchmark.pdf",
      "sha_abstract": "33c054c02e54d51c654da4c978d0c79c569da73345e9225f21cf57f089d47316",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199057205
    },
    {
      "_id": "SeikD5oBclM7MZc33o9B",
      "title": "Homomorphism Counts as Structural Encodings for Graph Learning",
      "title_normalized": "homomorphism_counts_as_structural_encodings_for_graph_learning",
      "authors": [
        "Linus Bao",
        "Emily Jin",
        "Michael M. Bronstein",
        "Ismail Ilkan Ceylan",
        "Matthias Lanzinger"
      ],
      "abstract": "Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \\emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \\emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Homomorphism_Counts_as_Structural_Encodings_for_Graph_Learning.pdf",
      "sha_abstract": "aabb8fb183e66f16ec0357e532f2beb59779f15090a469d651cfcfca639d9150",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199054377
    },
    {
      "_id": "TOikD5oBclM7MZc37Y8v",
      "title": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity",
      "title_normalized": "adam_exploits_ellinftygeometry_of_loss_landscape_via_coordinatewise_adaptivity",
      "authors": [
        "Shuo Xie",
        "Mohamad Amin Mohamadi",
        "Zhiyuan Li"
      ],
      "abstract": "Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically --  previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry rather than the more common $\\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Adam_Exploits___ell__infty_-geometry_of_Loss_Landscape_via_Coordinate-wise_Adaptivity.pdf",
      "sha_abstract": "2882a305d1a330b405e2214343e56b0bf3d6e093dcac7a434d7f79d153a7c913",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199058213
    },
    {
      "_id": "TeikD5oBclM7MZc38Y_K",
      "title": "SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs",
      "title_normalized": "singer_stochastic_network_graph_evolving_operator_for_high_dimensional_pdes",
      "authors": [
        "Mingquan Feng",
        "Yixin Huang",
        "Weixin Liao",
        "Yuhong Liu",
        "Yizhou Liu",
        "Junchi Yan"
      ],
      "abstract": "We present a novel framework, StochastIc Network Graph Evolving operatoR (SINGER), for learning the evolution operator of high-dimensional partial differential equations (PDEs). The framework uses a sub-network to approximate the solution at the initial time step and stochastically evolves the sub-network parameters over time by a graph neural network to approximate the solution at later time steps. The framework is designed to inherit the desirable properties of the parametric solution operator, including graph topology, semigroup, and stability, with a theoretical guarantee. Numerical experiments on 8 evolution PDEs of 5,10,15,20-dimensions show that our method outperforms existing baselines in almost all cases (31 out of 32), and that our method generalizes well to unseen initial conditions, equation dimensions, sub-network width, and time steps.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "SINGER__Stochastic_Network_Graph_Evolving_Operator_for_High_Dimensional_PDEs.pdf",
      "sha_abstract": "b04c6c4799ed42a5aae0b811473dfe01f0a9bac59403e97dd062a9b07ba0afa5",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199059385
    },
    {
      "_id": "SOikD5oBclM7MZc32I_1",
      "title": "A Coefficient Makes SVRG Effective",
      "title_normalized": "a_coefficient_makes_svrg_effective",
      "authors": [
        "Yida Yin",
        "Zhiqiu Xu",
        "Zhiyuan Li",
        "Trevor Darrell",
        "Zhuang Liu"
      ],
      "abstract": "Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our empirical analysis finds that, for deeper neural networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\\alpha$-SVRG. Our results show $\\alpha$-SVRG better optimizes models, consistently reducing training loss compared to the baseline and standard SVRG across various model architectures and multiple image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at github.com/davidyyd/alpha-SVRG.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "A_Coefficient_Makes_SVRG_Effective.pdf",
      "sha_abstract": "239e70df6a9d37c53d43744e790606c1a9a9cc55401e7c5e7658c9167d3edd5d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199053012
    },
    {
      "_id": "XeilD5oBclM7MZc3Qo8o",
      "title": "MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine",
      "title_normalized": "mavis_mathematical_visual_instruction_tuning_with_an_automatic_data_engine",
      "authors": [
        "Renrui Zhang",
        "Xinyu Wei",
        "Dongzhi Jiang",
        "Ziyu Guo",
        "Yichi Zhang",
        "Chengzhuo Tong",
        "Jiaming Liu",
        "Aojun Zhou",
        "Shanghang Zhang",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "abstract": "Multi-modal Large Language Models (MLLMs) have recently showcased superior proficiency in general visual scenarios. However, we identify their mathematical capabilities remain under-explored with three areas to be improved: visual encoding of math diagrams, diagram-language alignment, and chain-of-thought (CoT) reasoning. This draws forth an urgent demand for an effective training paradigm and a large-scale, comprehensive dataset with detailed CoT rationales, which is challenging to collect and costly to annotate manually. To tackle this issue, we propose MAVIS, a MAthematical VISual instruction tuning pipeline for MLLMs, featuring an automatic data engine to efficiently create mathematical visual datasets.\nWe design the data generation process to be entirely independent of human intervention or GPT API usage, while ensuring the diagram-caption correspondence, question-answer correctness, and CoT reasoning quality. With this approach, we curate two datasets, MAVIS-Caption (558K diagram-caption pairs) and MAVIS-Instruct (834K visual math problems with CoT rationales), and propose four progressive stages for training MLLMs from scratch.\nFirst, we utilize MAVIS-Caption to fine-tune a math-specific vision encoder (CLIP-Math) through contrastive learning, tailored for improved diagram visual encoding. Second, we also leverage MAVIS-Caption to align the CLIP-Math with a large language model (LLM) by a projection layer, enhancing vision-language alignment in mathematical domains. Third, we adopt MAVIS-Instruct to perform the instruction tuning for robust problem-solving skills, and term the resulting model as MAVIS-7B. Fourth, we apply Direct Preference Optimization (DPO) to enhance the CoT capabilities of our model, further refining its step-wise reasoning performance.\nOn various mathematical benchmarks, our MAVIS-7B achieves leading results among open-source MLLMs, e.g., surpassing other 7B models by +9.3% and the second-best LLaVA-NeXT (110B) by +6.9%, demonstrating the effectiveness of our method.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "MAVIS__Mathematical_Visual_Instruction_Tuning_with_an_Automatic_Data_Engine.pdf",
      "sha_abstract": "99aa00182297997a995df4d07a06b462ca20e0eb0d5ec13d53bd965d1dfdb059",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199079970
    },
    {
      "_id": "XOilD5oBclM7MZc3PY8B",
      "title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
      "title_normalized": "number_cookbook_number_understanding_of_language_models_and_how_to_improve_it",
      "authors": [
        "Haotong Yang",
        "Yi Hu",
        "Shijia Kang",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "abstract": "Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as $9.11 > 9.9$). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear.\nThrough the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work provides a more detailed and comprehensive understanding of NUPA in LLMs.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Number_Cookbook__Number_Understanding_of_Language_Models_and_How_to_Improve_It.pdf",
      "sha_abstract": "b9e4ca76fd7602c48b4e36f277cc0d07c939879f4fa018983059cad21dd4f05d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199078650
    },
    {
      "_id": "YeilD5oBclM7MZc3Vo8V",
      "title": "Attention with Markov: A Curious Case of Single-layer Transformers",
      "title_normalized": "attention_with_markov_a_curious_case_of_singlelayer_transformers",
      "authors": [
        "Ashok Vardhan Makkuva",
        "Marco Bondaschi",
        "Adway Girish",
        "Alliot Nagle",
        "Martin Jaggi",
        "Hyeji Kim",
        "Michael Gastpar"
      ],
      "abstract": "Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Attention_with_Markov__A_Curious_Case_of_Single-layer_Transformers.pdf",
      "sha_abstract": "2357b4d197c3643af13069e590f421b7f5ad04ba8f25a9154360ae40f7c955b0",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199085040
    },
    {
      "_id": "XuilD5oBclM7MZc3R49B",
      "title": "TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice",
      "title_normalized": "tcmoe_augmenting_mixture_of_experts_with_ternary_expert_choice",
      "authors": [
        "Shen Yan",
        "Xingyan Bin",
        "Sijun Zhang",
        "Yisen Wang",
        "Zhouchen Lin"
      ],
      "abstract": "The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters.\nThe effectiveness of MoE models depends primarily on their routing mechanisms, with the widely adopted Top-K routing scheme used for activating experts.\nHowever, the Top-K scheme has notable limitations,\nincluding unnecessary activations and underutilization of experts.\nIn this work, \nrather than modifying the routing mechanism as done in previous studies,\nwe propose the Ternary Choice MoE (TC-MoE),\na novel approach that expands the expert space by applying the ternary set {-1, 0, 1} to each expert.\nThis expansion allows more efficient and effective expert activations without incurring significant computational costs.\nAdditionally, \ngiven the unique characteristics of the expanded expert space,\nwe introduce a new load balance loss and reward loss to ensure workload balance and achieve a flexible trade-off between effectiveness and efficiency.\nExtensive experiments demonstrate that TC-MoE achieves an average improvement of over 1.1% compared with traditional approaches,\nwhile reducing the average number of activated experts by up to 9%.\nThese results confirm that TC-MoE effectively addresses the inefficiencies of conventional routing schemes,\noffering a more efficient and scalable solution for MoE-based large language models.\nCode and models are available at https://github.com/stiger1000/TC-MoE.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "TC-MoE__Augmenting_Mixture_of_Experts_with_Ternary_Expert_Choice.pdf",
      "sha_abstract": "d32dbbcc50ddf04339c857e4e8da7fd11a98f664f1005d0ccf3cc26e450e25bf",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199081270
    },
    {
      "_id": "YuilD5oBclM7MZc3Wo8P",
      "title": "The AdEMAMix Optimizer: Better, Faster, Older",
      "title_normalized": "the_ademamix_optimizer_better_faster_older",
      "authors": [
        "Matteo Pagliardini",
        "Pierre Ablin",
        "David Grangier"
      ],
      "abstract": "Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations which lose their relevance as the iterate moves along the loss landscape. This work questions the use of a single EMA to accumulate past gradients and empirically demonstrates how this choice can be sub-optimal: a single EMA cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients. Building on this observation, we propose AdEMAMix, a simple modification of the Adam optimizer with a mixture of two EMAs to better take advantage of past gradients. Our experiments on language modeling and image classification show---quite surprisingly---that gradients can stay relevant for tens of thousands of steps. They help to converge faster, and often to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B tokens performs comparably to an AdamW model trained on $197$B tokens ($+95\\%$). Moreover, our method significantly slows-down model forgetting during training. Our work motivates further exploration of different types of functions to leverage past gradients, beyond EMAs.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "The_AdEMAMix_Optimizer__Better__Faster__Older.pdf",
      "sha_abstract": "721eabf7ce68db7817d060b77be44e0509751c9f60cee971170ebcba5bcda532",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199086090
    },
    {
      "_id": "X-ilD5oBclM7MZc3S491",
      "title": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling",
      "title_normalized": "provably_accurate_shapley_value_estimation_via_leverage_score_sampling",
      "authors": [
        "Christopher Musco",
        "R. Teal Witter"
      ],
      "abstract": "Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just $O(n\\log n)$ model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we find that Leverage SHAP achieves an approximately 50% reduction in error compared to the highly optimized implementation of Kernel SHAP in the widely used SHAP library [Lundberg & Lee, 2017].",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Provably_Accurate_Shapley_Value_Estimation_via_Leverage_Score_Sampling.pdf",
      "sha_abstract": "9e69e05be24c6d20901f02235207284766d329e802d9836918f501fadd3ebc8a",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199082350
    },
    {
      "_id": "W-ilD5oBclM7MZc3N4-c",
      "title": "Three Mechanisms of Feature Learning in a Linear Network",
      "title_normalized": "three_mechanisms_of_feature_learning_in_a_linear_network",
      "authors": [
        "Yizhou Xu",
        "Liu Ziyin"
      ],
      "abstract": "Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamics of a one-hidden-layer linear network, with one-dimensional data, across any finite width, uniquely exhibiting both kernel and feature learning phases. This study marks a technical advancement by enabling the analysis of the training trajectory from any initialization and a detailed phase diagram under varying common hyperparameters such as width, layer-wise learning rates, and scales of output and initialization. We identify three novel prototype mechanisms specific to the feature learning regime: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling, which contrast starkly with the dynamics observed in the kernel regime. Our theoretical findings are substantiated with empirical evidence showing that these mechanisms also manifest in deep nonlinear networks handling real-world tasks, enhancing our understanding of neural network training dynamics and guiding the design of more effective learning strategies.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Three_Mechanisms_of_Feature_Learning_in_a_Linear_Network.pdf",
      "sha_abstract": "99beab18f7836b2ca1f5fa6582ca15725abd17e754b71086917059351d14b44d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199077270
    },
    {
      "_id": "WuilD5oBclM7MZc3M49Q",
      "title": "Affine Steerable Equivariant Layer for Canonicalization of Neural Networks",
      "title_normalized": "affine_steerable_equivariant_layer_for_canonicalization_of_neural_networks",
      "authors": [
        "Yikang Li",
        "Yeqing Qiu",
        "Yuxuan Chen",
        "Zhouchen Lin"
      ],
      "abstract": "In the field of equivariant networks, achieving affine equivariance, particularly for general group representations, has long been a challenge.\nIn this paper, we propose the steerable EquivarLayer, a generalization of InvarLayer (Li et al., 2024), by building on the concept of equivariants beyond invariants.\nThe steerable EquivarLayer supports affine equivariance with arbitrary input and output representations, marking the first model to incorporate steerability into networks for the affine group.\nTo integrate it with canonicalization, a promising approach for making pre-trained models equivariant, we introduce a novel Det-Pooling module, expanding the applicability of EquivarLayer and the range of groups suitable for canonicalization.\nWe conduct experiments on image classification tasks involving group transformations to validate the steerable EquivarLayer in the role of a canonicalization function, demonstrating its effectiveness over data augmentation.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Affine_Steerable_Equivariant_Layer_for_Canonicalization_of_Neural_Networks.pdf",
      "sha_abstract": "15d91b02b33bcd5c155838d6780930a3599d966f38a973ea5011e4d83997f6bf",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199076170
    },
    {
      "_id": "YOilD5oBclM7MZc3UY8A",
      "title": "Remove Symmetries to Control Model Expressivity and Improve Optimization",
      "title_normalized": "remove_symmetries_to_control_model_expressivity_and_improve_optimization",
      "authors": [
        "Liu Ziyin",
        "Yizhou Xu",
        "Isaac L. Chuang"
      ],
      "abstract": "When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a ``collapse.\" Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We first prove two concrete mechanisms through which symmetries lead to reduced capacities and ignored features during training and inference. We then propose a simple and theoretically justified algorithm, \\textit{syre}, to remove almost all symmetry-induced low-capacity states in neural networks. When this type of entrapment is especially a concern, removing symmetries with the proposed method is shown to correlate well with improved optimization or performance. A remarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge of the symmetry.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Remove_Symmetries_to_Control_Model_Expressivity_and_Improve_Optimization.pdf",
      "sha_abstract": "e02bb0c60f183294ca6248d9fa69673eebaa4d73f2314c74c829747d4bf9d07c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199083770
    },
    {
      "_id": "F-ijD5oBclM7MZc3wo-T",
      "title": "The 3D-PC: a benchmark for visual perspective taking in humans and machines",
      "title_normalized": "the_3dpc_a_benchmark_for_visual_perspective_taking_in_humans_and_machines",
      "authors": [
        "Drew Linsley",
        "Peisen Zhou",
        "Alekh Karkada Ashok",
        "Akash Nagaraj",
        "Gaurav Gaonkar",
        "Francis E Lewis",
        "Zygmunt Pizlo",
        "Thomas Serre"
      ],
      "abstract": "Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT (VPT-perturb) designed to limit the effectiveness of \"shortcut\" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "The_3D-PC__a_benchmark_for_visual_perspective_taking_in_humans_and_machines.pdf",
      "sha_abstract": "9585d404d528dfa529c1117d0589fd23f599b5c8abee4464b3f10bd7ef74450b",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198981752
    },
    {
      "_id": "FeiaD5oBclM7MZc3jI-A",
      "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory",
      "title_normalized": "delta_an_online_documentlevel_translation_agent_based_on_multilevel_memory",
      "authors": [
        "Yutong Wang",
        "Jiali Zeng",
        "Xuebo Liu",
        "Derek F. Wong",
        "Fandong Meng",
        "Jie Zhou",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT).\nHowever, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents.\nIn this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations.\nDelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components.\nExperimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average.\nDelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method.\nFurthermore, DelTA improves pronoun and context-dependent translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks.\nThe code and data of our approach are released at https://github.com/YutongWang1216/DocMTAgent.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "DelTA__An_Online_Document-Level_Translation_Agent_Based_on_Multi-Level_Memory.pdf",
      "sha_abstract": "f5aee9f167eb47888090f3c8ed664c0fea08c93410f1296da9a2eaf3e0521126",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198377955
    },
    {
      "_id": "E-iaD5oBclM7MZc3f4-m",
      "title": "From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle",
      "title_normalized": "from_an_llm_swarm_to_a_pddlempowered_hive_planning_selfexecuted_instructions_in_a_multimodal_jungle",
      "authors": [
        "Kaustubh Vyas",
        "Damien Graux",
        "Yijun Yang",
        "Sebastien Montella",
        "Chenxin Diao",
        "Wendi Zhou",
        "Pavlos Vougiouklis",
        "Ruofei Lai",
        "Yang Ren",
        "Keshuang Li",
        "Jeff Z. Pan"
      ],
      "abstract": "In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.\n\nOur system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "From_an_LLM_Swarm_to_a_PDDL-empowered_Hive__Planning_Self-executed_Instructions_in_a_Multi-modal_Jungle.pdf",
      "sha_abstract": "b4a622954b6e7560e33006a2eb89c6174322b980169809f88b9b2a00cd3663dd",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198374797
    },
    {
      "_id": "EuiaD5oBclM7MZc3eY9c",
      "title": "MiniPLM: Knowledge Distillation for Pre-training Language Models",
      "title_normalized": "miniplm_knowledge_distillation_for_pretraining_language_models",
      "authors": [
        "Yuxian Gu",
        "Hao Zhou",
        "Fandong Meng",
        "Jie Zhou",
        "Minlie Huang"
      ],
      "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. \nWhile effective in fine-tuning, KD during pre-training faces efficiency, flexibility, and effectiveness issues. \nExisting methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data.\nIn this work, we propose **MiniPLM**, a KD framework for pre-training LMs by refining the training data distribution with the teacher LM's knowledge.\nFor efficiency, MiniPLM performs offline teacher inference, allowing KD for multiple student LMs without adding training costs.\nFor flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families.\nFor effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the training data difficulty and diversity, helping student LMs acquire versatile and sophisticated knowledge.\nExtensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 common downstream tasks, improves language modeling capabilities, and reduces pre-training computation. \nThe benefit of MiniPLM extends to larger training scales, evidenced by the scaling curve extrapolation.\nFurther analysis reveals that MiniPLM supports KD across model families and enhances the pre-training data utilization. Our code, data, and models can be found at https://github.com/thu-coai/MiniPLM.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "MiniPLM__Knowledge_Distillation_for_Pre-training_Language_Models.pdf",
      "sha_abstract": "23c4a07f43020622c025e4aecec48dd5be474aeb05ee40c87ab7d2d8cc02dfee",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198373088
    },
    {
      "_id": "GOijD5oBclM7MZc3yY-C",
      "title": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding",
      "title_normalized": "contextual_selfpaced_learning_for_weakly_supervised_spatiotemporal_video_grounding",
      "authors": [
        "Akash Kumar",
        "Zsolt Kira",
        "Yogesh S Rawat"
      ],
      "abstract": "In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects  spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Contextual_Self-paced_Learning_for_Weakly_Supervised_Spatio-Temporal_Video_Grounding.pdf",
      "sha_abstract": "f0ef99df8e38574f2f50aa9ed6a0bb51cef9b41af8cda2245698d6a5d3c13bb8",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198983525
    },
    {
      "_id": "FuijD5oBclM7MZc3uo-H",
      "title": "Discrete Codebook World Models for Continuous Control",
      "title_normalized": "discrete_codebook_world_models_for_continuous_control",
      "authors": [
        "Aidan Scannell",
        "Mohammadreza Nakhaeinezhadfard",
        "Kalle Kujanpää",
        "Yi Zhao",
        "Kevin Sebastian Luck",
        "Arno Solin",
        "Joni Pajarinen"
      ],
      "abstract": "In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While previous approaches leveraging discrete latent spaces, such as DreamerV3, have demonstrated strong performance in discrete action settings and visual control tasks, their comparative performance in state-based continuous control remains underexplored. In contrast, methods with continuous latent spaces, such as TD-MPC2, have shown notable success in state-based continuous control benchmarks. In this paper, we demonstrate that modeling discrete latent states has benefits over continuous latent states and that discrete codebook encodings are more effective representations for continuous control, compared to alternative encodings, such as one-hot and label-based encodings. Based on these insights, we introduce DCWM: Discrete Codebook World Model, a self-supervised world model with a discrete and stochastic latent space, where latent states are codes from a codebook. We combine DCWM with decision-time planning to get our model-based RL algorithm, named DC-MPC: Discrete Codebook Model Predictive Control, which performs competitively against recent state-of-the-art algorithms, including TD-MPC2 and DreamerV3, on continuous control benchmarks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Discrete_Codebook_World_Models_for_Continuous_Control.pdf",
      "sha_abstract": "4ecb5992b96ab009e0644e81a836cf294143da8a5d51aac50000e3381dd5587d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198979629
    },
    {
      "_id": "GuijD5oBclM7MZc30o9Y",
      "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
      "title_normalized": "minictx_neural_theorem_proving_with_longcontexts",
      "authors": [
        "Jiewen Hu",
        "Thomas Zhu",
        "Sean Welleck"
      ],
      "abstract": "Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\\texttt{miniCTX}$, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. $\\texttt{miniCTX}$ contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for $\\texttt{miniCTX}$, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as $\\texttt{miniF2F}$. Alongside $\\texttt{miniCTX}$, we offer $\\texttt{ntp-toolkit}$ for automatically extracting and annotating theorem proving data, making it easy to add new projects into $\\texttt{miniCTX}$ to ensure that contexts are not seen during training. $\\texttt{miniCTX}$ offers a challenging and realistic evaluation of neural theorem provers.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "miniCTX__Neural_Theorem_Proving_with__Long-_Contexts.pdf",
      "sha_abstract": "2ecffffc4e68527d96a2c53287f70d7f192e37ae36cefe300b2529ef89d3f3ca",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198985781
    },
    {
      "_id": "D-iaD5oBclM7MZc3aY_-",
      "title": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models",
      "title_normalized": "beyond_next_token_prediction_patchlevel_training_for_large_language_models",
      "authors": [
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\\times$, without compromising the model performance compared to token-level training. Source code: \\url{https://github.com/shaochenze/PatchTrain}.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Beyond_Next_Token_Prediction__Patch-Level_Training_for_Large_Language_Models.pdf",
      "sha_abstract": "9708cfcf22e205df0997b86176d4e0ecbd09b2f0c1216a1a4d6e21eb6fdb2ced",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198369027
    },
    {
      "_id": "FOiaD5oBclM7MZc3hY9K",
      "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
      "title_normalized": "recite_reconstruct_recollect_memorization_in_lms_as_a_multifaceted_phenomenon",
      "authors": [
        "USVSN Sai Prashanth",
        "Alvin Deng",
        "Kyle O'Brien",
        "Jyothir S V",
        "Mohammad Aflah Khan",
        "Jaydeep Borkar",
        "Christopher A. Choquette-Choo",
        "Jacob Ray Fuehne",
        "Stella Biderman",
        "Tracy Ke",
        "Katherine Lee",
        "Naomi Saphra"
      ],
      "abstract": "Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors have different influences on the likelihood of memorization depending on the taxonomic category.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Recite__Reconstruct__Recollect__Memorization_in_LMs_as_a_Multifaceted_Phenomenon.pdf",
      "sha_abstract": "f478eb2ea9eaebe040e47758bd9f448039cb741aa313358df9d9c28a00a004be",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198376229
    },
    {
      "_id": "GeijD5oBclM7MZc3zY_j",
      "title": "Faster Cascades via Speculative Decoding",
      "title_normalized": "faster_cascades_via_speculative_decoding",
      "authors": [
        "Harikrishna Narasimhan",
        "Wittawat Jitkrittum",
        "Ankit Singh Rawat",
        "Seungyeon Kim",
        "Neha Gupta",
        "Aditya Krishna Menon",
        "Sanjiv Kumar"
      ],
      "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while  speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Faster_Cascades_via_Speculative_Decoding.pdf",
      "sha_abstract": "c1740e4314d3ff87a4c6b53014889f0eada36cfb0e6ecf2e8582608fb1d7f6db",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198984642
    },
    {
      "_id": "IOikD5oBclM7MZc3BY8Z",
      "title": "Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers",
      "title_normalized": "toward_exploratory_inverse_constraint_inference_with_generative_diffusion_verifiers",
      "authors": [
        "Runyi Zhao",
        "Sheng Xu",
        "Bo Yue",
        "Guiliang Liu"
      ],
      "abstract": "An important prerequisite for safe control is aligning the policy with the underlying constraints in the environment. In many real-world applications, due to the difficulty of manually specifying these constraints, existing works have proposed recovering constraints from expert demonstrations by solving the Inverse Constraint Learning (ICL) problem. However, ICL is inherently ill-posed, as multiple constraints can equivalently explain the experts' preferences, making the optimal solutions not uniquely identifiable. In this work, instead of focusing solely on a single constraint, we propose the novel approach of Exploratory ICL (ExICL). The goal of ExICL is to recover a diverse set of feasible constraints, thereby providing practitioners the flexibility to select the most appropriate constraint based on the practical needs of deployment. To achieve this goal, we design a generative diffusion verifier that guides the trajectory generation process using the probabilistic representation of an optimal constrained policy. By comparing these decisions with those made by expert agents, we can efficiently verify a candidate constraint. Driven by the verification feedback, ExICL implements an exploratory constraint update mechanism that strategically facilitates diversity within the collection of feasible constraints. Our empirical results demonstrate that ExICL can seamlessly and reliably generalize across different tasks and environments. The code is available at https://github.com/ZhaoRunyi/ExICL.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Toward_Exploratory_Inverse_Constraint_Inference_with_Generative_Diffusion_Verifiers.pdf",
      "sha_abstract": "4d18304d337bc38f84922e8603772c257d14737c3af47ca881dcc542344cdf01",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198998781
    },
    {
      "_id": "IeikD5oBclM7MZc3DI_z",
      "title": "Federated Domain Generalization with Data-free On-server Matching Gradient",
      "title_normalized": "federated_domain_generalization_with_datafree_onserver_matching_gradient",
      "authors": [
        "Trong Binh Nguyen",
        "Duong Minh Nguyen",
        "Jinsun Park",
        "Viet Quoc Pham",
        "Won-Joo Hwang"
      ],
      "abstract": "Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can efficiently leverage domain information from distributed domains. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome). The reproducible code is publicly available~\\footnote[1]{\\url{https://github.com/skydvn/fedomg}}.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Federated_Domain_Generalization_with_Data-free_On-server_Matching_Gradient.pdf",
      "sha_abstract": "e5451dd8105d59d1129baa649e484046b9abe7b66cec7d026cd08ddafc3bf941",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199000810
    },
    {
      "_id": "IuikD5oBclM7MZc3EI_M",
      "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
      "title_normalized": "contextparametric_inversion_why_instruction_finetuning_may_not_actually_improve_context_reliance",
      "authors": [
        "Sachin Goyal",
        "Christina Baek",
        "J Zico Kolter",
        "Aditi Raghunathan"
      ],
      "abstract": "Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\\textit{despite an initial expected increase}$. We call this phenomenon as the $\\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. \nWe tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Context-Parametric_Inversion__Why_Instruction_Finetuning_May_Not_Actually_Improve_Context_Reliance.pdf",
      "sha_abstract": "ddb23dfb5f724a0aa2c4422256a6951b7cea7096a55b22eda400200b1b3bfe64",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199001795
    },
    {
      "_id": "H-ijD5oBclM7MZc394_b",
      "title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains",
      "title_normalized": "nesyc_a_neurosymbolic_continual_learner_for_complex_embodied_tasks_in_open_domains",
      "authors": [
        "Wonje Choi",
        "Jinwoo Park",
        "Sanghyun Ahn",
        "Daehee Lee",
        "Honguk Woo"
      ],
      "abstract": "We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks—including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario—demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "NeSyC__A_Neuro-symbolic_Continual_Learner_For_Complex_Embodied_Tasks_in_Open_Domains.pdf",
      "sha_abstract": "174ab84a6e13c0f94b91c0fc30c0c130aabe6c3bb1898c7e106dddef745edae2",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198995392
    },
    {
      "_id": "G-ijD5oBclM7MZc32498",
      "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
      "title_normalized": "directional_gradient_projection_for_robust_finetuning_of_foundation_models",
      "authors": [
        "Chengyue Huang",
        "Junjiao Tian",
        "Brisa Maneechotesuwan",
        "Shivang Chopra",
        "Zsolt Kira"
      ],
      "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose $\\textbf{Di}$rectional $\\textbf{Gra}$dient $\\textbf{P}$rojection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Directional_Gradient_Projection_for_Robust_Fine-Tuning_of_Foundation_Models.pdf",
      "sha_abstract": "ba5d0249766995fe9964cf3837984acec094b1a550edbb523f9d42f97e68d001",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198988130
    },
    {
      "_id": "HeijD5oBclM7MZc35I_S",
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "title_normalized": "scaling_diffusion_language_models_via_adaptation_from_autoregressive_models",
      "authors": [
        "Shansan Gong",
        "Shivam Agarwal",
        "Yizhe Zhang",
        "Jiacheng Ye",
        "Lin Zheng",
        "Mukai Li",
        "Chenxin An",
        "Peilin Zhao",
        "Wei Bi",
        "Jiawei Han",
        "Hao Peng",
        "Lingpeng Kong"
      ],
      "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Scaling_Diffusion_Language_Models_via_Adaptation_from_Autoregressive_Models.pdf",
      "sha_abstract": "78f73b00d77f60394656599f83889b95403bbb4502ce0e3651797efb6939554d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198990524
    },
    {
      "_id": "HOijD5oBclM7MZc33I-I",
      "title": "Towards a Unified and Verified Understanding of Group-Operation Networks",
      "title_normalized": "towards_a_unified_and_verified_understanding_of_groupoperation_networks",
      "authors": [
        "Wilson Wu",
        "Louis Jaburi",
        "jacob drori",
        "Jason Gross"
      ],
      "abstract": "A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Towards_a_Unified_and_Verified_Understanding_of_Group-Operation_Networks.pdf",
      "sha_abstract": "d475afb9f3dd8840f600b5793dbe4d0b8a4fcf5d77c57facc38018254f2708d0",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198988393
    },
    {
      "_id": "I-ikD5oBclM7MZc3GY8j",
      "title": "Understanding Optimization in Deep Learning with Central Flows",
      "title_normalized": "understanding_optimization_in_deep_learning_with_central_flows",
      "authors": [
        "Jeremy Cohen",
        "Alex Damian",
        "Ameet Talwalkar",
        "J Zico Kolter",
        "Jason D. Lee"
      ],
      "abstract": "Optimization in deep learning remains poorly understood.  A key difficulty is that optimizers exhibit complex oscillatory dynamics, referred to as \"edge of stability,\" which cannot be captured by traditional optimization theory.  In this paper, we show that the path taken by an oscillatory optimizer can often be captured by a  _central flow_: a differential equation which directly models the time-averaged (i.e. smoothed) optimization trajectory. We empirically show that these central flows can predict long-term optimization trajectories for generic neural networks with a high degree of numerical accuracy.  By interpreting these flows, we are able to understand  how gradient descent makes progress even as the loss sometimes goes up; how adaptive optimizers ``adapt'' to the local loss landscape; and how adaptive optimizers implicitly seek out regions of weight space where they can take larger steps.  These insights (and others) are not apparent from the optimizers' update rules, but are revealed by the central flows.  Therefore, we believe that central flows constitute a promising tool for reasoning about optimization in deep learning.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Understanding_Optimization_in_Deep_Learning_with_Central_Flows.pdf",
      "sha_abstract": "c5c0a3b3911f0355fdb3889f0811c9f9985f0b99054055705ac877dc709b5494",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199003930
    },
    {
      "_id": "HuijD5oBclM7MZc38I8Q",
      "title": "Multi-domain Distribution Learning for De Novo Drug Design",
      "title_normalized": "multidomain_distribution_learning_for_de_novo_drug_design",
      "authors": [
        "Arne Schneuing",
        "Ilia Igashov",
        "Adrian W. Dobbelstein",
        "Thomas Castiglione",
        "Michael M. Bronstein",
        "Bruno Correia"
      ],
      "abstract": "We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Multi-domain_Distribution_Learning_for_De_Novo_Drug_Design.pdf",
      "sha_abstract": "7299172cd01a0123aae5f15f4fa31f56f0fe25422cdc70ad62a6b2f3687da9d2",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761198993402
    },
    {
      "_id": "JOikD5oBclM7MZc3Ho8j",
      "title": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters",
      "title_normalized": "inference_optimal_vlms_need_fewer_visual_tokens_and_more_parameters",
      "authors": [
        "Kevin Li",
        "Sachin Goyal",
        "João D. Semedo",
        "J Zico Kolter"
      ],
      "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations into the token inputs of Large Language Models (LLMs). However, their real-world deployment is often constrained by high latency during inference due to the substantial compute required by the LLM to process the large number of input tokens, predominantly arising from the image. To reduce inference costs, one can either downsize the LLM or reduce the number of input tokens needed to represent the image, the latter of which has been the focus of many recent efforts around token compression. However, it is unclear what the optimal trade-off is given a fixed inference budget. \nWe first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs is achieved by using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take the first steps toward designing token compression algorithms tailored for high-compression settings, utilizing prompt-based compression of tokens. Our work underscores the performance and efficiency benefits of operating in low visual token regimes and the importance of developing tailored token reduction algorithms for such conditions.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Inference_Optimal_VLMs_Need_Fewer_Visual_Tokens_and_More_Parameters.pdf",
      "sha_abstract": "c88738c13ff825d5d3d77962d25af31795562c13e261e7c4c382f4650d5b6780",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199005210
    },
    {
      "_id": "LOikD5oBclM7MZc3TY_f",
      "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
      "title_normalized": "agentharm_a_benchmark_for_measuring_harmfulness_of_llm_agents",
      "authors": [
        "Maksym Andriushchenko",
        "Alexandra Souly",
        "Mateusz Dziemian",
        "Derek Duenas",
        "Maxwell Lin",
        "Justin Wang",
        "Dan Hendrycks",
        "Andy Zou",
        "J Zico Kolter",
        "Matt Fredrikson",
        "Yarin Gal",
        "Xander Davies"
      ],
      "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents---which use external tools and can execute multi-stage tasks---may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly complaint with malicious agent requests without jailbreaking, (2) simple universal jailbreak strings can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "AgentHarm__A_Benchmark_for_Measuring_Harmfulness_of_LLM_Agents.pdf",
      "sha_abstract": "112e83a7b152a913153f288f3bee4ae334b3671856467caec6ca6ee3eaa1b397",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199017413
    },
    {
      "_id": "KOikD5oBclM7MZc3OY-C",
      "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
      "title_normalized": "madtd_modelaugmented_data_stabilizes_high_update_ratio_rl",
      "authors": [
        "Claas A Voelcker",
        "Marcel Hussing",
        "Eric Eaton",
        "Amir-massoud Farahmand",
        "Igor Gilitschenski"
      ],
      "abstract": "Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process.  Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "MAD-TD__Model-Augmented_Data_stabilizes_High_Update_Ratio_RL.pdf",
      "sha_abstract": "de33890c223c008f9657b46ae8a0f14091ca069c4b024347c2f3b8034ec28770",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199012210
    },
    {
      "_id": "JeikD5oBclM7MZc3J48k",
      "title": "StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces",
      "title_normalized": "stochsync_stochastic_diffusion_synchronization_for_image_generation_in_arbitrary_spaces",
      "authors": [
        "Kyeongmin Yeo",
        "Jaihoon Kim",
        "Minhyuk Sung"
      ],
      "abstract": "We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360◦ panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization–performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space–generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling–gradually updating the target space data through gradient descent–results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360◦ panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "StochSync__Stochastic_Diffusion_Synchronization_for_Image_Generation_in_Arbitrary_Spaces.pdf",
      "sha_abstract": "f7c538d27a0658b43507adc170d31e1f3acf1466e49103e6c84c570b4b9faf96",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199007512
    },
    {
      "_id": "KuikD5oBclM7MZc3RI90",
      "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws",
      "title_normalized": "adaptive_data_optimization_dynamic_sample_selection_with_scaling_laws",
      "authors": [
        "Yiding Jiang",
        "Allan Zhou",
        "Zhili Feng",
        "Sadhika Malladi",
        "J Zico Kolter"
      ],
      "abstract": "The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Adaptive_Data_Optimization__Dynamic_Sample_Selection_with_Scaling_Laws.pdf",
      "sha_abstract": "c4902c4850a377de50bb4fd6ba922c5296f41298277455585e6aafe05e879c19",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199015018
    },
    {
      "_id": "K-ikD5oBclM7MZc3SY8w",
      "title": "Compute-Optimal LLMs Provably Generalize Better with Scale",
      "title_normalized": "computeoptimal_llms_provably_generalize_better_with_scale",
      "authors": [
        "Marc Anton Finzi",
        "Sanyam Kapoor",
        "Diego Granziol",
        "Anming Gu",
        "Christopher De Sa",
        "J Zico Kolter",
        "Andrew Gordon Wilson"
      ],
      "abstract": "Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \\emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Compute-Optimal_LLMs_Provably_Generalize_Better_with_Scale.pdf",
      "sha_abstract": "b1f48346893cc97894134708442d6e2b754391da8abe3e023129a45c250fd82b",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199016230
    },
    {
      "_id": "JuikD5oBclM7MZc3K4_0",
      "title": "Data Unlearning in Diffusion Models",
      "title_normalized": "data_unlearning_in_diffusion_models",
      "authors": [
        "Silas Alberti",
        "Kenan Hasanaliyev",
        "Manav Shah",
        "Stefano Ermon"
      ],
      "abstract": "Recent work has shown that diffusion models memorize and reproduce training data examples. At the same time, large copyright lawsuits and legislation such as GDPR have highlighted the need for erasing datapoints from diffusion models. However, retraining from scratch is often too expensive. This motivates the setting of data unlearning, i.e., the study of efficient techniques for unlearning specific datapoints from the training set. Existing concept unlearning techniques require an anchor prompt/class/distribution to guide unlearning, which is not available in the data unlearning setting. General-purpose machine unlearning techniques were found to be either unstable or failed to unlearn data. We therefore propose a family of new loss functions called Subtracted Importance Sampled Scores (SISS) that utilize importance sampling and are the first method to unlearn data with theoretical guarantees. SISS is constructed as a weighted combination between simpler objectives that are responsible for preserving model quality and unlearning the targeted datapoints. When evaluated on CelebA-HQ and MNIST, SISS achieved Pareto optimality along the quality and unlearning strength dimensions. On Stable Diffusion, SISS successfully mitigated memorization on nearly 90% of the prompts we tested. We release our code online.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Data_Unlearning_in_Diffusion_Models.pdf",
      "sha_abstract": "f9809c3aa8412392e95ce3b1d7f9c31a490e00be541cbe489bbfc9e7a0c44abe",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199008750
    },
    {
      "_id": "J-ikD5oBclM7MZc3NI9W",
      "title": "Consistency Models Made Easy",
      "title_normalized": "consistency_models_made_easy",
      "authors": [
        "Zhengyang Geng",
        "Ashwini Pokle",
        "Weijian Luo",
        "Justin Lin",
        "J Zico Kolter"
      ],
      "abstract": "Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an effective scheme for training CMs that largely improves the efficiency of building such models. Specifically, by expressing CM trajectories via a particular differential equation, we argue that diffusion models can be viewed as a special case of CMs. We can thus fine-tune a consistency model starting from a pretrained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly reduced training times while improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained for hundreds of GPU hours. Owing to this computational efficiency, we investigate the scaling laws of CMs under ECT, showing that they obey the classic power law scaling, hinting at their ability to improve efficiency and performance at larger scales. Our [code](https://github.com/locuslab/ect) is available.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Consistency_Models_Made_Easy.pdf",
      "sha_abstract": "b978c9f23f73106902eda090ff5d0c4fcbb5debeed4bbbaee7fb1f176ae5fdef",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199010890
    },
    {
      "_id": "KeikD5oBclM7MZc3P4_b",
      "title": "Fast Uncovering of Protein Sequence Diversity from Structure",
      "title_normalized": "fast_uncovering_of_protein_sequence_diversity_from_structure",
      "authors": [
        "luca alessandro silva",
        "Barthelemy Meynard-Piganeau",
        "Carlo Lucibello",
        "Christoph Feinauer"
      ],
      "abstract": "We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of a pairwise probability distribution over the space of sequences, capturing the amino acid covariances observed in Multiple Sequence Alignments (MSA) of homologous proteins. This allows for the efficient generation of highly diverse protein sequences while preserving structural and functional integrity.\nWe demonstrate that this increased diversity in sampled sequences translates into greater variability in biochemical properties, highlighting the exciting potential of our method for applications such as protein design. The orders of magnitude improvement in sampling speed compared to existing methods unlocks new possibilities for high-throughput in virtual screening.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Fast_Uncovering_of_Protein_Sequence_Diversity_from_Structure.pdf",
      "sha_abstract": "8eb22f03bc71b0f57c61fa23d160455d7c6749d88e318cd896a068819c33f4b2",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199013843
    },
    {
      "_id": "NuikD5oBclM7MZc3gI88",
      "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
      "title_normalized": "everything_everywhere_all_at_once_is_mechanistic_interpretability_identifiable",
      "authors": [
        "Maxime Méloux",
        "Silviu Maniu",
        "François Portet",
        "Maxime Peyrard"
      ],
      "abstract": "As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms embedded within their structures to explain their behavior. This work systematically examines a fundamental question: for a fixed behavior to explain, and under the criteria that MI sets for itself, are we guaranteed a unique explanation? Drawing an analogy with the concept of identifiability in statistics, which ensures the uniqueness of parameters inferred from data under specific modeling assumptions, we speak about the identifiability of explanations produced by MI.\n\nWe identify two broad strategies to produce MI explanations: (i) \"where-then-what\", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) \"what-then-where\", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. \n\nWe systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons small enough to allow a complete enumeration of candidate explanations. Our experiments reveal overwhelming evidence of non-identifiability in all cases: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and a single algorithm can be causally aligned with different subspaces of the network.\n\nWe discuss whether the unicity intuition is necessary. One could adopt a pragmatic stance, requiring explanations only to meet predictive and/or manipulability standards. However, if unicity is considered essential, e.g., to provide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the inner interpretability framework that demands explanations to be validated by multiple complementary criteria. This work aims to contribute constructively to the ongoing effort to formalize what we expect from explanations in AI.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Everything__Everywhere__All_at_Once__Is_Mechanistic_Interpretability_Identifiable_.pdf",
      "sha_abstract": "ed9823162141df36378ad1db13a29f50a5126df24f7e0c9a309d4d2787cb911c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199030299
    },
    {
      "_id": "OuikD5oBclM7MZc3lI8S",
      "title": "Beyond Worst-Case Dimensionality Reduction for Sparse Vectors",
      "title_normalized": "beyond_worstcase_dimensionality_reduction_for_sparse_vectors",
      "authors": [
        "Sandeep Silwal",
        "David Woodruff",
        "Qiuyi Zhang"
      ],
      "abstract": "We study beyond worst-case dimensionality reduction for $s$-sparse vectors (vectors with at most $s$ non-zero coordinates). Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis:\n\n\\noindent (a)  We first consider average-case guarantees for embedding $s$-sparse vectors. Here, a well-known folklore upper bound based on the birthday-paradox states: For any collection $X$ of $s$-sparse vectors in $\\mathbb{R}^d$, there exists a linear map $A: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{O(s^2)}$ which \\emph{exactly} preserves the norm of $99\\%$ of the vectors in $X$ in any $\\ell_p$ norm (as opposed to the usual setting where guarantees hold for all vectors). We provide novel lower bounds showing that this is indeed optimal in many settings. Specifically, any oblivious linear map satisfying similar average-case guarantees must map to $\\Omega(s^2)$ dimensions. The same lower bound also holds for a wider class of sufficiently smooth maps, including `encoder-decoder schemes', where we compare the norm of the original vector to that of a smooth function of the embedding. These lower bounds reveal a surprising separation result for smooth embeddings of sparse vectors, as an upper bound of $O(s \\log(d))$ is possible if we instead use arbitrary functions, e.g., via compressed sensing algorithms.\n\n\n (b) Given these lower bounds, we specialize to sparse \\emph{non-negative} vectors to hopes of improved upper bounds. For a dataset $X$ of non-negative $s$-sparse vectors and any $p \\ge 1$, we can non-linearly embed $X$ to $O(s\\log(|X|s)/\\varepsilon^2)$ dimensions while preserving all pairwise distances in $\\ell_p$ norm up to $1\\pm \\varepsilon$, with no dependence on $p$. Surprisingly, the non-negativity assumption enables much smaller embeddings than arbitrary sparse vectors, where the best known bound suffers an exponential $(\\log |X|)^{O(p)}$ dependence. Our map also guarantees \\emph{exact} dimensionality reduction for the $\\ell_{\\infty}$ norm by embedding $X$ into $O(s\\log |X|)$ dimensions, which is tight. We further give separation results showing that both the non-linearity of $f$ and the non-negativity of $X$ are necessary, and provide downstream algorithmic improvements using our embedding.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Beyond_Worst-Case_Dimensionality_Reduction_for_Sparse_Vectors.pdf",
      "sha_abstract": "c617cdcdeff48204b886058fea7e8094a035f52e752bf42f1afbd9d0096f2cba",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199035398
    },
    {
      "_id": "PuikD5oBclM7MZc3qY8c",
      "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
      "title_normalized": "retrieval_head_mechanistically_explains_longcontext_factuality",
      "authors": [
        "Wenhao Wu",
        "Yizhong Wang",
        "Guangxuan Xiao",
        "Hao Peng",
        "Yao Fu"
      ],
      "abstract": "Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Retrieval_Head_Mechanistically_Explains_Long-Context_Factuality.pdf",
      "sha_abstract": "af70edec3b9426604b3c88ea7e943b7dc09f6c671ab0afd87f478a767fc26e26",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199040790
    },
    {
      "_id": "N-ikD5oBclM7MZc3hI9B",
      "title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling",
      "title_normalized": "masked_diffusion_models_are_secretly_timeagnostic_masked_models_and_exploit_inaccurate_categorical_sampling",
      "authors": [
        "Kaiwen Zheng",
        "Yongxin Chen",
        "Hanzi Mao",
        "Ming-Yu Liu",
        "Jun Zhu",
        "Qinsheng Zhang"
      ],
      "abstract": "Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. \nIn this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\\times$ speedup. In addition, our investigation raises doubts about whether MDMs can truly beat ARMs in text generation. We identify, for the first time, an underlying numerical issue, even with the commonly used 32-bit floating-point precision, which results in inaccurate categorical sampling. \nWe show that it lowers the effective temperature both theoretically and empirically, and the resulting decrease in token diversity makes previous evaluations, which assess the generation quality solely through the incomplete generative perplexity metric, somewhat unfair.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Masked_Diffusion_Models_are_Secretly_Time-Agnostic_Masked_Models_and_Exploit_Inaccurate_Categorical_Sampling.pdf",
      "sha_abstract": "0c27bc80e84183a0ab222964b4054fbc1ab68222bb7a744b09729da3407a2bc5",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199031351
    },
    {
      "_id": "OeikD5oBclM7MZc3j48s",
      "title": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning",
      "title_normalized": "lift_learning_to_finetune_via_bayesian_parameter_efficient_meta_finetuning",
      "authors": [
        "Minyoung Kim",
        "Timothy Hospedales"
      ],
      "abstract": "We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "LiFT__Learning_to_Fine-Tune_via_Bayesian_Parameter_Efficient_Meta_Fine-Tuning.pdf",
      "sha_abstract": "acb24036a419b330039e747be336d223aa500add9cd4638723a38c45b0f9229a",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199034150
    },
    {
      "_id": "POikD5oBclM7MZc3n4_C",
      "title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems",
      "title_normalized": "improving_data_efficiency_via_curating_llmdriven_rating_systems",
      "authors": [
        "Jinlong Pang",
        "Jiaheng Wei",
        "Ankit Shah",
        "Zhaowei Zhu",
        "Yaxuan Wang",
        "Chen Qian",
        "Yang Liu",
        "Yujia Bao",
        "Wei Wei"
      ],
      "abstract": "Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce $DS^2$, a **D**iversity-aware **S**core curation method for **D**ata **S**election. By systematically modeling error patterns through a score transition matrix, $DS^2$ corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3\\% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that ``more can be less''.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Improving_Data_Efficiency_via_Curating_LLM-Driven_Rating_Systems.pdf",
      "sha_abstract": "3c5115fbf29eb8d1030f0c8b99d76fd2e7c686dced2331392748dac665fed628",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199038393
    },
    {
      "_id": "O-ikD5oBclM7MZc3mY9W",
      "title": "Elucidating the Preconditioning in Consistency Distillation",
      "title_normalized": "elucidating_the_preconditioning_in_consistency_distillation",
      "authors": [
        "Kaiwen Zheng",
        "Guande He",
        "Jianfei Chen",
        "Fan Bao",
        "Jun Zhu"
      ],
      "abstract": "Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \\textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\\times$ to $3\\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Elucidating_the_Preconditioning_in_Consistency_Distillation.pdf",
      "sha_abstract": "92ac9a9c96c0178e57c8869d44e27ffa094e424fa40fffc5b6a8a5555482ea3c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199036741
    },
    {
      "_id": "OOikD5oBclM7MZc3io_4",
      "title": "Diffusion Bridge Implicit Models",
      "title_normalized": "diffusion_bridge_implicit_models",
      "authors": [
        "Kaiwen Zheng",
        "Guande He",
        "Jianfei Chen",
        "Fan Bao",
        "Jun Zhu"
      ],
      "abstract": "Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at \\url{https://github.com/thu-ml/DiffusionBridge}.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Diffusion_Bridge_Implicit_Models.pdf",
      "sha_abstract": "02ebb6c664597ebfb273f873b506b1c97b12ea6ef9940f138a945d4be57ee4ae",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199033070
    },
    {
      "_id": "PeikD5oBclM7MZc3pI9D",
      "title": "Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis",
      "title_normalized": "convergence_of_scorebased_discrete_diffusion_models_a_discretetime_analysis",
      "authors": [
        "Zikun Zhang",
        "Zixiang Chen",
        "Quanquan Gu"
      ],
      "abstract": "Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored. In this paper, we study the theoretical aspects of score-based discrete diffusion models under the Continuous Time Markov Chain (CTMC) framework. We introduce a discrete-time sampling algorithm in the general state space $[S]^d$ that utilizes score estimators at predefined time points. We derive convergence bounds for the Kullback-Leibler (KL) divergence and total variation (TV) distance between the generated sample distribution and the data distribution, considering both scenarios with and without early stopping under reasonable assumptions. Notably, our KL divergence bounds are nearly linear in the dimension $d$, aligning with state-of-the-art results for diffusion models. Our convergence analysis employs a Girsanov-based method and establishes key properties of the discrete score function, which are essential for characterizing the discrete-time sampling process.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Convergence_of_Score-Based_Discrete_Diffusion_Models__A_Discrete-Time_Analysis.pdf",
      "sha_abstract": "f70ec12c35a76a20f14fc445be1ff5527602e424c01ef1a7db22730b99960cbe",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199039550
    },
    {
      "_id": "VOilD5oBclM7MZc3FY8g",
      "title": "Revisiting Large-Scale Non-convex Distributionally Robust Optimization",
      "title_normalized": "revisiting_largescale_nonconvex_distributionally_robust_optimization",
      "authors": [
        "Qi Zhang",
        "Yi Zhou",
        "Simon Khan",
        "Ashley Prater-Bennette",
        "Lixin Shen",
        "Shaofeng Zou"
      ],
      "abstract": "Distributionally robust optimization (DRO) is a powerful technique to train robust machine learning models that perform well under distribution shifts. Compared with empirical risk minimization (ERM), DRO optimizes the expected loss under the worst-case distribution in\nan uncertainty set of distributions. This paper revisits the important problem of DRO with non-convex smooth loss functions. For this problem, Jin et al. (2021) showed that its dual problem is generalized $(L_0, L_1)$-smooth condition and gradient noise satisfies the affine variance condition, designed an algorithm of mini-batch normalized gradient descent with momentum, and proved its convergence and complexity.   In this paper, we show that the dual problem and the gradient noise satisfy simpler yet more precise partially generalized smoothness condition and partially affine variance condition by studying the optimization variable and dual variable separately, which further yields much simpler algorithm design and convergence analysis. We develop a double stochastic gradient descent with clipping (D-SGD-C) algorithm that converges to an $\\epsilon$-stationary point with $\\mathcal O(\\epsilon^{-4})$ gradient complexity, which matches with results in Jin et al. (2021). Our algorithm does not need to use momentum, and the proof is much simpler, thanks to the more precise characterization of partially generalized smoothness and partially affine variance noise. We further design a variance-reduced method that achieves a lower gradient complexity of $\\mathcal O(\\epsilon^{-3})$. Our theoretical results and insights are further verified numerically on a number of tasks, and our algorithms outperform the existing DRO method (Jin et al., 2021).",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Revisiting_Large-Scale_Non-convex_Distributionally_Robust_Optimization.pdf",
      "sha_abstract": "ac00ec278b8204c377f5fbc94e399e002c0ccf7de73aef0ca04609c4ea99a211",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199068434
    },
    {
      "_id": "UeilD5oBclM7MZc3CI-g",
      "title": "Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form",
      "title_normalized": "nearoptimal_policy_identification_in_robust_constrained_markov_decision_processes_via_epigraph_form",
      "authors": [
        "Toshinori Kitamura",
        "Tadashi Kozuno",
        "Wataru Kumagai",
        "Kenta Hoshino",
        "Yohei Hosoe",
        "Kazumi Kasaura",
        "Masashi Hamaya",
        "Paavo Parmas",
        "Yutaka Matsuo"
      ],
      "abstract": "Designing a safe policy for uncertain environments is crucial in real-world control systems. However, this challenge remains inadequately addressed within the Markov decision process (MDP) framework. This paper presents the first algorithm guaranteed to identify a near-optimal policy in a robust constrained MDP (RCMDP), where an optimal policy minimizes cumulative cost while satisfying constraints in the worst-case scenario across a set of environments. We first prove that the conventional policy gradient approach to the Lagrangian max-min formulation can become trapped in suboptimal solutions. This occurs when its inner minimization encounters a sum of conflicting gradients from the objective and constraint functions. To address this, we leverage the epigraph form of the RCMDP problem, which resolves the conflict by selecting a single gradient from either the objective or the constraints. Building on the epigraph form, we propose a bisection search algorithm with a policy gradient subroutine and prove that it identifies an $\\varepsilon$-optimal policy in an RCMDP with $\\widetilde{\\mathcal{O}}(\\varepsilon^{-4})$ robust policy evaluations.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Near-Optimal_Policy_Identification_in_Robust_Constrained_Markov_Decision_Processes_via_Epigraph_Form.pdf",
      "sha_abstract": "ab656bc49cbf15df676779a7d6e0fc62a27051b83a0676233c69972e7358057e",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199065235
    },
    {
      "_id": "WOilD5oBclM7MZc3KY_x",
      "title": "SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process",
      "title_normalized": "separate_a_simple_lowrank_projection_for_gradient_compression_in_modern_largescale_model_training_process",
      "authors": [
        "Hanzhen Zhao",
        "Xingyu Xie",
        "Cong Fang",
        "Zhouchen Lin"
      ],
      "abstract": "Training Large Language Models (LLMs) presents a significant communication bottleneck, predominantly due to the growing scale of the gradient to communicate across multi-device clusters. However, how to mitigate communication overhead in practice remains a formidable challenge due to the weakness of the methodology of the existing compression methods, especially the neglect of the characteristics of the gradient. In this paper, we consider and demonstrate the low-rank properties of gradient and Hessian observed in LLMs training dynamic, and take advantage of such natural properties to design SEPARATE, a simple low-rank projection for gradient compression in modern large-scale model training processes. SEPARATE realizes dimensional reduction by common random Gaussian variables and an improved moving average error-feedback technique. We theoretically demonstrate that SEPARATE-based optimizers maintain the original convergence rate for SGD and Adam-Type optimizers for general non-convex objectives. Experimental results show that SEPARATE accelerates training speed by up to 2× for GPT-2-Medium pre-training, and improves performance on various benchmarks for LLAMA2-7B fine-tuning.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "SEPARATE__A_Simple_Low-rank_Projection_for_Gradient_Compression_in_Modern_Large-scale_Model_Training_Process.pdf",
      "sha_abstract": "9c623cc84ba079ff04f49d648e40456fc65b7a9e8672541697d04b921b6fc25e",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199073770
    },
    {
      "_id": "U-ilD5oBclM7MZc3EI_c",
      "title": "Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression",
      "title_normalized": "optimality_and_adaptivity_of_deep_neural_features_for_instrumental_variable_regression",
      "authors": [
        "Juno Kim",
        "Dimitri Meunier",
        "Arthur Gretton",
        "Taiji Suzuki",
        "Zhu Li"
      ],
      "abstract": "We provide a convergence analysis of \\emph{deep feature instrumental variable} (DFIV) regression (Xu et al., 2021), a nonparametric approach to IV regression using data-adaptive features learned by deep neural networks in two stages. We prove that the DFIV algorithm achieves the minimax optimal learning rate when the target structural function lies in a Besov space. This is shown under standard nonparametric IV assumptions, and an additional smoothness assumption on the regularity of the conditional distribution of the covariate given the instrument, which controls the difficulty of Stage 1. We further demonstrate that DFIV, as a data-adaptive algorithm, is superior to fixed-feature (kernel or sieve) IV methods in two ways. First, when the target function possesses low spatial homogeneity (i.e., it has both smooth and spiky/discontinuous regions), DFIV still achieves the optimal rate, while fixed-feature methods are shown to be strictly suboptimal. Second, comparing with kernel-based two-stage regression estimators, DFIV is provably more data efficient in the Stage 1 samples.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Optimality_and_Adaptivity_of_Deep_Neural_Features_for_Instrumental_Variable_Regression.pdf",
      "sha_abstract": "1acb1ec50af45cbaf71b8400d91eea396ea72a87ff43826f87e6a1f7630189a6",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199067350
    },
    {
      "_id": "VeilD5oBclM7MZc3Go_d",
      "title": "Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM",
      "title_normalized": "mufu__multilingual_fused_learning_for_lowresource_translation_with_llm",
      "authors": [
        "Zheng Wei Lim",
        "Nitish Gupta",
        "Honglin Yu",
        "Trevor Cohn"
      ],
      "abstract": "Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which includes a selection of automatically generated multilingual candidates and an instruction to correct inaccurate translations in the prompt. Mufu prompts turn a translation task into a postediting one, and seek to harness the LLM’s reasoning capability with auxiliary translation candidates, from which the model is required to assess the input quality, align the semantics cross-lingually, copy from relevant inputs and override instances that are incorrect. Our experiments on En-XX translations over the Flores-200 dataset show LLMs finetuned against Mufu-style prompts are robust to poor quality auxiliary translation candidates, achieving performance superior to NLLB 1.3B distilled model in 64% of low- and very-low-resource language pairs. We then distill these models to reduce inference cost, while maintaining on average 3.1 chrF improvement over finetune-only baseline in low-resource translations.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Mufu___Multilingual_Fused_Learning_for_Low-Resource_Translation_with_LLM.pdf",
      "sha_abstract": "9e0306a186210d6b5127370786b642a629bff95a65fc51ae3a93a9bf7a55f249",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199069910
    },
    {
      "_id": "VuilD5oBclM7MZc3H4-M",
      "title": "ImageFolder: Autoregressive Image Generation with Folded Tokens",
      "title_normalized": "imagefolder_autoregressive_image_generation_with_folded_tokens",
      "authors": [
        "Xiang Li",
        "Kai Qiu",
        "Hao Chen",
        "Jason Kuen",
        "Jiuxiang Gu",
        "Bhiksha Raj",
        "Zhe Lin"
      ],
      "abstract": "Image tokenizers are crucial for visual generative models, \\eg, diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose \\textbf{ImageFolder}, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "ImageFolder__Autoregressive_Image_Generation_with_Folded_Tokens.pdf",
      "sha_abstract": "52f7ec61b5d45de995fbd284e4ebb7b828161d5190ed96ac16e5a08760ec133e",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199071110
    },
    {
      "_id": "WeilD5oBclM7MZc3Lo-p",
      "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning",
      "title_normalized": "can_a_misl_fly_analysis_and_ingredients_for_mutual_information_skill_learning",
      "authors": [
        "Chongyi Zheng",
        "Jens Tuyls",
        "Joanne Peng",
        "Benjamin Eysenbach"
      ],
      "abstract": "Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).\nOur analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Can_a_MISL_Fly__Analysis_and_Ingredients_for_Mutual_Information_Skill_Learning.pdf",
      "sha_abstract": "b2a861b1f2ced633a2e227ab80d96df133177dc809f8b88ee69729fdc2d8e2cb",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199074980
    },
    {
      "_id": "V-ilD5oBclM7MZc3JI-G",
      "title": "Aligning Human Motion Generation with Human Perceptions",
      "title_normalized": "aligning_human_motion_generation_with_human_perceptions",
      "authors": [
        "Haoru Wang",
        "Wentao Zhu",
        "Luyi Miao",
        "Yishu Xu",
        "Feng Gao",
        "Qi Tian",
        "Yizhou Wang"
      ],
      "abstract": "Human motion generation is a critical task with a wide spectrum of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. However, current evaluation metrics often rely on simple heuristics or distribution distances and do not align well with human perceptions. In this work, we propose a data-driven approach to bridge this gap by introducing a large-scale human perceptual evaluation dataset, MotionPercept, and a human motion critic model, MotionCritic, that capture human perceptual preferences. Our critic model offers a more accurate metric for assessing motion quality and could be readily integrated into the motion generation pipeline to enhance generation quality. Extensive experiments demonstrate the effectiveness of our approach in both evaluating and improving the quality of generated human motions by aligning with human perceptions. Code and data are publicly available at https://motioncritic.github.io/.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Aligning_Human_Motion_Generation_with_Human_Perceptions.pdf",
      "sha_abstract": "682aa0887a68dc7c941fccc84c79b4eb0d0599655a55f6abb2be18c81f98d151",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199072384
    },
    {
      "_id": "UuilD5oBclM7MZc3DI_f",
      "title": "Diffusion Transformers for Tabular Data Time Series Generation",
      "title_normalized": "diffusion_transformers_for_tabular_data_time_series_generation",
      "authors": [
        "Fabrizio Garuti",
        "Enver Sangineto",
        "Simone Luetto",
        "Lorenzo Forni",
        "Rita Cucchiara"
      ],
      "abstract": "Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, \ngenerating time series of tabular data, where each element of the series depends on the others,\nremains a largely unexplored domain. \nThis gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series.\nIn this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. \nUsing extensive experiments on six datasets, we show that the proposed approach  outperforms previous work by a large margin.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Diffusion_Transformers_for_Tabular_Data_Time_Series_Generation.pdf",
      "sha_abstract": "c31a6a36017ee539d9da44cb5efb4bdad2194eefe1b74aee98440ede84fb62d4",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199066313
    },
    {
      "_id": "Y-ilD5oBclM7MZc3Xo_o",
      "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
      "title_normalized": "livecodebench_holistic_and_contamination_free_evaluation_of_large_language_models_for_code",
      "authors": [
        "Naman Jain",
        "King Han",
        "Alex Gu",
        "Wen-Ding Li",
        "Fanjia Yan",
        "Tianjun Zhang",
        "Sida Wang",
        "Armando Solar-Lezama",
        "Koushik Sen",
        "Ion Stoica"
      ],
      "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for assessing their capabilities suffering from data contamination, overfitting, saturation, and focus on merely code generation. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which collects new problems over time from contests across three competition platforms, Leetcode, Atcoder, and Codeforces. Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts over six hundred coding problems that were published between May 2023 and Aug 2024. We evaluate over 50 LLMs on LiveCodeBench (LCB for brevity) presenting the largest evaluation study of code LLMs on competition problems. Based on the study, we present novel empirical findings on contamination, overfitting, and holistic evaluations. We demonstrate that time-segmented evaluations serve as a robust approach to evade contamination; they are successful at detecting contamination across a wide range of open and closed models including GPT-4O, Claude, Deepseek, and Codestral. Next, we highlight overfitting and saturation of traditional coding benchmarks like HumanEvla and demonstrate LCB allows more reliable evaluations. Finally, our holistic evaluation scenarios allow for measuring the different capabilities of programming agents in isolation.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "LiveCodeBench__Holistic_and_Contamination_Free_Evaluation_of_Large_Language_Models_for_Code.pdf",
      "sha_abstract": "addae2285a052467c9db4a025661a67521342994cb4d6a4642502e74d7166074",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199087330
    },
    {
      "_id": "aOilD5oBclM7MZc3e4-A",
      "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
      "title_normalized": "chatqa_2_bridging_the_gap_to_proprietary_llms_in_long_context_and_rag_capabilities",
      "authors": [
        "Peng Xu",
        "Wei Ping",
        "Xianchao Wu",
        "Chejian Xu",
        "Zihan Liu",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
      ],
      "abstract": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and\nleading proprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context un-\nderstanding and retrieval-augmented generation (RAG) capabilities. These two\ncapabilities are complementary to each other and essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt. We present\na detailed continued training recipe to extend the context window of Llama3-\n70B-base from 8K to 128K tokens, along with a three-stage instruction tun-\ning process to enhance the model’s instruction-following, RAG performance,\nand long-context understanding capabilities. Our results demonstrate that the\nLlama3-ChatQA-2-70B model outperforms most existing state-of-the-art models,\nincluding GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-\nInstruct, on ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark\nusing only a 4K context window, showing the strong long context capability across\nvarying sequence lengths. We further provide extensive comparisons between\ndirect long-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution using\nthe same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B and\nQwen2-72B-Instruct) on both 32K and 128K benchmarks. We open-source the\nmodel weights, training data, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "ChatQA_2__Bridging_the_Gap_to_Proprietary_LLMs_in_Long_Context_and_RAG_Capabilities.pdf",
      "sha_abstract": "7cb6dbfa45647dad72507a06bb7d8fcbe07816050d3642130487159f647e3f7c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199094650
    },
    {
      "_id": "aeilD5oBclM7MZc3go_f",
      "title": "Image-level Memorization Detection via Inversion-based Inference Perturbation",
      "title_normalized": "imagelevel_memorization_detection_via_inversionbased_inference_perturbation",
      "authors": [
        "Yue Jiang",
        "Haokun Lin",
        "Yang Bai",
        "Bo Peng",
        "Zhili Liu",
        "Yueming Lyu",
        "Yong Yang",
        "Xingzheng",
        "Jing Dong"
      ],
      "abstract": "Recent studies have discovered that widely used text-to-image diffusion models can replicate training samples during image generation, a phenomenon known as memorization. Existing detection methods primarily focus on identifying memorized prompts. However, in real-world scenarios, image owners may need to verify whether their proprietary or personal images have been memorized by the model, even in the absence of paired prompts or related metadata. We refer to this challenge as image-level memorization detection, where current methods relying on original prompts fall short. In this work, we uncover two characteristics of memorized images after perturbing the inference procedure: lower similarity of the original images and larger magnitudes of TCNP.\nBuilding on these insights, we propose Inversion-based Inference Perturbation (IIP), a new framework for image-level memorization detection. Our approach uses unconditional DDIM inversion to derive latent codes that contain core semantic information of original images and optimizes random prompt embeddings to introduce effective perturbation. Memorized images exhibit distinct characteristics within the proposed pipeline, providing a robust basis for detection. To support this task, we construct a comprehensive setup for the image-level memorization detection, carefully curating datasets to simulate realistic memorization scenarios. Using this setup, we evaluate our IIP framework across three different memorization settings, demonstrating its state-of-the-art performance in identifying memorized images in various settings, even in the presence of data augmentation attacks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Image-level_Memorization_Detection_via_Inversion-based_Inference_Perturbation.pdf",
      "sha_abstract": "64b0fcaedb9f52a834e8c867294bc80a9fcd3896a15024117c22691f983bcef8",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199096530
    },
    {
      "_id": "auilD5oBclM7MZc3iI88",
      "title": "gRNAde: Geometric Deep Learning for 3D RNA inverse design",
      "title_normalized": "grnade_geometric_deep_learning_for_3d_rna_inverse_design",
      "authors": [
        "Chaitanya K. Joshi",
        "Arian Rokkum Jamasb",
        "Ramon Viñas Torné",
        "Charles Harris",
        "Simon V Mathis",
        "Alex Morehead",
        "Rishabh Anand",
        "Pietro Lio"
      ],
      "abstract": "Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics. gRNAde uses a multi-state Graph Neural Network and autoregressive decoding to generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown. On a single-state fixed backbone re-design benchmark of 14 RNA structures from the PDB identified by Das et al. (2010), gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent ribozyme. Experimental wet lab validation on 10 different structured RNA backbones finds that gRNAde has a success rate of 50% at designing pseudoknotted RNA structures, a significant advance over 35% for Rosetta. Open source code and tutorials are available at: github.com/chaitjo/geometric-rna-design",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "gRNAde__Geometric_Deep_Learning_for_3D_RNA_inverse_design.pdf",
      "sha_abstract": "0c31c653ef5c9e6068f4776a9542c5baef22f8e0ba7a7402670054fb8c72d432",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199097910
    },
    {
      "_id": "Z-ilD5oBclM7MZc3do-8",
      "title": "SpinQuant: LLM Quantization with Learned Rotations",
      "title_normalized": "spinquant_llm_quantization_with_learned_rotations",
      "authors": [
        "Zechun Liu",
        "Changsheng Zhao",
        "Igor Fedorov",
        "Bilge Soran",
        "Dhruv Choudhary",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yuandong Tian",
        "Tijmen Blankevoort"
      ],
      "abstract": "Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot. Code is available at https://github.com/facebookresearch/SpinQuant.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "SpinQuant__LLM_Quantization_with_Learned_Rotations.pdf",
      "sha_abstract": "19a1e9b0210f36ee3a5ac1980fde673f6d4c0474e3b08532ec06c9cf1447cc1f",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199093430
    },
    {
      "_id": "ZOilD5oBclM7MZc3Zo8G",
      "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
      "title_normalized": "pyramidal_flow_matching_for_efficient_video_generative_modeling",
      "authors": [
        "Yang Jin",
        "Zhicheng Sun",
        "Ningyuan Li",
        "Kun Xu",
        "Kun Xu",
        "Hao Jiang",
        "Nan Zhuang",
        "Quzhe Huang",
        "Yang Song",
        "Yadong MU",
        "Zhouchen Lin"
      ],
      "abstract": "Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models are open-sourced at https://pyramid-flow.github.io.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Pyramidal_Flow_Matching_for_Efficient_Video_Generative_Modeling.pdf",
      "sha_abstract": "fc51f69814967dd25ebdc0060f19aefd8f3b4b9b096d373d6e2a1357fe41869f",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199089150
    },
    {
      "_id": "ZeilD5oBclM7MZc3a49o",
      "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models",
      "title_normalized": "dellma_decision_making_under_uncertainty_with_large_language_models",
      "authors": [
        "Ollie Liu",
        "Deqing Fu",
        "Dani Yogatama",
        "Willie Neiswanger"
      ],
      "abstract": "The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of *decision-making under uncertainty*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling *inference-time reasoning*, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "DeLLMa__Decision_Making_Under_Uncertainty_with_Large_Language_Models.pdf",
      "sha_abstract": "5f60da8f5660a3f9ca08ee81a0d93c937ab3438df7d1b37302b35f8d5d95b937",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199090530
    },
    {
      "_id": "a-ilD5oBclM7MZc3jI96",
      "title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence",
      "title_normalized": "a_policygradient_approach_to_solving_imperfectinformation_games_with_bestiterate_convergence",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman E. Ozdaglar"
      ],
      "abstract": "Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating \\emph{counterfactual} values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "A_Policy-Gradient_Approach_to_Solving_Imperfect-Information_Games_with_Best-Iterate_Convergence.pdf",
      "sha_abstract": "dd9845131d8bb2a36793379f26ab197675b30b0e5e8bd9ea51e564092fc8488e",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199098997
    },
    {
      "_id": "ZuilD5oBclM7MZc3cI8r",
      "title": "JPEG Inspired Deep Learning",
      "title_normalized": "jpeg_inspired_deep_learning",
      "authors": [
        "Ahmed H. Salamah",
        "Kaixiang Zheng",
        "Yiwen Liu",
        "EN-HUI YANG"
      ],
      "abstract": "Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in comparison with the standard DL,  JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is available on https://github.com/AhmedHussKhalifa/JPEG-Inspired-DL.git.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "JPEG_Inspired_Deep_Learning.pdf",
      "sha_abstract": "15e3779394814e5f249cf98e8a04665437e795ff9a6da8eeb1c6577509aa4092",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199091750
    },
    {
      "_id": "bOilD5oBclM7MZc3kY8D",
      "title": "Boltzmann priors for Implicit Transfer Operators",
      "title_normalized": "boltzmann_priors_for_implicit_transfer_operators",
      "authors": [
        "Juan Viguera Diez",
        "Mathias Jacob Schreiner",
        "Ola Engkvist",
        "Simon Olsson"
      ],
      "abstract": "Accurate prediction of thermodynamic properties is essential in drug discovery and materials science. Molecular dynamics (MD) simulations provide a principled approach to this task, yet they typically rely on prohibitively long sequential simulations. Implicit Transfer Operator (ITO) Learning offers a promising approach to address this limitation by enabling stable simulation with time steps orders of magnitude larger than MD. However, to train ITOs, we need extensive, unbiased MD data, limiting the scope of this framework. Here, we introduce Boltzmann Priors for ITO (BoPITO) to enhance ITO learning in two ways. First, BoPITO enables more efficient data generation, and second, it embeds inductive biases for long-term dynamical behavior, simultaneously improving sample efficiency by one order of magnitude and guaranteeing asymptotically unbiased equilibrium statistics. Furthermore, we showcase the use of BoPITO in a new tunable sampling protocol interpolating between ITOs trained on off-equilibrium simulations and an equilibrium model by incorporating unbiased correlation functions. Code is available at https://github.com/olsson-group/bopito.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Boltzmann_priors_for_Implicit_Transfer_Operators.pdf",
      "sha_abstract": "90396f6e0f64a54399caf017a2ae9275b223e16c1741405a6b430255cad1f04f",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199100157
    },
    {
      "_id": "beilD5oBclM7MZc3lY_d",
      "title": "Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering",
      "title_normalized": "conformal_generative_modeling_with_improved_sample_efficiency_through_sequential_greedy_filtering",
      "authors": [
        "Klaus-Rudolf Kladny",
        "Bernhard Schölkopf",
        "Michael Muehlebach"
      ],
      "abstract": "Generative models lack rigorous statistical guarantees with respect to their predictions. In this work, we propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a sequential conformal prediction method producing prediction sets that satisfy a rigorous statistical guarantee called conformal admissibility control. This guarantee means that the prediction sets contain at least one admissible (or valid) example, with high probability. To this end, our method first samples an initial set of i.i.d. examples from a black box generative model. Then, this set is iteratively pruned via so-called greedy filters. As a consequence of the iterative generation procedure, admissibility of the final prediction set factorizes as a Markov chain, where each factor can be controlled separately, using conformal prediction. In comparison to prior work, our method demonstrates a large reduction in the number of admissibility evaluations during calibration. This is crucial e.g. in safety-critical applications, where these evaluations must be conducted manually by domain experts and are therefore costly and time consuming. We highlight the advantages of our method in terms of admissibility evaluations and cardinality of the prediction set through experiments in natural language generation and molecular graph extension tasks.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "Conformal_Generative_Modeling_with_Improved_Sample_Efficiency_through_Sequential_Greedy_Filtering.pdf",
      "sha_abstract": "84f6528c37fb81b3321f92c2bbe029ff9ec1497d1eadd737a7215a5b52b87662",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199101397
    },
    {
      "_id": "buioD5oBclM7MZc3dI9X",
      "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?",
      "title_normalized": "how_does_visionlanguage_adaptation_impact_the_safety_of_vision_language_models",
      "authors": [
        "Seongyun Lee",
        "Geewook Kim",
        "Jiyeon Kim",
        "Hyunji Lee",
        "Hoyeon Chang",
        "Sue Hyun Park",
        "Minjoon Seo"
      ],
      "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning methods. Our analysis reveals that safety degradation occurs during VL adaptation, even when the training data is safe. While safety tuning techniques like supervised fine-tuning with safety datasets or reinforcement learning from human feedback mitigate some risks, they still lead to safety degradation and a reduction in helpfulness due to over-rejection issues. Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels. Additionally, our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal. To address this, we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness. These insights help guide the development of more reliable and secure LVLMs for real-world applications.",
      "date": "2024-10-04",
      "s3_bucket": "llm-research-papers",
      "s3_key": "How_Does_Vision-Language_Adaptation_Impact_the_Safety_of_Vision_Language_Models_.pdf",
      "sha_abstract": "b38cf3e78d39c9ddbed4a9d7f0b96028f03f425c0cbd46165357b1f203366a9d",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761199289410
    },
    {
      "_id": "b-jHD5oBclM7MZc3Eo9y",
      "title": "AIOpsLab  A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds",
      "title_normalized": "aiopslab__a_holistic_framework_to_evaluate_ai_agents_for_enabling_autonomous_clouds",
      "authors": [
        "Unknown"
      ],
      "abstract": "Research paper from unknown conference",
      "date": "2025-10-22",
      "s3_bucket": "llm-research-papers",
      "s3_key": "AIOpsLab__A_Holistic_Framework_to_Evaluate_AI_Agents_for_Enabling_Autonomous_Clouds.pdf",
      "sha_abstract": "6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761201295873
    },
    {
      "_id": "cOjHD5oBclM7MZc3Eo__",
      "title": "AI Metropolis  Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution",
      "title_normalized": "ai_metropolis__scaling_large_language_modelbased_multiagent_simulation_with_outoforder_execution",
      "authors": [
        "Unknown"
      ],
      "abstract": "Research paper from unknown conference",
      "date": "2025-10-22",
      "s3_bucket": "llm-research-papers",
      "s3_key": "AI_Metropolis__Scaling_Large_Language_Model-based_Multi-Agent_Simulation_with_Out-of-order_Execution.pdf",
      "sha_abstract": "6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761201296114
    },
    {
      "_id": "cejHD5oBclM7MZc3FY9Z",
      "title": "APOLLO  SGD-like Memory  AdamW-level Performance",
      "title_normalized": "apollo__sgdlike_memory__adamwlevel_performance",
      "authors": [
        "Unknown"
      ],
      "abstract": "Research paper from unknown conference",
      "date": "2025-10-22",
      "s3_bucket": "llm-research-papers",
      "s3_key": "APOLLO__SGD-like_Memory__AdamW-level_Performance.pdf",
      "sha_abstract": "6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c",
      "decision": "accept",
      "reason": "testing purposes",
      "relevance": "yes",
      "novelty": "yes",
      "ingested_at": 1761201296703
    }
  ]
}