{"_id": "5-j63JkBP8oloYi_6iI5", "title": "A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers", "cluster": 2, "preview": "A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers Storage systems account for a major portion of the total cost of ownership (TCO) of warehouse-scale computers, and thus have a major impact on the "}
{"_id": "6uj63JkBP8oloYi_7yKD", "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking", "cluster": 2, "preview": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower. This is unfortunate for large language model (LLM) token gener"}
{"_id": "4ujn3JkBP8oloYi_ySIt", "title": "Universal Image Restoration Pre-training via Degradation Classification", "cluster": 5, "preview": "Universal Image Restoration Pre-training via Degradation Classification This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal"}
{"_id": "7Oj63JkBP8oloYi_8yJ4", "title": "Youmu: Efficient Columnar Data Pipeline for LLM Training", "cluster": 2, "preview": "Youmu: Efficient Columnar Data Pipeline for LLM Training Large language models (LLMs) training is extremely data-intensive, often involving over trillion-level tokens. Although LLM datasets are usually ingested and stored in columnar format"}
{"_id": "5ujn3JkBP8oloYi_2CLD", "title": "Framer: Interactive Frame Interpolation", "cluster": 1, "preview": "Framer: Interactive Frame Interpolation We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end fra"}
{"_id": "5Ojn3JkBP8oloYi_1yJI", "title": "Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks", "cluster": 3, "preview": "Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when consid"}
{"_id": "4-jn3JkBP8oloYi_1SKS", "title": "On the Benefits of Attribute-Driven Graph Domain Adaptation", "cluster": 3, "preview": "On the Benefits of Attribute-Driven Graph Domain Adaptation Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Rece"}
{"_id": "6ej63JkBP8oloYi_7SI_", "title": "LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers", "cluster": 2, "preview": "LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers Transformer-based large language models are memory hungry and incur significant inference latencies even on cutting edge AI-accelerators, such a"}
{"_id": "6-j63JkBP8oloYi_8CJH", "title": "TurboAttention: Efficient attention approximation for high throughputs llm", "cluster": 2, "preview": "TurboAttention: Efficient attention approximation for high throughputs llm Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanisms. While techniques, such as quan"}
{"_id": "5ejn3JkBP8oloYi_2CIm", "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal", "cluster": 5, "preview": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing eva"}
{"_id": "6Oj63JkBP8oloYi_7CKj", "title": "Scaling Deep Learning Training with MPMD Pipeline Parallelism", "cluster": 0, "preview": "Scaling Deep Learning Training with MPMD Pipeline Parallelism We present JaxPP, a system for efficiently scaling the training of large deep learning models with flexible pipeline parallelism. We introduce a seamless programming model that a"}
{"_id": "7ej63JkBP8oloYi_9iKE", "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "cluster": 2, "preview": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As "}
{"_id": "8Oj73JkBP8oloYi_AiJV", "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine", "cluster": 2, "preview": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine Language models for scientific tasks are trained on text from scientific publications---most distributed as PDFs that require parsing. PDF parsing approaches range from "}
{"_id": "7-j63JkBP8oloYi__SJW", "title": "FLStore: Efficient Federated Learning Storage for non-training workloads", "cluster": 2, "preview": "FLStore: Efficient Federated Learning Storage for non-training workloads Federated Learning (FL) is an approach for privacy-preserving Machine Learning (ML), enabling model training across multiple clients without centralized data collectio"}
{"_id": "EeiaD5oBclM7MZc3c4-d", "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models", "cluster": 6, "preview": "Aligned Better, Listen Better for Audio-Visual Large Language Models Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video"}
{"_id": "7uj63JkBP8oloYi_-CJp", "title": "Efficient On-Device Machine Learning with a Biologically-Plausible Forward-Only Algorithm", "cluster": 1, "preview": "Efficient On-Device Machine Learning with a Biologically-Plausible Forward-Only Algorithm The training of the state-of-the-art Deep Neural Networks (DNNs) consumes massive amounts of energy, while the human brain learns new tasks with remar"}
{"_id": "EOiaD5oBclM7MZc3bo84", "title": "CTSyn: A Foundation Model for Cross Tabular Data Generation", "cluster": 2, "preview": "CTSyn: A Foundation Model for Cross Tabular Data Generation Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presen"}
{"_id": "MeikD5oBclM7MZc3Zo_A", "title": "Bridging the Data Provenance Gap Across Text, Speech, and Video", "cluster": 2, "preview": "Bridging the Data Provenance Gap Across Text, Speech, and Video Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established "}
{"_id": "LeikD5oBclM7MZc3Uo8Y", "title": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation", "cluster": 6, "preview": "Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning c"}
{"_id": "NeikD5oBclM7MZc3e49F", "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials", "cluster": 2, "preview": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop softwa"}
{"_id": "MOikD5oBclM7MZc3Yo88", "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs", "cluster": 2, "preview": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the cruc"}
{"_id": "L-ikD5oBclM7MZc3XY-7", "title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors", "cluster": 1, "preview": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in "}
{"_id": "LuikD5oBclM7MZc3Vo9h", "title": "Learning mirror maps in policy mirror descent", "cluster": 2, "preview": "Learning mirror maps in policy mirror descent Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the sel"}
{"_id": "M-ikD5oBclM7MZc3cY-R", "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness", "cluster": 1, "preview": "Improved Diffusion-based Generative Model with Better Adversarial Robustness Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue o"}
{"_id": "NOikD5oBclM7MZc3do-f", "title": "Generative Classifiers Avoid Shortcut Solutions", "cluster": 5, "preview": "Generative Classifiers Avoid Shortcut Solutions Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on featur"}
{"_id": "MuikD5oBclM7MZc3bI8S", "title": "Decoupled Subgraph Federated Learning", "cluster": 3, "preview": "Decoupled Subgraph Federated Learning We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-con"}
{"_id": "RuikD5oBclM7MZc3z4_z", "title": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View", "cluster": 0, "preview": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total n"}
{"_id": "Q-ikD5oBclM7MZc3wo-_", "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization", "cluster": 2, "preview": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. Howev"}
{"_id": "QOikD5oBclM7MZc3sY-1", "title": "Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping", "cluster": 1, "preview": "Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it wa"}
{"_id": "ReikD5oBclM7MZc3y49p", "title": "Towards Learning High-Precision Least Squares Algorithms with Sequence Models", "cluster": 1, "preview": "Towards Learning High-Precision Least Squares Algorithms with Sequence Models This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our"}
{"_id": "QeikD5oBclM7MZc3to-e", "title": "Large (Vision) Language Models are Unsupervised In-Context Learners", "cluster": 6, "preview": "Large (Vision) Language Models are Unsupervised In-Context Learners Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various ad"}
{"_id": "ROikD5oBclM7MZc3xo-n", "title": "Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable", "cluster": 6, "preview": "Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable Modern language models have demonstrated remarkable reasoning capabilities by using chain-of-thought (CoT). One hypothesis about the inner workings of CoT is that it bre"}
{"_id": "QuikD5oBclM7MZc3vo9L", "title": "ThunderKittens: Simple, Fast, and $\\textit{Adorable}$ Kernels", "cluster": 4, "preview": "ThunderKittens: Simple, Fast, and $\\textit{Adorable}$ Kernels The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet "}
{"_id": "P-ikD5oBclM7MZc3rY9_", "title": "Do LLMs estimate uncertainty well in instruction-following?", "cluster": 6, "preview": "Do LLMs estimate uncertainty well in instruction-following? Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown s"}
{"_id": "R-ikD5oBclM7MZc31I9k", "title": "nGPT: Normalized Transformer with Representation Learning on the Hypersphere", "cluster": 7, "preview": "nGPT: Normalized Transformer with Representation Learning on the Hypersphere We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the"}
{"_id": "T-ikD5oBclM7MZc3_o9s", "title": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars", "cluster": 0, "preview": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometr"}
{"_id": "SuikD5oBclM7MZc35I-p", "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers", "cluster": 6, "preview": "Reasoning with Latent Thoughts: On the Power of Looped Transformers Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver."}
{"_id": "TuikD5oBclM7MZc3-Y-8", "title": "InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation", "cluster": 1, "preview": "InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc"}
{"_id": "UOilD5oBclM7MZc3BI92", "title": "MAP: Multi-Human-Value Alignment Palette", "cluster": 6, "preview": "MAP: Multi-Human-Value Alignment Palette Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be p"}
{"_id": "S-ikD5oBclM7MZc36Y9Y", "title": "PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark", "cluster": 6, "preview": "PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark Despite extensive research, recovering PDE expressions from experimental observations often involves symbolic regression. This method generally lacks the incorp"}
{"_id": "SeikD5oBclM7MZc33o9B", "title": "Homomorphism Counts as Structural Encodings for Graph Learning", "cluster": 3, "preview": "Homomorphism Counts as Structural Encodings for Graph Learning Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on"}
{"_id": "TOikD5oBclM7MZc37Y8v", "title": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity", "cluster": 4, "preview": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically -- previous convergence analysis for Adam and "}
{"_id": "TeikD5oBclM7MZc38Y_K", "title": "SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs", "cluster": 3, "preview": "SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs We present a novel framework, StochastIc Network Graph Evolving operatoR (SINGER), for learning the evolution operator of high-dimensional partial differential equ"}
{"_id": "SOikD5oBclM7MZc32I_1", "title": "A Coefficient Makes SVRG Effective", "cluster": 0, "preview": "A Coefficient Makes SVRG Effective Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in dee"}
{"_id": "XeilD5oBclM7MZc3Qo8o", "title": "MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine", "cluster": 6, "preview": "MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine Multi-modal Large Language Models (MLLMs) have recently showcased superior proficiency in general visual scenarios. However, we identify their mathematical capabili"}
{"_id": "XOilD5oBclM7MZc3PY8B", "title": "Number Cookbook: Number Understanding of Language Models and How to Improve It", "cluster": 6, "preview": "Number Cookbook: Number Understanding of Language Models and How to Improve It Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and proc"}
{"_id": "YeilD5oBclM7MZc3Vo8V", "title": "Attention with Markov: A Curious Case of Single-layer Transformers", "cluster": 7, "preview": "Attention with Markov: A Curious Case of Single-layer Transformers Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential mo"}
{"_id": "XuilD5oBclM7MZc3R49B", "title": "TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice", "cluster": 2, "preview": "TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters. The effec"}
{"_id": "YuilD5oBclM7MZc3Wo8P", "title": "The AdEMAMix Optimizer: Better, Faster, Older", "cluster": 7, "preview": "The AdEMAMix Optimizer: Better, Faster, Older Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the p"}
{"_id": "X-ilD5oBclM7MZc3S491", "title": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling", "cluster": 2, "preview": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions t"}
{"_id": "W-ilD5oBclM7MZc3N4-c", "title": "Three Mechanisms of Feature Learning in a Linear Network", "cluster": 7, "preview": "Three Mechanisms of Feature Learning in a Linear Network Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamic"}
{"_id": "WuilD5oBclM7MZc3M49Q", "title": "Affine Steerable Equivariant Layer for Canonicalization of Neural Networks", "cluster": 7, "preview": "Affine Steerable Equivariant Layer for Canonicalization of Neural Networks In the field of equivariant networks, achieving affine equivariance, particularly for general group representations, has long been a challenge. In this paper, we pro"}
{"_id": "YOilD5oBclM7MZc3UY8A", "title": "Remove Symmetries to Control Model Expressivity and Improve Optimization", "cluster": 2, "preview": "Remove Symmetries to Control Model Expressivity and Improve Optimization When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a ``collapse.\" Being trapped in the"}
{"_id": "F-ijD5oBclM7MZc3wo-T", "title": "The 3D-PC: a benchmark for visual perspective taking in humans and machines", "cluster": 0, "preview": "The 3D-PC: a benchmark for visual perspective taking in humans and machines Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which deve"}
{"_id": "FeiaD5oBclM7MZc3jI-A", "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory", "cluster": 4, "preview": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces sign"}
{"_id": "E-iaD5oBclM7MZc3f4-m", "title": "From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle", "cluster": 5, "preview": "From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we intro"}
{"_id": "EuiaD5oBclM7MZc3eY9c", "title": "MiniPLM: Knowledge Distillation for Pre-training Language Models", "cluster": 2, "preview": "MiniPLM: Knowledge Distillation for Pre-training Language Models Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre"}
{"_id": "GOijD5oBclM7MZc3yY-C", "title": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding", "cluster": 5, "preview": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-t"}
{"_id": "FuijD5oBclM7MZc3uo-H", "title": "Discrete Codebook World Models for Continuous Control", "cluster": 2, "preview": "Discrete Codebook World Models for Continuous Control In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While "}
{"_id": "GuijD5oBclM7MZc30o9Y", "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts", "cluster": 0, "preview": "miniCTX: Neural Theorem Proving with (Long-)Contexts Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\\texttt{miniCTX}$, wh"}
{"_id": "D-iaD5oBclM7MZc3aY_-", "title": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models", "cluster": 2, "preview": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, "}
{"_id": "FOiaD5oBclM7MZc3hY9K", "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon", "cluster": 0, "preview": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization "}
{"_id": "GeijD5oBclM7MZc3zY_j", "title": "Faster Cascades via Speculative Decoding", "cluster": 2, "preview": "Faster Cascades via Speculative Decoding Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches interleave two models, but via fundamentally distinct mechanisms: defer"}
{"_id": "IOikD5oBclM7MZc3BY8Z", "title": "Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers", "cluster": 0, "preview": "Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers An important prerequisite for safe control is aligning the policy with the underlying constraints in the environment. In many real-world applications, due t"}
{"_id": "IeikD5oBclM7MZc3DI_z", "title": "Federated Domain Generalization with Data-free On-server Matching Gradient", "cluster": 3, "preview": "Federated Domain Generalization with Data-free On-server Matching Gradient Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in "}
{"_id": "IuikD5oBclM7MZc3EI_M", "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance", "cluster": 6, "preview": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. St"}
{"_id": "H-ijD5oBclM7MZc394_b", "title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains", "cluster": 2, "preview": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain e"}
{"_id": "G-ijD5oBclM7MZc32498", "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models", "cluster": 5, "preview": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily fo"}
{"_id": "HeijD5oBclM7MZc35I_S", "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models", "cluster": 1, "preview": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) mo"}
{"_id": "HOijD5oBclM7MZc33I-I", "title": "Towards a Unified and Verified Understanding of Group-Operation Networks", "cluster": 7, "preview": "Towards a Unified and Verified Understanding of Group-Operation Networks A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation o"}
{"_id": "I-ikD5oBclM7MZc3GY8j", "title": "Understanding Optimization in Deep Learning with Central Flows", "cluster": 1, "preview": "Understanding Optimization in Deep Learning with Central Flows Optimization in deep learning remains poorly understood. A key difficulty is that optimizers exhibit complex oscillatory dynamics, referred to as \"edge of stability,\" which cann"}
{"_id": "HuijD5oBclM7MZc38I8Q", "title": "Multi-domain Distribution Learning for De Novo Drug Design", "cluster": 0, "preview": "Multi-domain Distribution Learning for De Novo Drug Design We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art perfo"}
{"_id": "JOikD5oBclM7MZc3Ho8j", "title": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters", "cluster": 2, "preview": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations i"}
{"_id": "LOikD5oBclM7MZc3TY_f", "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "cluster": 0, "preview": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting a"}
{"_id": "KOikD5oBclM7MZc3OY-C", "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL", "cluster": 2, "preview": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explore"}
{"_id": "JeikD5oBclM7MZc3J48k", "title": "StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces", "cluster": 1, "preview": "StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360â—¦ panoramas and a mesh surface for texture) using a pret"}
{"_id": "KuikD5oBclM7MZc3RI90", "title": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws", "cluster": 2, "preview": "Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational bud"}
{"_id": "K-ikD5oBclM7MZc3SY8w", "title": "Compute-Optimal LLMs Provably Generalize Better with Scale", "cluster": 2, "preview": "Compute-Optimal LLMs Provably Generalize Better with Scale Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compu"}
{"_id": "JuikD5oBclM7MZc3K4_0", "title": "Data Unlearning in Diffusion Models", "cluster": 1, "preview": "Data Unlearning in Diffusion Models Recent work has shown that diffusion models memorize and reproduce training data examples. At the same time, large copyright lawsuits and legislation such as GDPR have highlighted the need for erasing dat"}
{"_id": "J-ikD5oBclM7MZc3NI9W", "title": "Consistency Models Made Easy", "cluster": 1, "preview": "Consistency Models Made Easy Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8"}
{"_id": "KeikD5oBclM7MZc3P4_b", "title": "Fast Uncovering of Protein Sequence Diversity from Structure", "cluster": 0, "preview": "Fast Uncovering of Protein Sequence Diversity from Structure We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of "}
{"_id": "NuikD5oBclM7MZc3gI88", "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?", "cluster": 7, "preview": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable? As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to "}
{"_id": "OuikD5oBclM7MZc3lI8S", "title": "Beyond Worst-Case Dimensionality Reduction for Sparse Vectors", "cluster": 1, "preview": "Beyond Worst-Case Dimensionality Reduction for Sparse Vectors We study beyond worst-case dimensionality reduction for $s$-sparse vectors (vectors with at most $s$ non-zero coordinates). Our work is divided into two parts, each focusing on a"}
{"_id": "PuikD5oBclM7MZc3qY8c", "title": "Retrieval Head Mechanistically Explains Long-Context Factuality", "cluster": 2, "preview": "Retrieval Head Mechanistically Explains Long-Context Factuality Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitr"}
{"_id": "N-ikD5oBclM7MZc3hI9B", "title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling", "cluster": 1, "preview": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their s"}
{"_id": "OeikD5oBclM7MZc3j48s", "title": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning", "cluster": 5, "preview": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but s"}
{"_id": "POikD5oBclM7MZc3n4_C", "title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems", "cluster": 2, "preview": "Improving Data Efficiency via Curating LLM-Driven Rating Systems Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can"}
{"_id": "O-ikD5oBclM7MZc3mY9W", "title": "Elucidating the Preconditioning in Consistency Distillation", "cluster": 1, "preview": "Elucidating the Preconditioning in Consistency Distillation Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward o"}
{"_id": "OOikD5oBclM7MZc3io_4", "title": "Diffusion Bridge Implicit Models", "cluster": 1, "preview": "Diffusion Bridge Implicit Models Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in ta"}
{"_id": "PeikD5oBclM7MZc3pI9D", "title": "Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis", "cluster": 1, "preview": "Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-"}
{"_id": "VOilD5oBclM7MZc3FY8g", "title": "Revisiting Large-Scale Non-convex Distributionally Robust Optimization", "cluster": 5, "preview": "Revisiting Large-Scale Non-convex Distributionally Robust Optimization Distributionally robust optimization (DRO) is a powerful technique to train robust machine learning models that perform well under distribution shifts. Compared with emp"}
{"_id": "UeilD5oBclM7MZc3CI-g", "title": "Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form", "cluster": 5, "preview": "Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form Designing a safe policy for uncertain environments is crucial in real-world control systems. However, this challenge remains inadequately a"}
{"_id": "WOilD5oBclM7MZc3KY_x", "title": "SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process", "cluster": 5, "preview": "SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process Training Large Language Models (LLMs) presents a significant communication bottleneck, predominantly due to the growing scale of th"}
{"_id": "U-ilD5oBclM7MZc3EI_c", "title": "Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression", "cluster": 0, "preview": "Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression We provide a convergence analysis of \\emph{deep feature instrumental variable} (DFIV) regression (Xu et al., 2021), a nonparametric approach to IV regres"}
{"_id": "VeilD5oBclM7MZc3Go_d", "title": "Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM", "cluster": 2, "preview": "Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-re"}
{"_id": "VuilD5oBclM7MZc3H4-M", "title": "ImageFolder: Autoregressive Image Generation with Folded Tokens", "cluster": 1, "preview": "ImageFolder: Autoregressive Image Generation with Folded Tokens Image tokenizers are crucial for visual generative models, \\eg, diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling."}
{"_id": "WeilD5oBclM7MZc3Lo-p", "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning", "cluster": 3, "preview": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning"}
{"_id": "V-ilD5oBclM7MZc3JI-G", "title": "Aligning Human Motion Generation with Human Perceptions", "cluster": 2, "preview": "Aligning Human Motion Generation with Human Perceptions Human motion generation is a critical task with a wide spectrum of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. However"}
{"_id": "UuilD5oBclM7MZc3DI_f", "title": "Diffusion Transformers for Tabular Data Time Series Generation", "cluster": 3, "preview": "Diffusion Transformers for Tabular Data Time Series Generation Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element"}
{"_id": "Y-ilD5oBclM7MZc3Xo_o", "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code", "cluster": 0, "preview": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia a"}
{"_id": "aOilD5oBclM7MZc3e4-A", "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities", "cluster": 6, "preview": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K context window, designed to bridge the gap between open-source LLMs and leading pr"}
{"_id": "aeilD5oBclM7MZc3go_f", "title": "Image-level Memorization Detection via Inversion-based Inference Perturbation", "cluster": 5, "preview": "Image-level Memorization Detection via Inversion-based Inference Perturbation Recent studies have discovered that widely used text-to-image diffusion models can replicate training samples during image generation, a phenomenon known as memor"}
{"_id": "auilD5oBclM7MZc3iI88", "title": "gRNAde: Geometric Deep Learning for 3D RNA inverse design", "cluster": 0, "preview": "gRNAde: Geometric Deep Learning for 3D RNA inverse design Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conform"}
{"_id": "Z-ilD5oBclM7MZc3do-8", "title": "SpinQuant: LLM Quantization with Learned Rotations", "cluster": 2, "preview": "SpinQuant: LLM Quantization with Learned Rotations Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), bu"}
{"_id": "ZOilD5oBclM7MZc3Zo8G", "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling", "cluster": 2, "preview": "Pyramidal Flow Matching for Efficient Video Generative Modeling Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing appro"}
{"_id": "ZeilD5oBclM7MZc3a49o", "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models", "cluster": 2, "preview": "DeLLMa: Decision Making Under Uncertainty with Large Language Models The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often f"}
{"_id": "a-ilD5oBclM7MZc3jI96", "title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence", "cluster": 1, "preview": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable propertie"}
{"_id": "ZuilD5oBclM7MZc3cI8r", "title": "JPEG Inspired Deep Learning", "cluster": 7, "preview": "JPEG Inspired Deep Learning Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted"}
{"_id": "bOilD5oBclM7MZc3kY8D", "title": "Boltzmann priors for Implicit Transfer Operators", "cluster": 1, "preview": "Boltzmann priors for Implicit Transfer Operators Accurate prediction of thermodynamic properties is essential in drug discovery and materials science. Molecular dynamics (MD) simulations provide a principled approach to this task, yet they "}
{"_id": "beilD5oBclM7MZc3lY_d", "title": "Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering", "cluster": 2, "preview": "Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering Generative models lack rigorous statistical guarantees with respect to their predictions. In this work, we propose Sequential Conformal Predic"}
{"_id": "buioD5oBclM7MZc3dI9X", "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?", "cluster": 2, "preview": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models? Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this pro"}
{"_id": "b-jHD5oBclM7MZc3Eo9y", "title": "AIOpsLab  A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds", "cluster": 4, "preview": "AIOpsLab A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds Research paper from unknown conference"}
{"_id": "cOjHD5oBclM7MZc3Eo__", "title": "AI Metropolis  Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution", "cluster": 4, "preview": "AI Metropolis Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution Research paper from unknown conference"}
{"_id": "cejHD5oBclM7MZc3FY9Z", "title": "APOLLO  SGD-like Memory  AdamW-level Performance", "cluster": 4, "preview": "APOLLO SGD-like Memory AdamW-level Performance Research paper from unknown conference"}
