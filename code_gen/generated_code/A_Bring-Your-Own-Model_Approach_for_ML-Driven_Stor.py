"""
A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers

Generated by AWS Bedrock Claude
Paper ID: 5-j63JkBP8oloYi_6iI5
Authors: Chenxi Yang, Yan Li, Martin Maas, Mustafa Uysal, Ubaid Ullah Hafeez, Arif Merchant, Richard McDougall
Generated at: 2025-10-20T00:12:46.372259
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from typing import Tuple, List

# Data Loading and Preprocessing

class StoragePlacementDataset(Dataset):
    """
    Custom dataset class for storage placement data.
    """

    def __init__(self, data: pd.DataFrame):
        """
        Initialize the dataset.

        Args:
            data (pd.DataFrame): Pandas DataFrame containing the dataset.
        """
        self.data = data
        self.features = data.drop('target', axis=1).values
        self.targets = data['target'].values

    def __len__(self):
        """
        Return the length of the dataset.
        """
        return len(self.data)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get the sample at the given index.

        Args:
            idx (int): Index of the sample.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Features and target tensors.
        """
        features = torch.tensor(self.features[idx], dtype=torch.float32)
        target = torch.tensor(self.targets[idx], dtype=torch.float32)
        return features, target

def load_data(data_path: str, batch_size: int, test_size: float = 0.2, random_state: int = 42) -> Tuple[DataLoader, DataLoader]:
    """
    Load and preprocess the data.

    Args:
        data_path (str): Path to the dataset file.
        batch_size (int): Batch size for the data loaders.
        test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.
        random_state (int, optional): Random state for data splitting. Defaults to 42.

    Returns:
        Tuple[DataLoader, DataLoader]: Train and test data loaders.
    """
    data = pd.read_csv(data_path)
    X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=test_size, random_state=random_state)
    train_data = pd.concat([X_train, y_train], axis=1)
    test_data = pd.concat([X_test, y_test], axis=1)

    train_dataset = StoragePlacementDataset(train_data)
    test_dataset = StoragePlacementDataset(test_data)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader

# Model Architecture

class StoragePlacementModel(nn.Module):
    """
    PyTorch model for storage placement.
    """

    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Initialize the model.

        Args:
            input_size (int): Size of the input features.
            hidden_size (int): Size of the hidden layer.
            output_size (int): Size of the output layer.
        """
        super(StoragePlacementModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the model.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        """
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# Training Loop

def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, epochs: int, lr: float, device: str = 'cpu') -> Tuple[List[float], List[float]]:
    """
    Train the model and evaluate on the test set.

    Args:
        model (nn.Module): PyTorch model.
        train_loader (DataLoader): Train data loader.
        test_loader (DataLoader): Test data loader.
        epochs (int): Number of epochs to train.
        lr (float): Learning rate.
        device (str, optional): Device to use for training (e.g., 'cpu' or 'cuda'). Defaults to 'cpu'.

    Returns:
        Tuple[List[float], List[float]]: Lists of train and test losses for each epoch.
    """
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_losses = []
    test_losses = []

    for epoch in range(epochs):
        train_loss = 0.0
        for features, targets in train_loader:
            features, targets = features.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, targets.unsqueeze(1))
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        test_loss = 0.0
        with torch.no_grad():
            for features, targets in test_loader:
                features, targets = features.to(device), targets.to(device)
                outputs = model(features)
                loss = criterion(outputs, targets.unsqueeze(1))
                test_loss += loss.item()

        test_loss /= len(test_loader)
        test_losses.append(test_loss)

        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

    return train_losses, test_losses

# Evaluation

def evaluate(model: nn.Module, data_loader: DataLoader, device: str = 'cpu') -> float:
    """
    Evaluate the model on the given data loader.

    Args:
        model (nn.Module): PyTorch model.
        data_loader (DataLoader): Data loader for evaluation.
        device (str, optional): Device to use for evaluation (e.g., 'cpu' or 'cuda'). Defaults to 'cpu'.

    Returns:
        float: Mean squared error on the evaluation data.
    """
    model.eval()
    criterion = nn.MSELoss()
    mse = 0.0

    with torch.no_grad():
        for features, targets in data_loader:
            features, targets = features.to(device), targets.to(device)
            outputs = model(features)
            loss = criterion(outputs, targets.unsqueeze(1))
            mse += loss.item()

    mse /= len(data_loader)
    return mse

# Visualization

def plot_losses(train_losses: List[float], test_losses: List[float]):
    """
    Plot the train and test losses.

    Args:
        train_losses (List[float]): List of train losses for each epoch.
        test_losses (List[float]): List of test losses for each epoch.
    """
    epochs = range(1, len(train_losses) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, test_losses, label='Test Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Train and Test Losses')
    plt.legend()
    plt.show()

### Usage Example

# Load data
train_loader, test_loader = load_data('storage_data.csv', batch_size=32)

# Create model
model = StoragePlacementModel(input_size=10, hidden_size=64, output_size=1)

# Train model
train_losses, test_losses = train(model, train_loader, test_loader, epochs=100, lr=0.001, device='cuda')

# Evaluate model
test_mse = evaluate(model, test_loader, device='cuda')
print(f'Test MSE: {test_mse:.4f}')

# Plot losses
plot_losses(train_losses, test_losses)

### Key Features
- Data loading and preprocessing with custom dataset class
- Flexible model architecture with configurable input, hidden, and output sizes
- Efficient training loop with PyTorch optimizers and loss functions
- Evaluation of model performance using mean squared error
- Visualization of train and test losses for monitoring training progress
- Comprehensive documentation and type hints for better code readability and maintainability
- Support for both CPU and GPU training
- Modular and reusable code structure for easy integration and extension