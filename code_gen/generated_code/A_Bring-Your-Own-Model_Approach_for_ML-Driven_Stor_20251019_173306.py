"""
A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers

Generated by AWS Bedrock Claude
Paper ID: 5-j63JkBP8oloYi_6iI5
Authors: Chenxi Yang, Yan Li, Martin Maas, Mustafa Uysal, Ubaid Ullah Hafeez, Arif Merchant, Richard McDougall
Generated at: 2025-10-19T17:33:06.092145
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Load and preprocess data
class StoragePlacementDataset(Dataset):
    """
    Custom PyTorch dataset for storage placement data.
    """

    def __init__(self, data_path):
        """
        Initialize the dataset.

        Args:
            data_path (str): Path to the data file.
        """
        self.data = pd.read_csv(data_path)
        self.features = self.data.drop('target', axis=1).values
        self.targets = self.data['target'].values

    def __len__(self):
        """
        Return the length of the dataset.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Get a sample from the dataset.

        Args:
            idx (int): Index of the sample.

        Returns:
            tuple: (features, target) for the given index.
        """
        return self.features[idx], self.targets[idx]

# Load the data
data_path = 'storage_placement_data.csv'
dataset = StoragePlacementDataset(data_path)

# Split the data into train and test sets
train_ratio = 0.8
train_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset) * train_ratio), len(dataset) - int(len(dataset) * train_ratio)])

# Create data loaders
batch_size = 64
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

# Define the model
class StoragePlacementModel(nn.Module):
    """
    Lightweight model for storage placement predictions.
    """

    def __init__(self, input_size, hidden_size, output_size):
        """
        Initialize the model.

        Args:
            input_size (int): Size of the input features.
            hidden_size (int): Size of the hidden layer.
            output_size (int): Size of the output layer.
        """
        super(StoragePlacementModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """
        Forward pass of the model.

        Args:
            x (torch.Tensor): Input features.

        Returns:
            torch.Tensor: Output predictions.
        """
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# Initialize the model
input_size = dataset.features.shape[1]
hidden_size = 64
output_size = 1
model = StoragePlacementModel(input_size, hidden_size, output_size)

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters())

# Training loop
num_epochs = 100
train_losses = []
test_losses = []

for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0.0
    for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):
        optimizer.zero_grad()
        outputs = model(inputs.float())
        loss = criterion(outputs, targets.unsqueeze(1).float())
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_loader)
    train_losses.append(train_loss)

    # Evaluation
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs.float())
            loss = criterion(outputs, targets.unsqueeze(1).float())
            test_loss += loss.item()
    test_loss /= len(test_loader)
    test_losses.append(test_loss)

    # Print progress
    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

# Visualization
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Training and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluation
model.eval()
with torch.no_grad():
    test_preds = []
    test_targets = []
    for inputs, targets in test_loader:
        outputs = model(inputs.float())
        test_preds.extend(outputs.squeeze().tolist())
        test_targets.extend(targets.tolist())

# Calculate evaluation metrics
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(test_targets, test_preds)
r2 = r2_score(test_targets, test_preds)

print(f'Mean Squared Error: {mse:.4f}')
print(f'R-squared: {r2:.4f}')

# Load the data
data_path = 'storage_placement_data.csv'
dataset = StoragePlacementDataset(data_path)

# Split the data into train and test sets
train_ratio = 0.8
train_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset) * train_ratio), len(dataset) - int(len(dataset) * train_ratio)])

# Create data loaders
batch_size = 64
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

# Initialize the model
input_size = dataset.features.shape[1]
hidden_size = 64
output_size = 1
model = StoragePlacementModel(input_size, hidden_size, output_size)

# Train the model
num_epochs = 100
train_model(model, train_loader, test_loader, num_epochs)

# Evaluate the model
evaluate_model(model, test_loader)