paper_id,title,decision,reason,similarity,trainium_compatibility
5OhJW5oBclM7MZc3PpJ9,Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer,accept,"The paper proposes novel surrogate loss functions and algorithms for learning to defer with multiple experts, which is relevant to model architectures and training algorithms. The claims of realizable H-consistency, H-consistency bounds, and Bayes-consistency are novel beyond the state of the art. The methods are expressed in PyTorch/XLA and use standard transformer-compatible operations, making them compatible with AWS Trainium. The paper addresses a distinct problem from the retrieved references.",no,yes
7-hIZpoBclM7MZc3O5OI,High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws,accept,"The paper is relevant as it focuses on knowledge distillation, a key technique in modern ML. It proposes a novel theoretical analysis of knowledge distillation in high-dimensional regression, which is distinct from the previously indexed papers. The method appears compatible with Trainium as it does not mention any hardware-specific requirements.",no,yes
ouhHZpoBclM7MZc305Oo,CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening,accept,"The paper is relevant as it proposes a new multimodal foundation model (CL-MFAP) for molecular property prediction and antibiotic screening, which aligns with the specified areas of interest. The paper claims novelty in its use of contrastive learning and multimodal data, which appears to be a material improvement over the retrieved papers. The abstract suggests the method is compatible with Trainium as it uses standard transformer-based and MLP encoders.",no,yes
ruhHZpoBclM7MZc34pMa,Catastrophic Failure of LLM Unlearning via Quantization,accept,"The paper is relevant as it focuses on improving machine unlearning techniques for large language models, which is a key area of current ML research. The proposed quantization-robust unlearning strategy is novel and distinct from the previously indexed work, which did not address the issue of knowledge recovery through quantization.",no,yes
tehHZpoBclM7MZc37JNW,Discrete Diffusion Schrödinger Bridge Matching for Graph Transformation,accept,"The paper proposes a novel framework called Discrete Diffusion Schrödinger Bridge Matching (DDSBM) for graph transformation, which extends the Iterative Markovian Fitting method to discrete domains and can be applied to molecular optimization. This is distinct from the previously indexed work on diffusion bridge models and optimal transport for 3D molecule generation.",no,yes
EehTW5oBclM7MZc33pMJ,Learning mirror maps in policy mirror descent,accept,"The paper is relevant as it focuses on improving policy mirror descent, a key technique in reinforcement learning. It proposes learning the mirror map, which is novel beyond the state-of-the-art. The abstract indicates the method can be expressed in PyTorch/XLA and is compatible with Trainium. The paper is distinct from the provided references, which do not cover learning the mirror map in policy mirror descent.",no,yes
7ehIZpoBclM7MZc3OZOv,Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning,accept,"The paper is relevant as it focuses on unlearning techniques for LLMs, which is a key area of current ML research. It proposes a novel 'benign relearning' attack that can reverse the effects of unlearning, which is a distinct contribution from the prior work in the RAG. The abstract suggests the method can be implemented using standard ML techniques compatible with Trainium.",no,yes
SOhHZpoBclM7MZc3UpM9,MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization,accept,"The paper proposes a new multi-agent contrastive preference optimization (MACPO) framework for weak-to-strong alignment of large language models, which is relevant to current LLM/AI/ML work. The method is novel compared to existing strong-to-weak and self-alignment techniques. The abstract indicates the method is compatible with PyTorch/XLA and does not mention any CUDA-only or proprietary hardware requirements, suggesting it is compatible with AWS Trainium.",no,yes
tOhHZpoBclM7MZc365N1,DataGen: Unified Synthetic Dataset Generation via Large Language Models,accept,"The paper proposes a novel LLM-powered framework called DataGen that addresses challenges in synthetic data generation, including generalization, controllability, diversity, and truthfulness. It introduces innovative mechanisms like attribute-guided generation and code-based mathematical assessment that go beyond the state of the art represented in the provided papers.",no,yes
nuhHZpoBclM7MZc3z5NF,HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes,accept,"The paper is relevant as it focuses on improving 3D Gaussian Splatting for novel view synthesis under degraded image conditions, which is a key technique in LLM/AI/ML. The proposed method of leveraging edge-semantic fusion and structural loss is novel compared to the state-of-the-art techniques described in the provided references. The method appears to be compatible with Trainium as it is based on standard neural network operations without any specialized hardware requirements.",no,yes
DehIZpoBclM7MZc3YJQy,Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data,accept,"The paper proposes a new reparameterized absorbing discrete diffusion (RADD) model that characterizes the time-independent conditional probabilities of clean data, which is relevant to current LLM/AI/ML work. The paper claims SOTA performance on language modeling benchmarks, suggesting novelty beyond the state of the art. The abstract indicates the method is expressible in PyTorch/XLA and uses common diffusion model operations, suggesting compatibility with AWS Trainium.",no,yes
nehHZpoBclM7MZc3zpN8,Transition Path Sampling with Improved Off-Policy Training of Diffusion Path Samplers,accept,"The paper proposes a novel approach to train diffusion path samplers for transition path sampling, which is relevant to current ML research on model architectures and training algorithms. The method introduces learnable control variates and off-policy training techniques, which are novel beyond the state of the art. The approach is compatible with Trainium as it is based on diffusion models and does not require specialized hardware. The paper is distinct from the referenced works, which focus on different techniques like Boltzmann priors, neural guided diffusion, and generative flows.",no,yes
W-hHZpoBclM7MZc3c5MY,SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance,accept,"The paper proposes a novel multi-turn math reasoning framework called SBSC that enables LLMs to generate step-by-step programs for solving Olympiad-level math problems, which is relevant to current LLM/AI/ML work. The method is novel beyond the state of the art and can be implemented on AWS Trainium. The paper is distinct from the referenced work, which focuses on different aspects of math reasoning in LLMs.",no,yes
HuhUW5oBclM7MZc3IpOe,Elucidating the Preconditioning in Consistency Distillation,accept,"The paper is relevant as it focuses on improving consistency distillation, a key technique for accelerating diffusion models. It proposes a novel, principled preconditioning method that enhances the alignment between student and teacher models, leading to faster training of consistency trajectory models. This is distinct from the previously indexed work on consistency models and diffusion model acceleration.",no,yes
7ehTW5oBclM7MZc3GpJb,Universal Image Restoration Pre-training via Degradation Classification,accept,"The paper proposes a new pre-training method called Degradation Classification Pre-Training (DCPT) that enables models to learn how to classify the degradation type of input images, which can then be used for universal image restoration. This is a novel technique that is relevant to current LLM/AI/ML work on model architectures and training algorithms.",no,yes
vOhHZpoBclM7MZc39ZN7,GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding,accept,"The paper is relevant as it focuses on improving multimodal large language models for understanding and interacting with graphical user interfaces, which is a key area of current LLM/AI/ML work. The paper proposes a new dataset and benchmark, GUI-World, which is a novel contribution beyond the state of the art. The abstract does not provide enough detail to determine if the methods are compatible with AWS Trainium, so the compatibility is unclear. The paper appears distinct from the previously indexed work, which focuses on different aspects of GUI agents and multimodal understanding.",no,unclear
puhGW5oBclM7MZc3BJI6,ELMO : Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces,accept,"The paper proposes a novel low-precision training framework for large output space models, which is relevant to current LLM/AI/ML work. The method uses BFloat16 and Float8 data types, and includes memory optimization techniques, which are compatible with AWS Trainium. The paper makes a clear material improvement over the prior work in the RAG context.",no,yes
L-hUW5oBclM7MZc3dpNb,Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity,reject,Claude evaluation failed: Invalid \escape: line 6 column 104 (char 196),unknown,unknown
IuhUW5oBclM7MZc3OJNB,Do LLMs estimate uncertainty well in instruction-following?,accept,"The paper is relevant as it focuses on evaluating the uncertainty estimation abilities of LLMs in the context of instruction-following, which is a key capability for AI agents. The paper proposes a novel controlled evaluation setup to systematically study this problem, which is distinct from the previously indexed work.",no,yes
--hIZpoBclM7MZc3TpMR,Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence,accept,"The paper proposes a new method for computing Concept Activation Vectors (CAVs) that aims to better capture the true concept direction, which is relevant to model interpretability and robustness. This is distinct from the previously indexed work, which does not address this specific issue with CAVs.",no,yes
KOhUW5oBclM7MZc3VZPe,Towards Learning High-Precision Least Squares Algorithms with Sequence Models,accept,"The paper is relevant as it focuses on learning numerical algorithms, specifically gradient descent, using sequence models. It proposes new architectures and training techniques to achieve high-precision and numerical generality, which are novel beyond the state of the art. The methods appear compatible with Trainium as they are expressed in terms of standard neural network components.",no,yes
LuhUW5oBclM7MZc3cpMn,PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark,accept,"The paper is relevant as it proposes a new ML4Sci task paradigm focused on interpreting experimental data within the framework of prior physical hypotheses and theories, which is distinct from the retrieved papers. The method is novel as it formulates the approach as a nonlinear mixed-integer programming problem, and the experiments on new datasets demonstrate the interpretability and feasibility of the approach. The method appears compatible with Trainium as it does not mention any hardware-specific requirements.",no,yes
4-hIZpoBclM7MZc3K5MC,Towards Understanding the Universality of Transformers for Next-Token Prediction,accept,"The paper is relevant as it focuses on understanding the in-context autoregressive learning ability of Transformers, which is a key aspect of current LLM/AI/ML work. The paper proposes a novel causal kernel descent method to learn context-dependent functions, which is distinct from the techniques discussed in the provided references.",no,yes
auhHZpoBclM7MZc3hpPp,Image-level Memorization Detection via Inversion-based Inference Perturbation,accept,"The paper proposes a new framework for image-level memorization detection in diffusion models, which is relevant to current LLM/AI/ML work. The method uses unconditional DDIM inversion and prompt optimization, which appears novel beyond the state of the art. The abstract indicates the approach can be expressed in PyTorch/XLA and is compatible with Trainium. The method is distinct from the previously indexed work, which focuses on different aspects of diffusion models and memorization.",no,yes
p-hHW5oBclM7MZc375LC,Online Learning in Risk Sensitive constrained MDP,accept,"The paper is relevant as it proposes a new algorithm for risk-sensitive constrained Markov decision processes, which is a key problem in AI/ML. The paper claims to be the first to establish sublinear regret and violation bounds for this problem, indicating novelty. However, the abstract lacks details on the algorithm's implementation, so Trainium compatibility is unclear.",no,unclear
n-hHZpoBclM7MZc30JMd,Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective,accept,"The paper is relevant as it focuses on analyzing and benchmarking the spectral properties of graph neural networks, which is directly related to model architectures and training algorithms. The proposed approach is novel as it introduces a new research problem and evaluation protocol to study GNNs from a spectral perspective, going beyond the prevalent neighborhood aggregation view. The methods appear compatible with Trainium as they are based on standard GNN operations.",no,yes
E-hTW5oBclM7MZc355NT,CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs,accept,"The paper introduces a new benchmark, CodeMMLU, that focuses on evaluating code understanding and reasoning capabilities of CodeLLMs, which is relevant to current LLM/AI/ML work. The benchmark covers diverse tasks beyond just code generation, which is novel compared to existing benchmarks. The abstract suggests the methods are compatible with PyTorch/XLA and do not mention any CUDA-only requirements, indicating Trainium compatibility. The paper proposes a distinct benchmark from the previously indexed work.",no,yes
2ehIZpoBclM7MZc3H5Pg,GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement,accept,"The paper proposes a novel approach for 3D mesh reconstruction from multi-view images, introducing key components to improve the large reconstruction model LRM, including modifications to the architecture, differentiable mesh extraction, and a lightweight per-instance texture refinement procedure. This is directly relevant to current LLM/AI/ML work and offers a material improvement over the state of the art.",no,yes
LehUW5oBclM7MZc3bZOX,Reasoning with Latent Thoughts: On the Power of Looped Transformers,accept,"The paper proposes a novel looped transformer architecture for reasoning tasks, which is relevant to current LLM/AI/ML work. The method is distinct from the retrieved papers and can be implemented on Trainium.",no,yes
sOhHZpoBclM7MZc345PF,Perm: A Parametric Representation for Multi-Style 3D Hair Modeling,accept,"The paper proposes a novel parametric representation for 3D hair modeling, which is relevant to current LLM/AI/ML work on model architectures and generative techniques. The method appears distinct from the retrieved papers, which focus on other 3D reconstruction and rendering tasks.",no,yes
wehHZpoBclM7MZc3-pNj,Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning,accept,"The paper is relevant as it focuses on mutual information skill learning, a technique for self-supervised reinforcement learning. It proposes a new MISL method and analyzes the key ingredients for effective skill learning, which is distinct from the previously indexed papers.",no,yes
duhHZpoBclM7MZc3mJPn,What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis,accept,"The paper is relevant as it analyzes the Transformer architecture, a key component of LLMs. It provides novel theoretical insights into the Hessian structure of Transformers, which is distinct from prior work. However, the abstract lacks details on the specific techniques used, so Trainium compatibility is unclear.",no,unclear
pOhHZpoBclM7MZc31pM7,Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment,accept,"The paper proposes a novel skill learning framework (DCSL) that introduces state-transition based skill definition, skill similarity function learning, and dynamic skill length adjustment, which are directly relevant to current LLM/AI/ML work. The method offers material improvements over existing skill learning approaches, as evidenced by the claims of better performance in task completion and efficiency.",no,yes
0-hIW5oBclM7MZc32pJL,Reflection-Window Decoding: Text Generation with Selective Refinement,accept,"The paper proposes a new text generation approach called Reflection-Window Decoding that incorporates a sliding reflection window and a pausing criterion to enable refinement and generation, which is distinct from the techniques described in the retrieved papers.",no,yes
yOhIW5oBclM7MZc3oZIj,Online Differentially Private Conformal Prediction for Uncertainty Quantification,accept,"The paper proposes a novel online differentially private conformal prediction framework, which is relevant to ML techniques for uncertainty quantification. The method offers a material improvement over existing conformal prediction approaches by ensuring individual privacy with a one-pass algorithm, as cited in the abstract. The method appears compatible with Trainium as it is described in PyTorch-compatible terms.",no,yes
3ehIZpoBclM7MZc3JJO7,LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations,accept,"The paper is relevant as it focuses on analyzing the internal representations of LLMs to understand and mitigate hallucinations, which is a key challenge in current LLM research. The paper proposes novel techniques for enhancing error detection and predicting error types, going beyond the state of the art. The methods described appear compatible with Trainium as they do not require specialized hardware or CUDA-only components.",no,yes
uuhHZpoBclM7MZc385Nu,Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization,accept,"The paper proposes a new loss function, Proximal Mapping Loss (PML), for crowd counting and localization, which is directly relevant to current ML research. PML offers a novel approach that addresses limitations of existing density regression methods. The abstract indicates the method can be implemented using standard ML techniques compatible with Trainium.",no,yes
AOhIZpoBclM7MZc3UpTu,"Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy",accept,"The paper proposes a novel information bottleneck-based imitation learning framework for implicit skill discovery and video-conditioned policy learning, which is relevant to current LLM/AI/ML work. The method can be realistically implemented on Trainium and is distinct from the previously indexed work.",no,yes
3ehJW5oBclM7MZc3GZKw,GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code,accept,"The paper proposes a novel method, GRAIL, that uses LLMs to generate programs for computing graph edit distance, which is a relevant problem in ML. GRAIL addresses key limitations of existing neural methods and demonstrates improved performance and cross-domain generalization.",no,yes
2ehJW5oBclM7MZc3A5JZ,Understanding the Limits of Deep Tabular Methods with Temporal Shift,accept,"The paper proposes a new approach to handle temporal shifts in deep tabular models, including a temporal embedding based on Fourier series expansion and an improved data splitting strategy. This is distinct from the previous work focused on tabular data generation, periodic signal learning, and irregular time series forecasting.",no,yes
DOhIZpoBclM7MZc3X5RP,Unsupervised Model Tree Heritage Recovery,accept,"The paper is relevant as it proposes a new unsupervised method for recovering the model heritage tree from model weights, which is directly applicable to current LLM/AI/ML work. The method is novel as it introduces a new approach to decoding the underlying tree structure from model weights, which is distinct from the techniques described in the provided context. The method appears compatible with AWS Trainium as it does not require any specialized hardware or CUDA-only custom ops.",no,yes
u-hIW5oBclM7MZc3WZJi,Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport,accept,"The paper proposes a novel backup strategy for MCTS that models value and action-value nodes as Gaussian distributions and computes the Wasserstein barycenter, which is relevant to current LLM/AI/ML work. The method is novel beyond the state of the art and can be implemented on Trainium, as it does not rely on any proprietary hardware or CUDA-only custom ops.",no,yes
Q-hHZpoBclM7MZc3SJPm,LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,accept,"The paper proposes a new comprehensive benchmark called LiveCodeBench that evaluates LLMs on a broader range of code-related capabilities beyond just code generation, including self-repair, code execution, and test output prediction. This is novel beyond the state of the art and can be implemented on Trainium as it uses standard ML techniques.",no,yes
POhHZpoBclM7MZc3OJPw,"The AdEMAMix Optimizer: Better, Faster, Older",accept,"The paper proposes a new optimizer, AdEMAMix, which uses a mixture of two EMAs to better leverage past gradients. This is relevant to model architectures and training algorithms. The method is novel compared to the state-of-the-art techniques described in the provided papers. The optimizer can be implemented using PyTorch/XLA and is compatible with Trainium.",no,yes
3OhJW5oBclM7MZc3FJI6,"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching",accept,"The paper proposes a new adaptive message passing framework to mitigate oversmoothing, oversquashing, and underreaching in deep graph networks, which is relevant to current LLM/AI/ML work. The method is novel compared to the state-of-the-art techniques described in the provided context. The abstract indicates the method is implementable using PyTorch/XLA and compatible with Trainium.",no,yes
A-hTW5oBclM7MZc3jZMt,Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers,accept,"The paper proposes a novel approach for Exploratory Inverse Constraint Learning (ExICL) using generative diffusion verifiers, which is relevant to current ML research on constraint inference and policy alignment. The method is distinct from the previously indexed works and can be implemented on Trainium.",no,yes
rehIW5oBclM7MZc3D5LF,Toward a Unified Theory of Gradient Descent under Generalized Smoothness,accept,"The paper is relevant as it proposes a new gradient descent algorithm under generalized smoothness assumptions, which is directly applicable to model architectures, training algorithms, and efficiency improvements. The method is novel as it derives a new step size rule that improves upon existing convergence rates. The abstract indicates the method can be expressed in PyTorch/XLA and is compatible with Trainium. The paper is distinct from the provided references, which focus on different optimization techniques and assumptions.",no,yes
NehUW5oBclM7MZc3lpNY,Diffusion Transformers for Tabular Data Time Series Generation,accept,"The paper is relevant as it proposes a novel Diffusion Transformers (DiTs) approach for generating time series of tabular data, which is a largely unexplored domain. The method addresses the challenges of heterogeneous tabular data and variable-length sequences, and outperforms previous work according to the experiments.",no,yes
EOhTW5oBclM7MZc32ZPN,Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation,accept,"The paper proposes a novel framework called ProverGen that synergizes LLMs and symbolic provers to generate a scalable, diverse, and high-quality first-order logic reasoning dataset, ProverQA. This is distinct from the previously indexed work, which focuses on different aspects of reasoning in LLMs.",no,yes
jOhHZpoBclM7MZc3upMV,UNSURE: self-supervised learning with Unknown Noise level  and Stein's Unbiased Risk Estimate,accept,"The paper proposes a new self-supervised learning method for image reconstruction that can handle unknown noise levels, which is relevant to current ML work. The method is novel as it extends the SURE approach to handle unknown noise, which is a material improvement over existing self-supervised methods. The abstract indicates the method is compatible with PyTorch/XLA and does not mention any CUDA-only custom ops, suggesting it can run on Trainium.",no,yes
