paper_id,title,decision,reason,similarity,similar_paper
2-hIZpoBclM7MZc3IpO1,FLOPS: Forward Learning with OPtimal Sampling,accept,"The paper is relevant as it focuses on improving the efficiency of forward learning algorithms, which are relevant to LLM training and optimization. The proposed query allocator is novel and claims to significantly enhance the scalability of forward-learning algorithms. The method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
kuhHZpoBclM7MZc3wpNc,Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection,reject,"The paper is about out-of-distribution detection, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques.",,
qehHZpoBclM7MZc325Oc,ImageFolder: Autoregressive Image Generation with Folded Tokens,accept,"The paper is relevant as it proposes a new image tokenizer architecture (ImageFolder) for autoregressive image generation, which is a key component of LLMs. The method is novel as it introduces a flexible solution to the tradeoff between reconstruction and generation quality by leveraging dual-branch product quantization. The method appears compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
8OhIZpoBclM7MZc3PJNo,MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations,accept,"The paper is relevant as it proposes a novel contrastive learning method (MIM-Refiner) to improve the performance of pre-trained MIM models, which is applicable to LLM/AI/ML work. The method is novel as it leverages intermediate layer representations to boost downstream task performance. The abstract indicates the method is compatible with PyTorch/XLA and uses standard transformer-compatible operations, suggesting it can be implemented on AWS Trainium.",,
F-hTW5oBclM7MZc3_pM2,Generative Classifiers Avoid Shortcut Solutions,accept,"The paper is relevant as it proposes a new generative classifier approach to improve robustness to distribution shift, which is a key challenge in LLM/AI/ML work. The method is novel compared to widely known 2024-2025 techniques, and it appears compatible with AWS Trainium as it uses standard PyTorch/XLA operations.",,
tuhIW5oBclM7MZc3PZJk,NeuronTune: Towards Self-Guided Spurious Bias Mitigation,accept,"The paper is relevant as it addresses the issue of spurious bias in deep neural networks, which is a key challenge in LLM/AI/ML work. The proposed method, NeuronTune, is novel as it introduces a self-guided approach to mitigate spurious bias without requiring external annotations. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
BOhIZpoBclM7MZc3WJRH,Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning,accept,"The paper is relevant as it focuses on integrating LLMs with graph neural networks for molecular design and retrosynthetic planning, which are important for LLM/AI/ML work. The proposed Llamole model is novel as it is the first multimodal LLM capable of interleaved text and graph generation for this task. However, the abstract lacks details on the specific implementation, so Trainium compatibility is unclear.",,
VOhHZpoBclM7MZc3apOS,Adaptive Energy Alignment for Accelerating Test-Time Adaptation,accept,"The paper is relevant as it proposes a new method (AEA) for efficient test-time adaptation of pre-trained models, which is an important problem in LLM/AI/ML. The method is novel as it introduces two new strategies for aligning the energy levels and class-wise correlations between source and target domains. The method appears compatible with Trainium as it is expressed in PyTorch and does not require any specialized hardware or CUDA-only custom kernels.",,
n-hHZpoBclM7MZc30JMd,Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective,reject,"The paper is focused on graph neural networks and their spectral properties, which is not directly related to LLMs, AI/ML work, or AWS Trainium.",,
2ea799b44aa94eb0659df92bf75e0f713eb2cd7adb9a05dadc4cd208e4dfa702,AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing,accept,"The paper is about an LLM-based framework for automated and explainable analog circuit sizing, which is relevant to current LLM/AI/ML work. The proposed multi-agent workflow with specialized LLM-based agents appears novel beyond the state of the art. However, the abstract lacks details on the specific techniques used, so Trainium compatibility is unclear.",,
5ehIZpoBclM7MZc3LZN3,TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice,accept,"The paper proposes a novel Mixture of Experts (MoE) architecture called TC-MoE that expands the expert space and introduces new loss functions to improve efficiency and effectiveness, which is relevant to LLM/AI/ML work. The abstract claims TC-MoE achieves significant improvements over traditional MoE approaches, indicating novelty beyond the state of the art. The method is described as using PyTorch/XLA and transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
9uhIZpoBclM7MZc3SpMQ,Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning,accept,"The paper is relevant as it proposes a novel meta-reinforcement learning framework for efficient task identification and adaptation, which is applicable to LLM/AI/ML work. The method uses latent task belief similarity to handle sparse reward settings, which is a novel contribution beyond the state of the art. The approach is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
iehHZpoBclM7MZc3tJNz,An Auditing Test to Detect Behavioral Shift in Language Models,accept,"The paper is relevant as it proposes a method for detecting behavioral shifts in language models, which is crucial for evaluating and monitoring the capabilities and alignment of LLMs. The method is novel as it presents a statistical test to efficiently detect distribution shifts in model outputs. The method is compatible with Trainium as it is described in PyTorch/XLA and uses standard transformer-compatible operations.",,
yuhIW5oBclM7MZc3q5J7,Bayesian Basis Function Approximation for Scalable Gaussian Process Priors in Deep Generative Models,accept,"The paper is relevant as it proposes a new scalable Gaussian process prior for VAE models, which is applicable to LLM and generative AI. The basis function approximation technique is novel and claims to improve scalability and performance. The method appears compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
g-hHZpoBclM7MZc3q5Pz,Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation,accept,The paper is relevant as it focuses on improving LLM performance through KG-based retrieval-augmented generation. It proposes a novel SubgraphRAG approach that integrates an MLP with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval. The method is compatible with Trainium as it uses standard transformer-based operations.,,
x-hIW5oBclM7MZc3npJ6,Spatial Reasoning with Denoising Models,accept,"The paper is relevant as it proposes a new framework for spatial reasoning using denoising generative models, which is relevant to LLM/AI/ML work. The paper claims to introduce novel techniques for improving the accuracy of specific reasoning tasks.",,
YuhHZpoBclM7MZc3fZM5,Cross-Embodiment Dexterous Grasping with Reinforcement Learning,accept,"The paper is relevant as it focuses on learning dexterous grasping policies for diverse robot hands, which is an important problem in LLM/AI/ML. The proposed approach of using a universal action space based on human hand eigengrasps is novel compared to prior work focused on specific robot hands.",,
mOhHZpoBclM7MZc3yZMu,Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks,accept,"The paper is relevant as it focuses on backdoor attacks, a key security concern for LLMs. It proposes a novel selective poisoning strategy for clean-label backdoor attacks, which is more practical than prior methods. However, the abstract lacks details on the specific techniques used, so Trainium compatibility is unclear.",,
aOhHZpoBclM7MZc3hZNH,From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities,accept,"The paper is relevant as it proposes a novel image tokenizer for multimodal language models, which is a key component of LLM architectures. The method of applying Byte-Pair Encoding to visual data is novel beyond the state of the art. The abstract indicates the method is compatible with Transformer models, which can be expressed in PyTorch/XLA and run on Trainium.",,
zOhIZpoBclM7MZc3D5Px,DEEM: Diffusion models serve as the eyes of large language models for image perception,accept,"The paper is relevant as it focuses on improving the image perception capabilities of large language models, which is a key aspect of LLM/AI/ML work. The proposed DEEM approach is novel as it utilizes diffusion models to enhance the visual perception of LMMs. The method is compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
JOhUW5oBclM7MZc3QZMD,Large (Vision) Language Models are Unsupervised In-Context Learners,accept,The paper is relevant as it focuses on improving LLM performance through unsupervised adaptation techniques. The proposed joint inference framework and unsupervised fine-tuning/ICL methods are novel beyond the state of the art. The methods are compatible with Trainium as they are expressible in PyTorch/XLA and use transformer-compatible ops.,,
BOhTW5oBclM7MZc3k5PP,Federated Domain Generalization with Data-free On-server Matching Gradient,accept,"The paper is relevant as it addresses domain generalization in federated learning, a key challenge in LLM/AI/ML. The proposed FedOMG method is novel as it introduces a new approach to leverage distributed domain information without additional communication cost. The method appears compatible with Trainium as it is expressed in PyTorch and does not require any specialized hardware.",,
eehHZpoBclM7MZc3m5N6,Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference,accept,"The paper is relevant as it studies how to subvert large language models from following prompt-specified rules, which is an important topic in LLM research. The proposed logic-based framework for analyzing LLMs in rule-based settings is novel. However, the abstract lacks details on the specific methods, making it unclear if the approach is compatible with AWS Trainium.",,
TOhHZpoBclM7MZc3WpPu,Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping,accept,"The paper is relevant as it proposes a novel mixture-of-experts approach for efficient multi-task reinforcement learning for dexterous grasping, which is applicable to LLM/AI/ML work. The method is novel beyond the state-of-the-art and can be implemented on AWS Trainium as it uses standard PyTorch/XLA operations.",,
8-hIZpoBclM7MZc3RZOM,RGB-Event ISP: The Dataset and Benchmark,reject,Bedrock throttling - max retries exceeded,,
3uhIZpoBclM7MZc3JZO2,MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine,accept,"The paper is relevant as it focuses on improving LLM capabilities in mathematical reasoning and visual understanding. It proposes a novel training pipeline, MAVIS, that leverages automatically generated datasets to enhance vision-language alignment and chain-of-thought reasoning. The methods appear compatible with Trainium as they are expressed in PyTorch/XLA and use standard transformer-based operations.",,
7-hIZpoBclM7MZc3O5OI,High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws,accept,"The paper is relevant as it focuses on knowledge distillation, a key technique for training efficient LLMs. It proposes a novel theoretical analysis of the weak-to-strong generalization in high-dimensional regression, which is beyond the state of the art. The methods described appear compatible with PyTorch/XLA and Trainium's capabilities.",,
vuhHZpoBclM7MZc395N6,Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel,accept,"The paper is relevant as it proposes new sampling algorithms for diffusion models, which are a key component of large language models. The algorithms claim novel theoretical guarantees compared to prior work. However, the abstract lacks details on the specific implementation, so Trainium compatibility is unclear.",,
GOhUW5oBclM7MZc3BJMH,AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials,reject,"The paper is about generating agent trajectories for GUI automation, which is not directly related to LLM/AI/ML work. The method does not propose any novel algorithms or architectures for LLMs or ML training/inference.",,
LehUW5oBclM7MZc3bZOX,Reasoning with Latent Thoughts: On the Power of Looped Transformers,accept,"The paper is relevant as it focuses on improving the reasoning capabilities of large language models, a key area of current LLM/AI/ML research. The proposed looped transformer architecture is novel and claims to outperform non-looped models on various reasoning tasks. The method appears compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
wOhHZpoBclM7MZc3-ZN9,Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research,accept,"The paper is relevant as it focuses on accelerating goal-conditioned reinforcement learning, which is an important area of LLM/AI/ML research. The proposed method is novel as it introduces a high-performance codebase and benchmark for self-supervised GCRL, which can reduce training time by up to 22x. The method is compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
CehIZpoBclM7MZc3XJSu,Towards Foundation Models for Mixed Integer Linear Programming,accept,"The paper is relevant as it focuses on improving LLM-based approaches for Mixed Integer Linear Programming, a key task in AI and ML. The proposed MILP-Evolve framework for generating diverse MILP datasets is novel beyond the state of the art. The methods appear compatible with AWS Trainium as they are expressed in PyTorch and use transformer-compatible operations.",,
6ehJW5oBclM7MZc3W5JQ,Learning Extrapolative Sequence Transformations from Markov Chains,accept,"The paper is relevant as it proposes a novel autoregressive model for extrapolative sequence transformations, which is applicable to LLM tasks like protein design and text generation. The method is novel compared to standard MCMC approaches and can be implemented using PyTorch/XLA on Trainium.",,
9ehTW5oBclM7MZc3PpLl,MiniPLM: Knowledge Distillation for Pre-training Language Models,accept,"The paper is relevant as it focuses on knowledge distillation for pre-training language models, which is a key technique for training efficient LLMs. The proposed MiniPLM framework is novel as it addresses efficiency, flexibility, and effectiveness issues in existing KD methods. The method is compatible with Trainium as it operates on the training corpus and does not require proprietary hardware.",,
RuhHZpoBclM7MZc3UJM6,MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code,accept,The paper is relevant as it focuses on improving the mathematical reasoning abilities of large language models through continued pretraining on a novel dataset of mathematical code and reasoning steps. The method of generating this dataset is novel beyond the state of the art. The approach appears compatible with AWS Trainium as it uses standard PyTorch/XLA operations without any proprietary hardware requirements.,,
-ehTW5oBclM7MZc3U5K-,Discrete Codebook World Models for Continuous Control,accept,"The paper is relevant as it proposes a new world model architecture (DCWM) and a model-based RL algorithm (DC-MPC) for continuous control tasks, which are relevant to current LLM/AI/ML work. The paper claims the proposed methods are novel and outperform recent state-of-the-art approaches. The methods are described as using PyTorch-compatible discrete latent representations, suggesting they are compatible with AWS Trainium.",,
rehIW5oBclM7MZc3D5LF,Toward a Unified Theory of Gradient Descent under Generalized Smoothness,accept,"The paper is relevant as it proposes a new gradient descent method for optimizing under generalized smoothness, which is applicable to LLM training and inference. The method is novel compared to widely known 2024-2025 techniques. The approach is compatible with Trainium as it can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.",,
-OhIZpoBclM7MZc3S5PI,Attention with Markov: A Curious Case of Single-layer Transformers,accept,"The paper is relevant as it studies the behavior of Transformers, a key LLM architecture, on Markov chains. It proposes a novel theoretical framework to analyze the loss landscape of single-layer Transformers, which is a credible technical contribution. The methods described appear compatible with PyTorch/XLA and Trainium requirements.",,
V-hHZpoBclM7MZc3bpPI,KAN: Kolmogorovâ€“Arnold Networks,reject,"The paper proposes a new neural network architecture called Kolmogorov-Arnold Networks (KANs) that use learnable activation functions, which is novel beyond the state of the art. However, the abstract does not provide enough details about the implementation to judge its compatibility with AWS Trainium.",,
X-hHZpoBclM7MZc3eJN6,ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities,accept,"The paper is relevant as it focuses on improving LLM architectures and capabilities, specifically long-context understanding and retrieval-augmented generation. The proposed ChatQA 2 model is novel as it extends the context window of Llama3 from 8K to 128K tokens and enhances the model's instruction-following and RAG performance. The method is compatible with Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
huhHZpoBclM7MZc3sJP1,Deep Linear Probe Generators for Weight Space Learning,accept,"The paper is relevant as it proposes a new method (ProbeGen) for weight space learning, which is applicable to LLM architectures. The method is novel as it introduces a deep linear probe generator, which is a significant improvement over existing probing approaches. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
-ehIZpoBclM7MZc3TJOV,Automated Design of Agentic Systems,accept,"The paper is relevant as it discusses the design of agentic systems, which are closely related to LLMs and AI/ML. It proposes a novel approach to automatically discover new agentic system designs, which goes beyond the state of the art. However, the abstract lacks details on the specific technical implementation, so the Trainium compatibility is unclear.",,
pehGW5oBclM7MZc3ApIn,Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization,reject,Claude evaluation failed: Invalid \escape: line 5 column 186 (char 264),,
z-hIW5oBclM7MZc3w5Kb,Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives,reject,"The paper is relevant as it discusses scaling LLM agents and the need for asymptotic analysis with LLM primitives. However, it does not propose any new algorithms or architectures, and the techniques described are compatible with PyTorch/XLA and Trainium.",,
9OhIZpoBclM7MZc3RpN3,ADAM Optimization with Adaptive Batch Selection,accept,"The paper is relevant as it proposes a new optimizer, AdamCB, that aims to improve training efficiency of neural networks. The abstract claims AdamCB is novel compared to prior work and achieves faster convergence. The method appears compatible with Trainium as it is based on Adam, a widely used optimizer, and does not mention any hardware-specific requirements.",,
p-hHZpoBclM7MZc32ZOQ,Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints,reject,"The paper is about tabular data generation using constrained deep generative models, which is not directly related to LLM/AI/ML work. The method does not appear to be novel beyond the state of the art in this area.",,
VuhHZpoBclM7MZc3bZPq,Wavelet-based Positional Representation for Long Context,accept,"The paper proposes a new position representation method for long-context LLMs, which is relevant to LLM architectures and scaling. The method leverages wavelet transforms to capture multiple scales, which is novel beyond the state of the art. The method is expressed in PyTorch and uses standard transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
-uhTW5oBclM7MZc3WpK1,The 3D-PC: a benchmark for visual perspective taking in humans and machines,reject,"The paper is about a benchmark for visual perspective taking, which is not directly related to LLM/AI/ML work. It does not propose any new algorithms or architectures.",,
W-hHZpoBclM7MZc3c5MY,SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance,reject,"The paper is about using LLMs for solving math olympiad problems, which is a narrow end-user application without new ML methods.",,
4OhIZpoBclM7MZc3KJMb,Continuous Diffusion for Mixed-Type Tabular Data,reject,"The paper is about a diffusion model for tabular data, which is not directly related to LLMs or their training/inference efficiency. The abstract does not provide enough details to assess Trainium compatibility.",,
buhHZpoBclM7MZc3jZO4,Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding,accept,"The paper is relevant as it proposes a novel Transformer-based architecture for spatio-temporal video grounding, a task related to LLMs and AI. The proposed TA-STVG method with text-guided temporal sampling and attribute-aware spatial activation is novel compared to existing Transformer-based approaches. The method is compatible with Trainium as it is expressed in PyTorch and uses standard Transformer-compatible operations.",,
0OhIW5oBclM7MZc3yZIS,Is Noise Conditioning Necessary for Denoising Generative Models?,accept,"The paper is relevant as it proposes a new approach to denoising generative models, which is a key component of LLM/AI/ML work. The proposed noise-unconditional model is novel compared to the widely known noise-conditional models. The method can be implemented on AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
s-hIW5oBclM7MZc3LZJ7,Faster Stochastic Optimization with Arbitrary Delays via Adaptive Asynchronous Mini-Batching,accept,"The paper is relevant as it proposes a new asynchronous stochastic optimization method that improves convergence rates for non-convex and convex smooth problems. The method is novel as it adapts to arbitrary delay sequences and achieves better rates than existing approaches. The technique is compatible with Trainium as it is based on standard stochastic first-order methods and asynchronous mini-batching, which can be expressed in PyTorch/XLA.",,
U-hHZpoBclM7MZc3aJPE,PEARL: Parallel Speculative Decoding with Adaptive Draft Length,accept,"The paper proposes a novel speculative decoding method (PEARL) to improve inference efficiency of LLMs, which is relevant to current LLM/AI/ML work. The method uses parallel drafting and verification phases to address the mutual waiting problem, which is a novel contribution beyond the state of the art. The method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible ops.",,
PehHZpoBclM7MZc3OpN7,Inverse Constitutional AI: Compressing Preferences into Principles,accept,"The paper is relevant as it proposes a method for interpreting and compressing preference data from LLMs, which is crucial for training and evaluating AI models. The proposed Inverse Constitutional AI algorithm is novel and provides a credible approach to extracting interpretable principles from preference data. The method appears compatible with Trainium as it is based on PyTorch and does not require any specialized hardware.",,
3ehJW5oBclM7MZc3GZKw,GRAIL: Graph Edit Distance and Node Alignment using LLM-Generated Code,accept,"The paper is relevant as it proposes a novel LLM-based approach for computing Graph Edit Distance, a key problem in graph analysis. The method aims to address limitations of existing neural approaches. However, the abstract lacks details on the specific implementation, making it unclear if the method is compatible with AWS Trainium.",,
_OhIZpoBclM7MZc3TpPU,Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words,accept,"The paper is relevant as it focuses on improving the interpretability of large language models, a key aspect of current LLM/AI/ML work. It proposes a novel suite of evaluations to analyze the quality of monosemantic features in sparse autoencoders, which is beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
ZuhHZpoBclM7MZc3gZOF,Data Shapley in One Training Run,accept,"The paper is relevant as it proposes a novel method for efficiently evaluating data contribution in large-scale model training, which is crucial for LLM/AI/ML work. The 'In-Run Data Shapley' method is novel compared to the traditional retraining-based approach and can be implemented efficiently on Trainium, as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",,
VehHZpoBclM7MZc3a5NX,Real-Time Video Generation with Pyramid Attention Broadcast,accept,"The paper is relevant as it focuses on improving the efficiency of video generation, a key application of large language models. The proposed Pyramid Attention Broadcast method is novel, as it introduces a new attention-based approach to speed up the diffusion process. The method appears compatible with AWS Trainium as it is based on PyTorch and uses transformer-compatible operations.",,
DuhTW5oBclM7MZc30JOX,Compute-Optimal LLMs Provably Generalize Better with Scale,accept,"The paper is relevant as it focuses on improving the generalization of large language models, a key aspect of current LLM research. It proposes a novel, empirical generalization bound that accounts for loss variance and quantization error, which is a novel contribution beyond the state of the art. The methods described appear compatible with AWS Trainium as they are expressed in terms of standard neural network operations.",,
2uhJW5oBclM7MZc3CJJb,Inverse Optimization via Learning Feasible Regions,reject,"The paper is about inverse optimization and learning feasible regions, which is not directly related to LLM/AI/ML work. The abstract does not mention any techniques relevant to LLMs or Trainium.",,
wuhHZpoBclM7MZc3_pPy,A Statistical Framework for Ranking LLM-based Chatbots,reject,Bedrock throttling - max retries exceeded,,
AuhTW5oBclM7MZc3iJNl,NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains,accept,"The paper is relevant as it focuses on neuro-symbolic approaches for embodied agents to tackle complex tasks in open-domain environments, which is relevant to LLM and AI research. The proposed NeSyC framework is novel in its use of LLMs and symbolic tools for continual learning and knowledge refinement.",,
1uhIZpoBclM7MZc3G5P9,In-Context Editing: Learning Knowledge from Self-Induced Distributions,accept,"The paper is relevant as it focuses on improving the efficiency of incorporating new information into language models, a key challenge in LLM research. The proposed Consistent In-Context Editing (ICE) method is novel in leveraging the model's in-context learning capability. The method appears compatible with Trainium as it is described in terms of gradient-based tuning and optimizing output distributions, which can be expressed in PyTorch/XLA.",,
DOhTW5oBclM7MZc3x5Nz,Fast Uncovering of Protein Sequence Diversity from Structure,reject,"The paper is about an inverse folding method for generating diverse protein sequences, which is not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
u-hIW5oBclM7MZc3WZJi,Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport,reject,"The paper is about Monte-Carlo Tree Search, which is not directly related to LLMs, AI/ML, or Trainium. The abstract does not mention any concrete technical contributions to LLM architectures, training, or inference.",,
DuhIZpoBclM7MZc3YZQH,OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs,accept,"The paper is relevant as it focuses on improving the efficiency of long-context LLMs, a key challenge in current LLM research. The proposed OmniKV method is novel as it introduces a token-dropping-free and training-free inference technique to reduce KV cache memory usage. The method is compatible with AWS Trainium as it is expressible in PyTorch/XLA and uses transformer-compatible ops.",,
9uhTW5oBclM7MZc3RJLR,From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle,accept,"The paper is relevant as it focuses on LLM-based planning and multi-modal task execution. It proposes a novel system called Hive that plans and executes explainable action sequences across multiple models. However, the abstract lacks details on the specific technical implementation, so Trainium compatibility is unclear.",,
x-hIZpoBclM7MZc3B5Mt,Learning How Hard to Think: Input-Adaptive Allocation of LM Computation,accept,"The paper is relevant as it addresses techniques for improving the efficiency of language model decoding, which is crucial for LLM work. The proposed adaptive computation allocation approach is novel compared to existing fixed decoding procedures. The method appears compatible with Trainium as it is described in PyTorch-compatible terms and does not require specialized hardware.",,
YehHZpoBclM7MZc3epNA,Procedural Synthesis of Synthesizable Molecules,reject,Bedrock throttling - max retries exceeded,,
guhHZpoBclM7MZc3q5MD,Accelerating neural network training: An analysis of the AlgoPerf competition,accept,"The paper is relevant as it focuses on improving training efficiency of neural networks, which is crucial for LLM/AI/ML work. The winning submissions demonstrate novel training algorithms that outperform popular methods like Adam, and are compatible with Trainium as they can be expressed in PyTorch/XLA and use FP16/BF16 operations.",,
TehHZpoBclM7MZc3XJPT,Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models,accept,"The paper proposes a new method, Infilling Score, for detecting pretraining data in LLMs, which is relevant to improving training and evaluation. The method uses non-causal token likelihoods and achieves better performance than prior state-of-the-art methods, indicating novelty. The method is based on PyTorch/XLA and uses standard transformer operations, suggesting compatibility with AWS Trainium.",,
--hIZpoBclM7MZc3TpMR,Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence,accept,"The paper is relevant as it focuses on improving concept activation vectors, which are important for understanding and interpreting LLM predictions. The proposed pattern-based CAVs are novel compared to the standard separability-oriented CAVs. The method appears compatible with Trainium as it is expressed in PyTorch and does not require any specialized hardware.",,
AuhIZpoBclM7MZc3VJTJ,SimPER: A Minimalist Approach to Preference  Alignment without Hyperparameters,accept,"The paper is relevant as it proposes a new preference optimization algorithm for language model alignment, which is a key aspect of current LLM/AI/ML work. The method is novel as it introduces a simple, hyperparameter-free approach that outperforms existing state-of-the-art methods. The method is compatible with AWS Trainium as it is expressible in PyTorch/XLA and uses FP16/BF16 operations.",,
Oeg2ZpoBclM7MZc3z5Pj,How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?,accept,"The paper is relevant as it examines the impact of vision-language adaptation on the safety capabilities of large language models. It proposes a novel weight merging approach to address the divergent objectives of adaptation and safety tuning. However, the abstract lacks details on the specific methods used, making it unclear if they are compatible with AWS Trainium.",,
sehHZpoBclM7MZc35ZN0,Aligning Human Motion Generation with Human Perceptions,reject,"The paper is about human motion generation and perception, which is not directly related to LLM/AI/ML work. It does not propose any novel algorithms or architectures, and the abstract lacks details on the technical implementation to judge Trainium compatibility.",,
UehHZpoBclM7MZc3ZpNn,Measuring And Improving Persuasiveness Of Large Language Models,accept,"The paper is relevant as it focuses on measuring and improving the persuasiveness of large language models, which is an important aspect of current LLM/AI/ML work. The introduction of the 'transsuasion' task and the findings on the persuasive capabilities of LLMs are novel beyond the state of the art.",,
FuhTW5oBclM7MZc39pNn,Improved Diffusion-based Generative Model with Better Adversarial Robustness,accept,"The paper is relevant as it addresses training and inference efficiency of diffusion-based generative models, which are closely related to LLMs. The proposed Adversarial Training approach is novel and credibly demonstrated to improve the models. The methods are expressed in PyTorch and rely on standard transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
vOhIW5oBclM7MZc3XpJG,Power Mean Estimation in Stochastic Continuous Monte-Carlo Tree Search,reject,"This paper is about Monte Carlo Tree Search for continuous, stochastic Markov Decision Processes, which is not directly related to LLMs, AI/ML, or AWS Trainium.",,
N-g2ZpoBclM7MZc3xpPD,Boltzmann priors for Implicit Transfer Operators,reject,"The paper is about molecular dynamics simulations and thermodynamic property prediction, which are not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
E-hTW5oBclM7MZc355NT,CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs,accept,"The paper is relevant as it focuses on evaluating the code understanding and reasoning capabilities of CodeLLMs, which is crucial for advancing AI-assisted development. The benchmark is novel as it goes beyond traditional code generation tasks. The methods described are likely compatible with AWS Trainium as they are based on PyTorch/XLA and do not require specialized hardware.",,
9-hIZpoBclM7MZc3S5ME,ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains,accept,"The paper is relevant as it focuses on evaluating the chronological knowledge of LLMs, which is an important aspect of their performance. The proposed ChroKnowBench benchmark and ChroKnowledge framework are novel approaches to this problem. However, the abstract lacks details on the specific technical implementation, so the Trainium compatibility is unclear.",,
7OhIZpoBclM7MZc3N5Ma,Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models,accept,"The paper is relevant as it proposes a novel method (Intelligent Go-Explore) that leverages large pretrained foundation models to improve exploration in complex environments, which is a key challenge in LLM/AI/ML work. The method is novel compared to existing Go-Explore approaches, and appears compatible with AWS Trainium as it is based on PyTorch and uses standard transformer-compatible operations.",,
1OhIW5oBclM7MZc335Jb,Fairness on Principal Stratum: A New Perspective on Counterfactual Fairness,reject,"The paper is about fairness in decision-making and proposes a new concept of principal counterfactual fairness, which is not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
_uhTW5oBclM7MZc3cZLX,Directional Gradient Projection for Robust Fine-Tuning of Foundation Models,accept,"The paper is relevant as it proposes a novel method (DiGraP) for robust fine-tuning of large language models, which is a key area of LLM research. The method uses directional gradient information to improve both in-distribution generalization and out-of-distribution robustness, which is a novel contribution. The method is also compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
7-hTW5oBclM7MZc3IZJs,Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks,accept,"The paper is relevant as it proposes a new architecture for long-range propagation in graph neural networks, which is an important topic in LLM/AI/ML. The abstract claims the method is novel and provides theoretical guarantees on information conservation.",,
5OhJW5oBclM7MZc3PpJ9,Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer,accept,"The paper is relevant as it addresses learning to defer with multiple experts, which is a critical challenge in natural language generation. The paper introduces novel surrogate loss functions and efficient algorithms with strong theoretical learning guarantees, which is novel beyond the state of the art. The methods are compatible with AWS Trainium as they are expressible in PyTorch/XLA and use FP16/BF16 operations.",,
ouhHZpoBclM7MZc305Oo,CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening,accept,"The paper is relevant as it focuses on a multimodal foundation model for molecular property prediction and antibiotic screening, which are key areas in LLM/AI/ML work. The model's novel contrastive learning approach and use of different molecular data modalities suggest it is beyond the state of the art. The paper indicates the model is implemented in PyTorch and uses standard transformer-compatible operations, suggesting it is compatible with AWS Trainium.",,
ZOhHZpoBclM7MZc3f5Pu,Competing Large Language Models in Multi-Agent Gaming Environments,accept,"The paper is relevant as it focuses on evaluating LLM performance in multi-agent gaming environments, which is an important aspect of current LLM/AI/ML work. The proposed GAMA-Bench framework is novel beyond the state of the art, and the methods appear compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",,
0ehIW5oBclM7MZc3zZKs,Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting,accept,"The paper is relevant as it proposes a sample weighting scheme to mitigate catastrophic forgetting in fine-tuning LLMs. The method is novel as it operates in the sample space, unlike existing methods that focus on parameter or gradient space. The method is compatible with Trainium as it is expressible in PyTorch/XLA and uses standard transformer-compatible operations.",,
3-hJW5oBclM7MZc3JpJ4,Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning,accept,"The paper is relevant as it proposes a novel offline model-based reinforcement learning method for improving performance in sparse reward and long-horizon environments, which is relevant to current LLM/AI/ML work. The proposed Temporal Distance-Aware Transition Augmentation (TempDATA) method is novel beyond the state of the art. The method is compatible with AWS Trainium as it is expressible in PyTorch/XLA and uses FP16/BF16 operations.",,
UOhHZpoBclM7MZc3ZZNE,TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes,reject,"This paper is about a geometric representation method for 3D shape modeling, which is not directly relevant to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
EehTW5oBclM7MZc33pMJ,Learning mirror maps in policy mirror descent,accept,"The paper is relevant as it focuses on improving policy optimization in reinforcement learning, which is relevant to LLM training. It proposes a novel method of learning the mirror map in Policy Mirror Descent, which is shown to outperform the standard negative entropy mirror map. The method is compatible with Trainium as it is based on standard RL algorithms that can be expressed in PyTorch/XLA.",,
cuhHZpoBclM7MZc3k5Me,Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks,accept,"The paper is relevant as it focuses on improving the robustness of graph neural network (GNN) explainability methods against graph perturbation attacks, which is an important aspect of current LLM/AI/ML work. The proposed XGNNCert method is novel as it provides provable robustness guarantees, which is beyond the state of the art. However, the abstract does not provide enough detail to determine if the method is compatible with AWS Trainium.",,
uehHZpoBclM7MZc38ZNf,Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model,accept,"The paper is relevant as it proposes a novel ECG language model for efficient representation learning, which is applicable to LLM/AI/ML work. The method is novel in treating ECG signals as words and rhythms as sentences, and the proposed HeartLang framework is compatible with AWS Trainium as it is based on PyTorch and uses transformer-compatible operations.",,
gOhHZpoBclM7MZc3qZMh,Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy,reject,"The paper is about non-rigid point cloud registration, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques.",,
5-hIZpoBclM7MZc3MpP3,Reconciling Model Multiplicity for Downstream Decision Making,reject,"The paper is about reconciling model multiplicity for downstream decision-making, which is not directly related to LLM/AI/ML work. It does not propose any new LLM architectures, training/inference methods, or evaluation techniques.",,
UuhHZpoBclM7MZc3Z5Mo,JPEG Inspired Deep Learning,accept,"The paper is relevant as it proposes a novel DL framework that integrates a trainable JPEG compression layer, which can improve model performance and robustness. The method is novel beyond the state of the art and can be implemented on AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
oehHZpoBclM7MZc30pPF,Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM,accept,"The paper is relevant as it focuses on improving translation performance of LLMs in low-resource languages, which is a key challenge in current LLM/AI/ML work. The proposed Mufu method is novel as it introduces a new prompt-based approach to leverage auxiliary translation candidates. The method is compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
GuhUW5oBclM7MZc3DZOs,Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling,accept,"The paper is relevant as it discusses techniques for improving training and inference efficiency of masked diffusion models, which are a class of LLMs. The proposed first-hitting sampler and analysis of numerical issues in categorical sampling are novel contributions beyond the state of the art. The methods appear compatible with Trainium as they are expressed in PyTorch and use standard transformer-compatible operations.",,
quhIW5oBclM7MZc3AJJr,Fine-Grained Captioning of Long Videos through Scene Graph Consolidation,reject,"The paper is about video captioning, which is a narrow end-user application without method advances in LLMs, training efficiency, or inference efficiency.",,
O-hHZpoBclM7MZc3NZN7,Adversarial Mixup Unlearning,accept,"The paper is relevant as it focuses on machine unlearning, a critical area for LLM safety and privacy. The proposed MixUnlearn approach is novel in using adversarial mixup examples to address catastrophic unlearning. The method is compatible with Trainium as it is based on standard PyTorch/XLA operations without custom CUDA-only kernels.",,
jOhHZpoBclM7MZc3upMV,UNSURE: self-supervised learning with Unknown Noise level  and Stein's Unbiased Risk Estimate,accept,"The paper is relevant as it proposes a new self-supervised learning method for image reconstruction, which is applicable to LLM/AI/ML work. The method is novel as it extends the SURE approach to handle unknown noise levels, which is an improvement over existing self-supervised methods. The method appears compatible with AWS Trainium as it is based on standard PyTorch/XLA operations.",,
4ehJW5oBclM7MZc3L5JD,Preserving AUC Fairness in Learning with Noisy Protected Groups,accept,"The paper is relevant as it focuses on fairness in AUC optimization for LLM/AI/ML applications. It proposes a novel robust approach to preserve AUC fairness under noisy protected groups, which is a practical concern. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
y-hIW5oBclM7MZc3sJL0,Contrastive Visual Data Augmentation,accept,"The paper is relevant as it focuses on improving LLM's ability to recognize novel visual concepts, a key challenge in current LLM/AI/ML work. The proposed Contrastive visual Data Augmentation (CoDA) strategy is novel beyond the state of the art. The method uses multimodal generative models and can be expressed in PyTorch/XLA, making it compatible with AWS Trainium.",,
w-hIZpoBclM7MZc3AZMe,Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense,accept,"The paper proposes a novel 'Dynamic Neural Fortresses' defense method against model extraction attacks, which is relevant to LLM security and efficiency. The method introduces a dynamic early-exit architecture, which is novel beyond the state of the art. The method is also compatible with Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
XehHZpoBclM7MZc3dpPG,Mixture of Attentions For Speculative Decoding,accept,"The paper is about improving speculative decoding, a key technique for efficient inference in large language models, which is relevant to current LLM/AI/ML work. The proposed Mixture of Attentions architecture is novel compared to the state of the art and can be implemented using PyTorch/XLA on AWS Trainium.",,
fehHZpoBclM7MZc3opPm,ESE: Espresso Sentence Embeddings,accept,"The paper is about improving sentence embeddings, which is relevant to LLM and AI/ML work. The proposed ESE model with learn-to-express and learn-to-compress processes is novel beyond the state of the art. The method uses PyTorch-compatible operations and can likely run on Trainium.",,
AOhIZpoBclM7MZc3UpTu,"Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy",reject,"The paper is about video-conditioned policy learning, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques.",,
puhHZpoBclM7MZc315O2,Scaling Transformers for Low-Bitrate High-Quality Speech Coding,accept,"The paper is relevant as it focuses on scaling Transformers for speech coding, a key LLM-related task. It proposes a novel Transformer-based architecture with a flexible quantization bottleneck, which outperforms existing baselines. The method can be expressed in PyTorch/XLA and uses FP16/BF16, making it compatible with AWS Trainium.",,
yuhIZpoBclM7MZc3DJOt,KooNPro: A Variance-Aware Koopman Probabilistic Model Enhanced by Neural Process for Time Series Forecasting,reject,"The paper is about a probabilistic time series forecasting model, which is not directly related to LLMs, AI/ML architectures, or inference efficiency. The abstract does not provide enough details to judge Trainium compatibility.",,
KuhUW5oBclM7MZc3X5MB,nGPT: Normalized Transformer with Representation Learning on the Hypersphere,accept,"The paper proposes a novel Transformer architecture, nGPT, with representation learning on the hypersphere, which is relevant to current LLM/AI/ML work. The experiments show that nGPT learns much faster, which is a novel contribution beyond the state of the art. The method is expressed in PyTorch and uses standard Transformer-compatible operations, making it compatible with AWS Trainium.",,
v-hHZpoBclM7MZc3-JOh,Robustness Inspired Graph Backdoor Defense,reject,"The paper is about defending graph neural networks against backdoor attacks, which is not directly related to LLMs or AI/ML work. The abstract does not provide enough detail to judge Trainium compatibility.",,
1ehIZpoBclM7MZc3GpPc,TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting,reject,"The paper is about a spiking neural network architecture for time series forecasting, which is not directly relevant to large language models, AI/ML, or AWS Trainium.",,
4ehIZpoBclM7MZc3KZMY,BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval,accept,"The paper is relevant as it focuses on improving retrieval performance for reasoning-intensive queries, which is an important aspect of LLM/AI/ML work. The benchmark proposed is novel beyond the state of the art, and the methods described can be implemented on AWS Trainium as they use standard PyTorch/XLA operations.",,
uOhHZpoBclM7MZc38JM3,Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping,accept,"The paper is relevant as it proposes a new method for reconstructing realistic and controllable upper body avatars from monocular videos, which is relevant to LLM/AI/ML work. The method is novel as it uses a neural texture and Gaussian anchors to model the body part, which is beyond the state of the art. The method can be implemented on AWS Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
7uhTW5oBclM7MZc3HZLb,On the Benefits of Attribute-Driven Graph Domain Adaptation,reject,Bedrock throttling - max retries exceeded,,
I-hUW5oBclM7MZc3PJN6,Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping,accept,"The paper is relevant as it proposes new optimization methods for nonconvex stochastic optimization under heavy-tailed noise, which is an important problem in LLM training. The method is novel as it achieves optimal convergence rates without gradient clipping, which is a common technique. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
26ceb7270ddde187be633046b734c86c3cfaf41b9dca90f716a62492cc03087b,Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask,accept,"The paper is relevant as it focuses on modeling perspective-dependent grounding in collaborative dialogue, which is important for LLM/AI/ML work. The proposed perspectivist annotation scheme and analysis of referential misalignment are novel beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
IehUW5oBclM7MZc3M5O0,Retrieval Head Mechanistically Explains Long-Context Factuality,accept,"The paper is relevant as it focuses on LLM architectures and retrieval mechanisms, which are key to improving long-context factuality. It proposes a novel 'retrieval head' concept with credible experimental evidence. The methods described appear compatible with Trainium as they use standard transformer-based operations.",,
LOhUW5oBclM7MZc3aZMP,Homomorphism Counts as Structural Encodings for Graph Learning,accept,"The paper is relevant as it proposes a novel structural encoding method (MoSE) for graph transformers, which are a key component of LLMs. The abstract claims MoSE outperforms existing encodings and achieves state-of-the-art results, indicating novelty. The method is expressed in PyTorch and uses standard transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
c82ec1259ded48dcd98428fe92bb2cc2eee155c3d4fe4525671b936629d5c1dc,Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning,accept,"The paper is relevant as it focuses on developing an AI agent for the multi-player game of Liar's Poker, which involves reasoning under uncertainty. The agent is claimed to be novel as it outperforms large language models and human players.",,
5ehJW5oBclM7MZc3Q5KT,Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data,accept,"The paper is relevant as it proposes a new class-imbalanced margin loss function and learning algorithms for imbalanced classification, which is an important problem in LLM/AI/ML. The novel theoretical framework and empirical results demonstrate its novelty beyond the state of the art. The methods are compatible with Trainium as they are expressed in PyTorch and use standard transformer-compatible operations.",,
zehIW5oBclM7MZc3upKN,FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic  Synaptic Intelligence,accept,"The paper is relevant as it proposes a new regularization technique (FedSSI) for continual federated learning, which is an important problem in LLM/AI/ML. The method is novel as it tailors synaptic intelligence for heterogeneous data settings, going beyond existing regularization techniques. The abstract indicates the method is expressible in PyTorch and uses standard transformer-compatible operations, suggesting it is compatible with AWS Trainium.",,
8ehTW5oBclM7MZc3LJJi,Framer: Interactive Frame Interpolation,reject,"The paper is about interactive frame interpolation, which is not directly related to LLM/AI/ML work. It does not propose any novel algorithms or architectures for LLMs or other AI models.",,
wehHZpoBclM7MZc3-pNj,Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning,accept,"The paper is relevant as it proposes a new mutual information skill learning (MISL) method for reinforcement learning, which is related to LLM training efficiency and optimization. The method is novel as it introduces a new 'contrastive successor features' approach that improves on prior work. The method appears compatible with AWS Trainium as it is based on standard transformer-based operations.",,
EuhTW5oBclM7MZc34pOP,DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors,accept,The paper is relevant as it proposes a novel Diffusion Transformer-based TTS model that achieves state-of-the-art performance without relying on domain-specific factors. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.,,
yehIW5oBclM7MZc3qJId,Janus: Dual-Server Multi-Round Secure Aggregation with Verifiability for Federated Learning,accept,"The paper is relevant as it addresses secure aggregation, a key challenge in federated learning. It proposes a novel dual-server architecture with verifiability, which is an advancement beyond the state-of-the-art Flamingo protocol. However, the abstract lacks details on the specific techniques used, making it unclear if the method is compatible with AWS Trainium.",,
q-hIW5oBclM7MZc3BpI2,NeuroTree: Hierarchical Functional Brain Pathway Decoding for Mental Health Disorders,reject,"The paper is focused on fMRI-based graph neural networks for mental health disorders, which is not directly related to large language models, AI/ML, or inference efficiency. The abstract does not provide enough details to assess Trainium compatibility.",,
xOhIZpoBclM7MZc3ApOw,Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy,reject,"The paper is about multi-armed bandits with cost subsidy, which is not directly related to LLMs, AI/ML, or Trainium. The abstract does not provide enough details to judge Trainium compatibility.",,
2uhIZpoBclM7MZc3IJOq,Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models,reject,"The paper is about diffusion models for image generation, which is not directly related to LLMs or AI/ML work. The abstract does not mention any concrete technical contributions to LLM architectures, training, or inference.",,
6OhIZpoBclM7MZc3M5Pe,What to align in multimodal contrastive learning?,accept,"The paper is relevant as it proposes a new multimodal contrastive learning method (CoMM) that aims to capture redundant, unique, and synergistic information between modalities. The method is novel compared to existing multimodal contrastive learning approaches. The abstract suggests the method is compatible with Trainium as it is expressed in PyTorch and does not require any proprietary hardware.",,
sOhHZpoBclM7MZc345PF,Perm: A Parametric Representation for Multi-Style 3D Hair Modeling,reject,"This paper is about a parametric representation for 3D hair modeling, which is not directly related to LLMs, AI/ML training/inference efficiency, or evaluation/safety methods.",,
8ehIZpoBclM7MZc3Q5OU,Remove Symmetries to Control Model Expressivity and Improve Optimization,accept,"The paper is relevant as it addresses training efficiency and optimization for neural networks, which is crucial for LLM development. The proposed 'syre' algorithm is novel in removing symmetry-induced low-capacity states, and the method is model-agnostic, making it compatible with AWS Trainium.",,
9-hTW5oBclM7MZc3SZKt,"Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",accept,"The paper is relevant as it focuses on understanding memorization in language models, which is an important aspect of LLM research. The proposed taxonomy of memorization is novel and could lead to improved training and evaluation methods. The methods described appear compatible with AWS Trainium as they are based on standard transformer-based models and operations.",,
A-hTW5oBclM7MZc3jZMt,Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers,reject,"The paper is about inverse constraint learning, which is not directly related to LLMs, AI/ML, or inference efficiency. The abstract does not provide enough details to judge Trainium compatibility.",,
1OhIZpoBclM7MZc3GJPJ,Number Cookbook: Number Understanding of Language Models and How to Improve It,accept,"The paper is relevant as it focuses on improving the numerical understanding and processing ability (NUPA) of large language models, which is crucial for complex reasoning tasks. The proposed benchmark and evaluation of existing and potential techniques for enhancing NUPA are novel contributions beyond the current state of the art. The methods described in the paper appear to be compatible with AWS Trainium as they are expressed in PyTorch/XLA and use standard transformer-compatible operations.",,
c03b86be12ac30b75fbeeda5c74a2dec46e335af4ae2ce84a86566055338e122,Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models,reject,"The paper is relevant as it investigates the conspiratorial tendencies and biases in large language models, which are important for evaluating the social fidelity of LLMs. However, the methods proposed are not novel beyond the state of the art, and the paper indicates the techniques are compatible with AWS Trainium.",,
iuhHZpoBclM7MZc3tpNs,VLMaterial: Procedural Material Generation with Large Vision-Language Models,accept,"The paper is relevant as it proposes a method to generate procedural materials from images using large vision-language models, which is relevant to LLM and ML work. The method is novel as it leverages fine-tuning and program-level augmentation to generate procedural materials, which is beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
j-hHZpoBclM7MZc3vpPD,LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning,accept,"The paper is about a novel parameter-efficient fine-tuning method for large language models, which is relevant to current LLM/AI/ML work. The proposed LoCA method is a novel frequency-domain decomposition technique that surpasses the expressivity of traditional low-rank-based methods. The method can be implemented using PyTorch/XLA and is compatible with Trainium's FP16/BF16 support and transformer-compatible operations.",,
uuhIW5oBclM7MZc3VJL5,Online Robust Reinforcement Learning Through Monte-Carlo Planning,reject,"The paper is about robust reinforcement learning using Monte-Carlo planning, which is not directly related to LLMs, training/inference efficiency, or safety methods.",,
4-hIZpoBclM7MZc3K5MC,Towards Understanding the Universality of Transformers for Next-Token Prediction,accept,"The paper is relevant as it focuses on understanding the in-context autoregressive learning ability of Transformers, which is crucial for LLM architectures. The proposed causal kernel descent method is novel and provides a theoretical framework for Transformers to learn context-dependent functions. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
Y-hHZpoBclM7MZc3fpMS,Denoising Autoregressive Transformers for Scalable Text-to-Image Generation,accept,"The paper is about a novel transformer-based model for text-to-image generation, which is relevant to current LLM and AI research. The proposed DART model unifies autoregressive and diffusion approaches in a non-Markovian framework, which is novel beyond the state of the art. The model is also compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
QuhHZpoBclM7MZc3R5Ok,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,reject,"The paper is about a multi-modal approach for taxonomic classification of insects, which is not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
RehHZpoBclM7MZc3TpMX,NExUME: Adaptive Training and Inference for DNNs under Intermittent Power Environments,reject,"The paper is about adapting DNN training and inference for energy-constrained environments, which is not directly related to LLM/AI/ML work. The methods proposed, such as dynamic adjustment of dropout and quantization, are not novel beyond the state of the art.",,
uuhHZpoBclM7MZc385Nu,Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization,reject,"The paper is about crowd counting and localization, which are narrow computer vision tasks not directly related to LLMs or AI/ML research. The proposed method, Proximal Mapping Loss, is a density regression technique without clear connections to LLM architectures, training, or inference.",,
4uhJW5oBclM7MZc3NJIs,Principled Algorithms for Optimizing Generalized Metrics in Binary Classification,accept,"The paper is relevant as it proposes new algorithms for optimizing generalized metrics in binary classification, which is relevant to LLM/AI/ML work. The algorithms are novel beyond the state of the art and are compatible with AWS Trainium as they are expressed in PyTorch/XLA and use FP16/BF16 operations.",,
TuhHZpoBclM7MZc3XpOt,DeLLMa: Decision Making Under Uncertainty with Large Language Models,accept,"The paper is relevant as it focuses on using LLMs for decision-making under uncertainty, a key application area. It proposes a novel framework called DeLLMa that integrates principles from decision theory to enhance LLM performance. The method appears compatible with Trainium as it does not require any specialized hardware or custom kernels.",,
luhHZpoBclM7MZc3xZOR,TRENDy: Temporal Regression of Effective Nonlinear Dynamics,accept,"The paper is relevant as it proposes a new method (TRENDy) for learning low-dimensional, predictive models of spatiotemporal dynamics, which is applicable to LLM/AI/ML work. The method is novel as it introduces a new approach to mapping input data to a low-dimensional space of effective dynamics. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",,
0OhIZpoBclM7MZc3FJN-,MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge,accept,"The paper is relevant as it focuses on multimodal knowledge editing for large language models, which is a key area of current LLM/AI/ML research. The benchmark proposed is novel in its comprehensive approach to evaluating diverse visual knowledge editing tasks.",,
T-hHZpoBclM7MZc3YZOD,Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction,accept,"The paper is relevant as it addresses spatial reasoning and view prediction, which are key aspects of LLM/AI/ML work. The proposed GST model is novel in jointly optimizing pose estimation and novel view synthesis. The abstract indicates the model is expressible in PyTorch/XLA and uses transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
S-hHZpoBclM7MZc3WpMk,A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation,accept,"The paper proposes a novel 2D autoregressive transformer architecture for efficient fine-grained image generation, which is relevant to LLM and ML research. The abstract claims the model can generate higher quality images than previous work, indicating novelty beyond the state of the art.",,
1-hIW5oBclM7MZc38ZJh,StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models,accept,"The paper is relevant as it proposes a novel watermarking scheme for large language models, which is an important topic in LLM research. The method is novel as it enables multi-bit watermarking while preserving the original text distribution. The abstract suggests the method is compatible with Trainium as it can be implemented in PyTorch/XLA and uses standard transformer-compatible operations.",,
gehHZpoBclM7MZc3qpMS,Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix,accept,"The paper focuses on a novel pruning approach for attention matrices in LLMs, which is relevant to improving the efficiency of LLM inference. The authors claim their method outperforms existing state-of-the-art pruning techniques, indicating novelty. The method appears compatible with Trainium as it is based on PyTorch/XLA and uses standard transformer operations.",,
2OhIW5oBclM7MZc3-JJ9,Adaptive Exploration for Multi-Reward Multi-Policy Evaluation,accept,"The paper is relevant as it focuses on policy evaluation for multi-reward and multi-policy settings, which is an important problem in LLM/AI/ML. The proposed adaptive exploration scheme is novel compared to prior work on Multi-Reward Best Policy Identification. However, the abstract lacks details on the specific methods and implementations, so the Trainium compatibility is unclear.",,
A-hIZpoBclM7MZc3V5Ra,CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction,accept,"The paper is relevant as it proposes a new deep learning-based method (CryoGEN) for improving cryogenic electron tomography reconstruction, which is an important technique for visualizing subcellular structures. The method is novel as it introduces an energy-based probabilistic model to address the limitations of the previous IsoNet approach. However, the abstract does not provide enough details to assess the Trainium compatibility of the proposed method.",,
c-hHZpoBclM7MZc3k5Pm,GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack,accept,"The paper is relevant as it proposes a new score-based black-box attack method (GSBA^K) for crafting adversarial examples against classifiers, including those with multi-label learning. The method is novel as it introduces new gradient-based techniques to effectively exploit the geometry of the decision boundary. The abstract indicates the method can be expressed in PyTorch/XLA and is compatible with Trainium.",,
l-hHZpoBclM7MZc3yJOG,CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control,accept,"The paper is relevant as it proposes a method for combining motion diffusion models and reinforcement learning for physics-based character control, which is relevant to LLM and AI research. The method is novel as it introduces a closed-loop interaction between a diffusion planner and a tracking controller. The method appears compatible with AWS Trainium as it is expressed in PyTorch and uses transformer-compatible operations.",,
v-hIW5oBclM7MZc3bZK4,Geometry Informed Tokenization of Molecules for Language Model Generation,accept,"The paper is relevant as it focuses on tokenization of 3D molecular geometries for language model generation, which is relevant to LLM architectures and inference efficiency. The proposed method of canonical labeling and invariant spherical representation is novel beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
HOhUW5oBclM7MZc3GJM4,LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning,accept,"The paper is relevant as it focuses on parameter-efficient fine-tuning of pre-trained large language models, which is a key area of LLM research. The proposed LiFT method is novel as it introduces a hierarchical Bayesian model for learning-to-fine-tune, which is beyond the state of the art. The method is compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
M-hUW5oBclM7MZc3jJPz,MAP: Multi-Human-Value Alignment Palette,accept,"The paper is relevant as it addresses the challenge of aligning generative AI systems with multiple human values, which is crucial for LLM development. The proposed MAP approach is novel in formulating the alignment problem as an optimization task with user-defined constraints.",,
6ehIZpoBclM7MZc3NJOt,Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion,accept,"The paper is relevant as it focuses on multi-modal perception and uncertainty-aware sensor fusion, which are important topics in LLM/AI/ML. The proposed Cocoon framework is novel in its approach to uncertainty quantification and feature alignment across modalities.",,
DOhIZpoBclM7MZc3X5RP,Unsupervised Model Tree Heritage Recovery,accept,The paper is relevant as it addresses the important issue of model heritage and intellectual property in the context of LLMs. The proposed task of Unsupervised Model Tree Heritage Recovery is novel and could lead to better documentation and transparency of model lineage. The method appears compatible with Trainium as it is based on standard neural network operations.,,
muhHZpoBclM7MZc3ypP1,Revisiting Large-Scale Non-convex Distributionally Robust Optimization,reject,Bedrock throttling - max retries exceeded,,
IuhUW5oBclM7MZc3OJNB,Do LLMs estimate uncertainty well in instruction-following?,accept,"The paper is relevant as it focuses on evaluating the uncertainty estimation abilities of LLMs in instruction-following tasks, which is crucial for the reliability of LLMs as AI agents. The paper proposes a novel controlled evaluation setup to isolate and compare uncertainty estimation methods, which is a novel contribution beyond the current state of the art. The methods described in the paper appear to be compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",,
B-hIZpoBclM7MZc3W5QE,"Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology",accept,"The paper is relevant as it focuses on vision-language navigation for UAVs, which is a key area of LLM/AI/ML research. The proposed platform, benchmark, and methodology appear novel beyond the state of the art. However, the abstract lacks details on the specific technical implementation, making it unclear if the method is compatible with AWS Trainium.",,
8OhTW5oBclM7MZc3JpJd,SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal,accept,"The paper is relevant as it focuses on evaluating the safety refusal capabilities of large language models, which is crucial for practical deployment. It proposes a novel benchmark, SORRY-Bench, that addresses limitations of existing evaluations. The methods described appear compatible with AWS Trainium as they use PyTorch/XLA and standard transformer-based operations.",,
rOhIW5oBclM7MZc3C5JX,Position: You Can't Manufacture a NeRF,reject,"The paper is a position piece that does not propose any new technical methods for LLMs, training, or inference. It focuses on the limitations of existing 3D generative models for manufacturing, which is outside the scope of LLM/AI/ML work.",,
auhHZpoBclM7MZc3hpPp,Image-level Memorization Detection via Inversion-based Inference Perturbation,accept,"The paper is relevant as it proposes a new method (IIP) for detecting memorization in text-to-image diffusion models, which is a key challenge in current LLM/AI/ML work. The method is novel as it uses unconditional DDIM inversion and optimized prompt embeddings, going beyond existing prompt-based approaches. The method appears compatible with AWS Trainium as it is based on standard diffusion model operations.",,
BehTW5oBclM7MZc3mZPn,Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance,accept,"The paper is relevant as it focuses on improving the context reliance of large language models, a key aspect of LLM architectures and training. It proposes a novel phenomenon of 'context-parametric inversion' during instruction finetuning, which is a credible technical contribution. The methods described appear compatible with PyTorch/XLA and Trainium's capabilities.",,
7OhJW5oBclM7MZc3gZLW,Leveraging Randomness in Model and Data Partitioning for Privacy Amplification,accept,"The paper is relevant as it focuses on training efficiency and privacy amplification, which are important for LLM/AI/ML work. The proposed methods of data partitioning and model partitioning are novel beyond the state of the art. The methods are compatible with AWS Trainium as they can be expressed in PyTorch/XLA and use transformer-compatible operations.",,
_-hIZpoBclM7MZc3UpNE,Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching,accept,"The paper is relevant as it proposes a novel method for improving the covariance prediction in diffusion models, which are a key component of large language models. The abstract claims the method can substantially enhance sampling efficiency, recall rate, and likelihood, which are important for practical LLM/AI/ML work. The method appears compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
ruhHZpoBclM7MZc34pMa,Catastrophic Failure of LLM Unlearning via Quantization,accept,"The paper is relevant as it focuses on unlearning techniques for LLMs, a key challenge in training efficient and safe AI models. It proposes a novel finding that quantization can restore 'forgotten' knowledge, and the methods discussed appear compatible with AWS Trainium based on the use of standard PyTorch/XLA operations.",,
4uhIZpoBclM7MZc3KpMH,VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control,accept,"The paper is relevant as it focuses on camera control for transformer-based video diffusion models, which is an important aspect of LLM/AI/ML work. The proposed method is novel as it enables camera control for transformer-based video diffusion models, which is not addressed by existing approaches.",,
0uhIZpoBclM7MZc3F5MX,Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups,accept,"The paper is relevant as it proposes a novel approach for enforcing equivariance in neural networks, which is important for LLM and AI/ML work. The method, called LieLAC, can be integrated with pre-trained models to achieve equivariance, which is a novel contribution. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",,
suhIW5oBclM7MZc3KJKe,On the Provable Separation of Scales in Maximal Update Parameterization,accept,"The paper is relevant as it focuses on improving training efficiency and optimization for large neural networks, which is crucial for LLM development. The proposed Maximal Update Parameterization (Î¼P) technique is novel and shows promise in enabling zero-shot hyperparameter transfer across model scales. The method appears compatible with AWS Trainium as it is based on standard neural network operations.",,
PuhHZpoBclM7MZc3PZPl,DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control,accept,"The paper is relevant as it proposes a diffusion-based autoregressive motion model for real-time text-driven motion control, which is relevant to LLM and AI research. The method is novel as it enables long, complex motions that respond to streams of text descriptions. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",,
CehTW5oBclM7MZc3tJP0,Data Unlearning in Diffusion Models,accept,"The paper is relevant as it addresses the important problem of data unlearning in diffusion models, which is crucial for LLM safety and compliance. The proposed SISS method is novel as it provides theoretical guarantees for data unlearning without relying on anchor prompts. The method appears compatible with Trainium as it is based on standard PyTorch/XLA operations.",,
dOhHZpoBclM7MZc3lJOx,Conditional Testing based on Localized Conformal $p$-values,reject,"The paper is about conformal inference and conditional testing, which are not directly related to LLM/AI/ML work. The methods described do not appear to be specific to neural networks or machine learning models.",,
nuhHZpoBclM7MZc3z5NF,HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes,reject,"This paper is about novel view synthesis for 3D scenes, which is not directly related to LLMs, AI/ML architectures, or training/inference efficiency. The abstract does not provide enough details to judge Trainium compatibility.",,
qOhHZpoBclM7MZc32pOU,ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation,accept,"The paper is about improving text-to-video generation, which is relevant to LLM and AI research. It proposes a novel framework, ARLON, that integrates autoregressive and diffusion models to generate high-quality long videos, which is a novel contribution. The abstract indicates the method is compatible with PyTorch/XLA and uses standard transformer-compatible operations, suggesting it can be implemented on AWS Trainium.",,
WehHZpoBclM7MZc3cJNv,SpinQuant: LLM Quantization with Learned Rotations,accept,"The paper is relevant as it focuses on quantization techniques for improving the efficiency of large language models. The proposed SpinQuant method, which incorporates learned rotation matrices, is novel and claims to outperform existing quantization approaches. The method appears compatible with Trainium as it uses standard PyTorch/XLA operations without requiring custom CUDA-only kernels.",,
tOhHZpoBclM7MZc365N1,DataGen: Unified Synthetic Dataset Generation via Large Language Models,accept,"The paper is relevant as it focuses on improving LLM-powered synthetic data generation, which is crucial for current LLM/AI/ML work. It proposes a novel framework, DataGen, that enhances generalization, controllability, diversity, and truthfulness of generated datasets. The paper also indicates that DataGen is compatible with Trainium as it is adaptable and can be expressed in PyTorch/XLA.",,
JehUW5oBclM7MZc3R5ND,"ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels",accept,"The paper is relevant as it focuses on improving the performance of key AI kernels like GEMM and attention, which are crucial for LLM/AI/ML work. The proposed ThunderKittens framework is novel as it introduces new abstractions and templates to simplify the development of high-performance GPU kernels. The paper also claims the kernels are compatible with PyTorch/XLA and use FP16/BF16, suggesting they can run on AWS Trainium.",,
EOhTW5oBclM7MZc32ZPN,Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation,accept,"The paper is relevant as it focuses on improving LLM reasoning capabilities through a novel framework that combines LLMs and symbolic provers. The proposed method is novel in its approach to generating a scalable, diverse, and high-quality FOL reasoning dataset. The method is compatible with Trainium as it is expressed in PyTorch and uses transformer-compatible operations.",,
D-hIZpoBclM7MZc3YZTW,Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits,reject,"This paper is about batch linear contextual bandits, which is a statistical/optimization problem and not directly related to LLMs or ML architectures.",,
cehHZpoBclM7MZc3kZNR,From Commands to Prompts: LLM-based Semantic File System for AIOS,accept,"The paper is relevant as it proposes an LLM-based Semantic File System (LSFS) for prompt-driven file management, which is directly related to LLM architectures and inference efficiency. The method is novel as it introduces a comprehensive API for semantic file management functionalities, beyond traditional file systems. However, the abstract lacks details on the specific implementation, so Trainium compatibility is unclear.",,
pehHZpoBclM7MZc31pP8,Hidden in the Noise: Two-Stage Robust Watermarking for Images,accept,"The paper is relevant as it proposes a novel watermarking method for images generated by AI models, which is an important problem in the context of LLMs and AI safety. The two-stage watermarking approach with Fourier pattern augmentation is claimed to be novel and robust to various attacks, which is a credible technical contribution.",,
rOhHZpoBclM7MZc34JNI,Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge,accept,"The paper is relevant as it focuses on identifying and quantifying biases in LLM-as-a-Judge, which is an important aspect of current LLM/AI/ML work. The proposed CALM framework for bias quantification is novel beyond the state of the art. However, the abstract lacks details on the specific methods used, so the Trainium compatibility is unclear.",,
0-hIZpoBclM7MZc3GJMC,Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding,accept,"The paper is relevant as it addresses the issue of object hallucination in large vision-language models, which is an important problem in current LLM/AI/ML work. The proposed Ensemble Decoding method is novel as it introduces a new strategy to tackle this problem. The method is compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",,
Nug2ZpoBclM7MZc3wJOV,A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence,reject,"The paper is about policy gradient methods for solving imperfect-information games, which is not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
xuhIW5oBclM7MZc3nZJE,Delay-DSGN: A Dynamic Spiking Graph Neural Network with Delay Mechanisms for Evolving Graph,reject,"The paper is about a spiking neural network for dynamic graph representation learning, which is not directly related to large language models or AI/ML work on AWS Trainium.",,
6uhIZpoBclM7MZc3NZN3,Provably Accurate Shapley Value Estimation via Leverage Score Sampling,accept,"The paper is relevant as it proposes a new algorithm (Leverage SHAP) for efficient Shapley value estimation, which is a key technique in explainable AI. The method is novel as it provides provably accurate Shapley value estimates with fewer model evaluations than the widely used Kernel SHAP. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
4OhJW5oBclM7MZc3K5Iq,Rank-One Modified Value Iteration,reject,"The paper is about a novel algorithm for solving Markov decision processes, which is not directly related to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
FOhTW5oBclM7MZc37JNV,"Bridging the Data Provenance Gap Across Text, Speech, and Video",accept,"The paper is relevant as it examines data sourcing, restrictions, and representation across text, speech, and video datasets, which are crucial for LLM and AI research. The longitudinal audit and ecosystem-level analysis are novel contributions beyond the state of the art. However, the abstract lacks details on the specific methods used, so Trainium compatibility is unclear.",,
tOhIW5oBclM7MZc3MpLO,Gradient Aligned Regression via Pairwise Losses,reject,The paper is about a regression method and does not focus on LLM/AI/ML work. The proposed method is a variant of existing pairwise loss functions and does not appear to be a novel algorithm or architecture.,,
9OhTW5oBclM7MZc3OZK_,"Aligned Better, Listen Better for Audio-Visual Large Language Models",accept,"The paper is relevant as it focuses on improving audio-visual understanding in large language models, which is crucial for multimodal video understanding. The proposed Dolphin model with audio-visual multi-scale adapter and interleaved merging is novel beyond the state of the art. The method is compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",,
_ehIZpoBclM7MZc3T5O2,Animate-X: Universal Character Image Animation with Enhanced Motion Representation,accept,"The paper is relevant as it focuses on character image animation, which is related to LLM/AI/ML work. It proposes a novel framework called Animate-X that enhances motion representation for various character types, including anthropomorphic characters. However, the abstract lacks details on the specific techniques used, so the Trainium compatibility is unclear.",,
z-hIZpoBclM7MZc3E5Oj,An Effective Theory of Bias Amplification,accept,"The paper is relevant as it focuses on understanding and mitigating biases in machine learning models, which is crucial for current LLM/AI/ML work. The theory proposed is novel as it provides a unified and rigorous explanation of machine learning bias. The method is compatible with AWS Trainium as it is expressed in a PyTorch-compatible form and uses standard transformer-compatible operations.",,
3OhJW5oBclM7MZc3FJI6,"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching",accept,"The paper is relevant as it proposes a new message passing framework to address key limitations of existing graph neural networks, which are important for LLM/AI/ML work. The proposed method is novel as it introduces a variational inference framework to adapt the message passing depth and filtering, which is beyond the state of the art. The method is compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
nehHZpoBclM7MZc3zpN8,Transition Path Sampling with Improved Off-Policy Training of Diffusion Path Samplers,reject,Bedrock throttling - max retries exceeded,,
bOhHZpoBclM7MZc3iZOk,PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS,reject,"The paper is relevant as it focuses on evaluating the performance and efficiency of compressed LLMs on mobile devices, which is important for practical LLM deployment. However, the methods described are not novel beyond the state of the art. The paper indicates the methods are compatible with Trainium as they use standard quantization techniques and mobile hardware.",,
e-hHZpoBclM7MZc3oZMf,Can Reinforcement Learning Solve Asymmetric Combinatorial-Continuous Zero-Sum Games?,accept,"The paper is relevant as it proposes a new class of asymmetric combinatorial-continuous zero-sum games, which have applications in combinatorial optimization problems. The novel algorithm CCDO and its RL-based variant CCDORL are claimed to solve these games.",,
a-hHZpoBclM7MZc3iJPb,Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity,accept,"The paper is relevant as it focuses on efficient fine-tuning of LLMs, a key aspect of current LLM/AI/ML work. The proposed approach of combining static sparse ZO fine-tuning with quantization is novel beyond the state of the art. The method is compatible with AWS Trainium as it uses PyTorch/XLA and relies on transformer-compatible ops.",,
8uhTW5oBclM7MZc3MJK_,Beyond Next Token Prediction: Patch-Level Training for Large Language Models,accept,"The paper is relevant as it proposes a novel patch-level training method to improve the training efficiency of large language models, which is a key focus area in current LLM/AI/ML research. The method is novel as it introduces a new training approach beyond just token-level prediction. The abstract indicates the method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 without relying on proprietary hardware.",,
3-hIZpoBclM7MZc3JpN3,Protein Language Model Fitness is a Matter of Preference,accept,"The paper is relevant as it focuses on improving the performance of protein language models (pLMs) for zero-shot fitness estimation, which is a key task in LLM/AI/ML work. The proposed method of leveraging the likelihood of the engineered sequence to predict and improve pLM performance is novel. The method appears compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.",,
ruhIW5oBclM7MZc3FJLi,Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks,reject,"The paper is primarily a theoretical analysis of the relationship between function and representation in linear neural networks, without concrete technical contributions to LLM architectures, training, or inference.",,
b-hHZpoBclM7MZc3jpNx,gRNAde: Geometric Deep Learning for 3D RNA inverse design,accept,"The paper is relevant as it proposes a new geometric deep learning method for 3D RNA inverse design, which is relevant to LLM/AI/ML work. The method is novel compared to existing RNA design techniques. The paper states the method is implemented in PyTorch, suggesting it is compatible with AWS Trainium.",,
BuhIZpoBclM7MZc3WpQa,Learning and aligning single-neuron invariance manifolds in visual cortex,reject,"This paper is about learning and aligning invariance manifolds of visual neurons, which is not directly related to LLMs, AI/ML efficiency, or inference methods.",,
tuhHZpoBclM7MZc37ZMZ,Risk-Sensitive Variational Actor-Critic: A Model-Based Approach,reject,Bedrock throttling - max retries exceeded,,
eOhHZpoBclM7MZc3mpO0,Revisiting Source-Free Domain Adaptation: a New Perspective via Uncertainty Control,accept,"The paper is relevant as it focuses on improving training efficiency and uncertainty control for source-free domain adaptation, which is an important problem in LLM/AI/ML. The proposed method is novel as it introduces a new uncertainty control algorithm beyond existing SFDA techniques. The method appears compatible with Trainium as it is based on PyTorch and does not require specialized hardware.",,
5uhIZpoBclM7MZc3MpMp,Forte : Finding Outliers with Representation Typicality Estimation,accept,"The paper is relevant as it proposes a novel OOD detection method using representation learning and manifold estimation, which is applicable to LLM/AI/ML work. The method is novel beyond the state of the art and can be implemented on AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
6-hJW5oBclM7MZc3Z5IJ,Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel Images,accept,"The paper is relevant as it focuses on efficient long-sequence modeling for gigapixel images, a key challenge in LLM/AI/ML work. The proposed Querent framework is novel in its adaptive, query-aware approach to reducing the computational complexity of self-attention. The method appears compatible with AWS Trainium as it is expressed in PyTorch and uses efficient region-wise metadata computation.",,
9ehIZpoBclM7MZc3SZMU,Exploring the Camera Bias of Person Re-identification,reject,"This paper is about camera bias in person re-identification models, which is not directly related to LLM/AI/ML work. The methods proposed, such as feature normalization, are not novel beyond the state of the art.",,
ROhHZpoBclM7MZc3SpM4,Neural Functions for Learning Periodic Signal,accept,"The paper is relevant as it focuses on learning periodic signals, which is an important aspect of LLM/AI/ML work. The proposed novel network architecture for enhancing generalization and extrapolation performance is a credible technical contribution. The method appears compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses transformer-compatible operations.",,
CuhIZpoBclM7MZc3XZSa,Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning,accept,"The paper is relevant as it focuses on improving LLM performance on ambiguous instructions by using a multi-modal multi-turn chain-of-thought reasoning framework. The proposed method is novel as it does not notably increase computational overhead and is more general and effective compared to traditional methods. However, the abstract lacks details on the specific implementation, making it unclear if the method is compatible with AWS Trainium.",,
zehIZpoBclM7MZc3EJOz,R2Det: Exploring Relaxed Rotation Equivariance in 2D Object Detection,reject,"The paper is about a 2D object detection method using relaxed rotation equivariance, which is not directly relevant to LLM/AI/ML work. The abstract does not provide enough details to judge Trainium compatibility.",,
WOhHZpoBclM7MZc3b5OX,Weakly Supervised Video Scene Graph Generation via Natural Language Supervision,accept,"The paper is relevant as it focuses on improving video scene graph generation, a key task for LLMs. It proposes novel modules for handling temporality and action duration variability in video captions, which are credible technical contributions. The method uses PyTorch and transformer-compatible operations, making it compatible with AWS Trainium.",,
4-hJW5oBclM7MZc3OZL5,ResearchTown: Simulator of Human Research Community,reject,"The paper is about a simulator for research communities, which is not directly related to LLM architectures, training, or inference. The abstract does not provide enough details to judge Trainium compatibility.",,
8-hTW5oBclM7MZc3NJL7,CTSyn: A Foundation Model for Cross Tabular Data Generation,accept,"The paper is relevant as it proposes a new generative foundation model for tabular data, which is an important area for LLM/AI/ML work. The method is novel as it introduces a diffusion-based approach to tabular data generation, which is beyond the state of the art. The abstract indicates the method is expressible in PyTorch/XLA and uses FP16/BF16, suggesting compatibility with AWS Trainium.",,
y-hIZpoBclM7MZc3DZPF,On Disentangled Training for Nonlinear Transform in Learned Image Compression,accept,"The paper is relevant as it proposes a new method for improving the training efficiency of learned image compression models, which is a key challenge in LLM/AI/ML work. The proposed approach of using a linear auxiliary transform to disentangle the energy compaction process is novel beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.",,
lehHZpoBclM7MZc3xJPf,Training Large Language Models for Retrieval-Augmented Question Answering through Backtracking Correction,accept,"The paper is relevant as it focuses on improving retrieval-augmented question answering, a key component of large language models. The proposed Backtracking Correction method is novel in reformulating the task as a multi-step decision-making process with iterative self-correction. The method appears compatible with Trainium as it is described in PyTorch-compatible terms without any hardware-specific requirements.",,
r-hHZpoBclM7MZc34pP5,3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting,reject,"The paper is about a scene editing framework for 2D and 3D images, which is not directly related to LLM/AI/ML work. The abstract does not provide enough technical details to assess Trainium compatibility.",,
_-hTW5oBclM7MZc3dpJy,Towards a Unified and Verified Understanding of Group-Operation Networks,accept,"The paper is relevant as it focuses on understanding the internals of neural networks trained on group operations, which is relevant to LLM architectures and training. The paper claims to propose a novel explanation of the model internals that is more complete than previous work. The methods appear to be expressible in PyTorch/XLA and use standard transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
G-hUW5oBclM7MZc3E5Ps,Diffusion Bridge Implicit Models,accept,"The paper proposes a new class of diffusion bridge implicit models (DBIMs) that enable faster sampling for denoising diffusion bridge models, which are relevant to LLM/AI/ML work. The method is novel compared to widely known 2024-2025 techniques, and it appears compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",,
K-hUW5oBclM7MZc3Y5N4,A Coefficient Makes SVRG Effective,reject,Claude evaluation failed: Invalid \escape: line 5 column 79 (char 157),,
ouhFW5oBclM7MZc3-ZJs,On the Alignment between Fairness and Accuracy: from the Perspective of Adversarial Robustness,accept,"The paper is relevant as it focuses on adversarial robustness and fairness in machine learning, which are important topics in current LLM/AI/ML work. It proposes a novel unified framework for adversarial attacks against fairness and accuracy, and theoretically aligns robustness to these attacks. The methods appear compatible with AWS Trainium as they are expressed in PyTorch and use standard transformer-compatible operations.",,
MOhUW5oBclM7MZc3epP7,SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs,reject,"The paper is about learning the evolution operator for high-dimensional PDEs using a graph neural network, which is not directly related to LLMs, training efficiency, or inference efficiency.",,
D-hTW5oBclM7MZc31ZMb,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,accept,"The paper is relevant as it focuses on evaluating the robustness of LLM agents to jailbreak attacks, which is an important safety and security concern for current LLM/AI work. The benchmark proposed, AgentHarm, is novel in its scope and approach compared to existing work on LLM safety.",,
pOhGW5oBclM7MZc3AZIA,One-Pass Feature Evolvable Learning with Theoretical Guarantees,reject,"The paper is about feature evolvable learning in data streams, which is not directly related to LLMs, AI/ML, or inference efficiency. The abstract does not provide enough details to judge Trainium compatibility.",,
s-hHZpoBclM7MZc36ZPn,LoRA-Pro: Are Low-Rank Adapters Properly Optimized?,accept,"The paper is about improving the parameter-efficient fine-tuning of foundation models using LoRA, which is relevant to LLM/AI/ML work. The proposed LoRA-Pro method is novel as it introduces a strategic adjustment to the gradients of the low-rank matrices in LoRA. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
qOhHW5oBclM7MZc39ZJh,AdaWorld: Learning Adaptable World Models with Latent Actions,accept,"The paper is relevant as it proposes a novel world model learning approach for efficient adaptation, which is relevant to LLM/AI/ML work. The key idea of extracting latent actions and using them to condition the world model is novel beyond the state of the art. The method is compatible with AWS Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
YOhHZpoBclM7MZc3eZNk,Diff-Prompt: Diffusion-driven Prompt Generator with Mask Supervision,accept,"The paper is relevant as it focuses on prompt learning, a key technique for improving the performance of pre-trained multimodal models. The proposed Diff-Prompt method is novel as it uses a diffusion model to generate rich and fine-grained prompts, going beyond direct optimization of prompt parameters. However, the abstract lacks details on the specific implementation, so Trainium compatibility is unclear.",,
5OhIZpoBclM7MZc3LJMO,AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption,reject,Bedrock throttling - max retries exceeded,,
fuhHZpoBclM7MZc3pZPX,AutoCGP: Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations,reject,"The paper is about imitation learning for embodied agents and robotic tasks, which is not directly related to LLMs, AI/ML, or inference efficiency.",,
vehIW5oBclM7MZc3Y5Il,Boosting Adversarial Robustness with CLAT: Criticality Leveraged Adversarial Training,accept,The paper is relevant as it proposes a new adversarial training method (CLAT) to improve the robustness of neural networks. The method is novel as it identifies and fine-tunes critical layers to enhance clean accuracy and adversarial robustness. The method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.,,
k-hHZpoBclM7MZc3w5Mq,Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge,accept,"The paper is relevant as it focuses on improving video understanding and multi-turn dialogue in LLMs. It proposes a novel hierarchical memory system and parallel scheduling strategy, which are novel beyond the current state-of-the-art. The methods are also compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",,
R-hHZpoBclM7MZc3UZNO,Time-to-Event Pretraining for 3D Medical Imaging,accept,"The paper is relevant as it focuses on pretraining 3D medical imaging models to capture temporal context from longitudinal electronic health records, which is important for LLM/AI/ML work. The method is novel as it introduces a new pretraining framework that leverages time-to-event data, which is beyond the state of the art. The method is compatible with AWS Trainium as it uses standard PyTorch/XLA operations without any proprietary hardware requirements.",,
xehIW5oBclM7MZc3mZIp,Concept Reachability in Diffusion Models: Beyond Dataset Constraints,accept,"The paper is relevant as it focuses on improving the controllability and reachability of concepts in diffusion models, which is an important aspect of LLM/AI/ML work. The proposed methods for steering intermediate model activations to reach concepts in latent space appear novel beyond the state of the art. The methods seem compatible with AWS Trainium as they are described in general terms without relying on specialized hardware or CUDA-only custom kernels.",,
Z-hHZpoBclM7MZc3hJMy,Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,accept,"The paper is relevant as it focuses on evaluating the capabilities of spoken language models, which is crucial for LLM research. It proposes a novel, collaboratively expanding benchmark with 180 diverse tasks, going beyond the state-of-the-art.",,
_ehTW5oBclM7MZc3apLy,miniCTX: Neural Theorem Proving with (Long-)Contexts,reject,Bedrock throttling - max retries exceeded,,
NehUW5oBclM7MZc3lpNY,Diffusion Transformers for Tabular Data Time Series Generation,accept,"The paper is relevant as it proposes a Diffusion Transformers (DiTs) approach for tabular data time series generation, which is related to LLM/AI/ML work. The method is novel as it extends the DiTs framework to handle heterogeneous data and variable-length sequences, beyond the state of the art. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",,
xehIZpoBclM7MZc3A5PC,CR-CTC: Consistency regularization on CTC for improved speech recognition,accept,"The paper is relevant as it proposes a new CTC-based speech recognition method (CR-CTC) that improves upon the standard CTC. The novelty is in the consistency regularization technique, which provides credible evidence of improved performance. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",,
o-hGW5oBclM7MZc3AJII,Model Immunization from a Condition Number Perspective,reject,"The paper is about model immunization, which is not directly related to LLM architectures, training efficiency, inference efficiency, or safety methods. The abstract does not provide enough details to judge Trainium compatibility.",,
uehIW5oBclM7MZc3T5Lu,Hierarchical Reinforcement Learning with Targeted Causal Interventions,accept,"The paper is relevant as it proposes a new causal discovery algorithm for efficient hierarchical reinforcement learning, which is applicable to LLM training and optimization. The method is novel compared to existing HRL techniques. The abstract indicates the approach is expressible in PyTorch/XLA and uses standard transformer-compatible operations, suggesting compatibility with AWS Trainium.",,
SOhHZpoBclM7MZc3UpM9,MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization,reject,Bedrock throttling - max retries exceeded,,
2ehIZpoBclM7MZc3H5Pg,GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement,accept,"The paper proposes novel architectural modifications to a transformer-based 3D reconstruction model, and introduces a differentiable mesh extraction and texture refinement procedure, which are relevant to current LLM/AI/ML work. The proposed methods demonstrate state-of-the-art performance and are compatible with PyTorch/XLA and Trainium's FP16/BF16 support.",,
NOhUW5oBclM7MZc3kZNq,Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form,reject,"The paper is about robust constrained Markov Decision Processes, which is not directly related to LLMs, AI/ML, or inference efficiency. The abstract does not provide enough details to judge Trainium compatibility.",,
