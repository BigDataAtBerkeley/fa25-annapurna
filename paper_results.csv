paper_id,title,decision,reason,similarity,similar_paper
quhHZpoBclM7MZc33JNe,Generalization Bounds and Model Complexity for Kolmogorov–Arnold Networks,accept,"The paper is relevant as it provides a theoretical analysis of Kolmogorov-Arnold Networks (KANs), which are a novel architecture for LLMs. The paper establishes generalization bounds for KANs, which is a material improvement over the prior work on KANs. The methods described in the paper appear compatible with AWS Trainium based on the use of standard neural network operations.",no,
rehHZpoBclM7MZc34ZM4,Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics,reject,"The paper is primarily about a biologically constrained model of the barrel cortex, which is not directly related to LLM architectures, training efficiency, inference efficiency, or evaluation/safety methods. The abstract does not provide enough technical details to assess Trainium compatibility.",no,unclear
4uhJW5oBclM7MZc3NJIs,Principled Algorithms for Optimizing Generalized Metrics in Binary Classification,accept,"The paper is relevant as it focuses on optimizing generalized metrics for binary classification, which is important for LLM/AI/ML work. The proposed METRO algorithm is novel as it introduces a new approach to metric optimization with theoretical guarantees, beyond known techniques. The abstract indicates the method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16. The paper is distinct from the provided context as it introduces a new algorithm, rather than just packaging or tuning known methods.",no,yes
hehHZpoBclM7MZc3sJM_,Standardizing Structural Causal Models,reject,"This paper is about standardizing structural causal models, which is not directly related to LLM/AI/ML work. It does not propose any new algorithms or architectures for LLMs.",no,unclear
DOhTW5oBclM7MZc3x5Nz,Fast Uncovering of Protein Sequence Diversity from Structure,accept,"The paper is relevant as it presents a method for generating diverse protein sequences while preserving structural and functional integrity, which is relevant to LLM/AI/ML work. The method is novel as it proposes a new inverse folding algorithm to efficiently generate diverse sequences. However, the abstract lacks details on the implementation, so the Trainium compatibility is unclear.",no,unclear
tehIW5oBclM7MZc3OJJ5,HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting,accept,"The paper is relevant as it proposes a novel hypergraph neural network architecture for irregular multivariate time series forecasting, which is a key problem in LLM/AI/ML work. The method is novel as it introduces a unified representation and learning approach to capture temporal and variable dependencies, going beyond existing approaches. The method appears compatible with Trainium as it is described in PyTorch and uses standard transformer-compatible operations.",no,yes
behHZpoBclM7MZc3jJOz,LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently,accept,"The paper is relevant as it focuses on efficient lifelong learning for pre-trained language models, which is a key area of LLM/AI/ML research. The proposed LOIRE framework with novel growth operators and distillation strategies appears to be novel beyond the state of the art. The methods described seem compatible with AWS Trainium as they are expressed in PyTorch and use transformer-compatible operations.",no,yes
-uhTW5oBclM7MZc3WpK1,The 3D-PC: a benchmark for visual perspective taking in humans and machines,reject,"The paper is primarily about a benchmark for visual perspective taking, which is not directly related to LLM/AI/ML work. It does not propose any new algorithms or architectures.",no,unclear
s-hHZpoBclM7MZc36ZPn,LoRA-Pro: Are Low-Rank Adapters Properly Optimized?,accept,"The paper is relevant as it proposes a new method (LoRA-Pro) to improve the performance of low-rank adaptation (LoRA), which is a prominent technique for parameter-efficient fine-tuning of foundation models. The paper also provides theoretical derivations and extensive experiments demonstrating the effectiveness of LoRA-Pro. The method is compatible with Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
9uhTW5oBclM7MZc3RJLR,From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle,accept,"The paper is relevant as it focuses on LLM architectures and planning for multi-modal task execution. It proposes a novel system, Hive, that plans and executes chains of actions across multiple models to achieve user instructions. However, the abstract lacks details on the specific technical implementation, so Trainium compatibility is unclear.",no,unclear
J-hUW5oBclM7MZc3T5Pv,Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable,accept,"The paper is relevant as it focuses on improving the reasoning capabilities of LLMs through chain-of-thought (CoT) techniques. It proposes a novel algorithm that can learn complex tasks by decomposing them into simpler subtasks, which is shown to be more effective than learning the task directly. The method is compatible with Transformers and can be implemented on Trainium.",no,yes
EOhIZpoBclM7MZc3YpSd,Optimality of Matrix Mechanism on $\ell_p^p$-metric,reject,"The paper is focused on differential privacy and linear query optimization, which are not directly relevant to LLM/AI/ML work. The abstract does not mention any novel LLM architectures, training methods, or inference techniques.",no,unclear
3uhJW5oBclM7MZc3H5K6,Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points,reject,"The paper is focused on modeling high-dimensional systems from limited snapshot observations, which is not directly related to LLM/AI/ML work. The methods described, while novel, do not seem to have clear applications to current LLM/AI/ML practices.",no,unclear
EuhTW5oBclM7MZc34pOP,DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors,accept,The paper is relevant as it proposes a novel Diffusion Transformer (DiT)-based TTS model that achieves state-of-the-art performance without relying on domain-specific factors like phonemes and durations. The method is compatible with Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations. The paper makes a clear material improvement over the retrieved papers by demonstrating superior or comparable zero-shot performance to existing TTS models.,no,yes
8ehIZpoBclM7MZc3Q5OU,Remove Symmetries to Control Model Expressivity and Improve Optimization,accept,"The paper is relevant as it addresses the issue of symmetry-induced low-capacity states in neural networks, which is a key concern in LLM/AI/ML work. The proposed algorithm, 'syre', is novel in that it provides a theoretically justified method to remove these low-capacity states. The method is also compatible with AWS Trainium as it is model-agnostic and does not require any specialized hardware or CUDA-only custom ops.",no,yes
tuhHZpoBclM7MZc37ZMZ,Risk-Sensitive Variational Actor-Critic: A Model-Based Approach,accept,"The paper is relevant as it proposes a new risk-sensitive variational actor-critic algorithm for reinforcement learning, which is applicable to LLM/AI/ML work. The method is novel as it extends the variational framework to incorporate stochastic rewards and presents a variational model-based actor-critic approach. However, the abstract lacks sufficient details to determine if the method is compatible with AWS Trainium.",no,unclear
p-hHZpoBclM7MZc32ZOQ,Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints,reject,"This paper is about tabular data generation and enforcing constraints, which is not directly related to LLM/AI/ML work. The abstract does not mention any concrete technical contributions to LLM architectures, training, inference, or evaluation.",no,unclear
UuhHZpoBclM7MZc3Z5Mo,JPEG Inspired Deep Learning,accept,"The paper is relevant as it proposes a novel DL framework that integrates a trainable JPEG compression layer, which can improve performance and robustness. The method is compatible with Trainium as it is based on PyTorch and uses standard operations.",no,yes
hOhHZpoBclM7MZc3rJOk,Robust System Identification: Finite-sample Guarantees and Connection to Regularization,reject,"This paper is about robust system identification for nonlinear dynamical systems, which is not directly related to LLM/AI/ML work. The methods proposed do not seem to be specific to LLMs or neural networks.",no,unclear
mehHZpoBclM7MZc3ypMa,Flow-based Variational Mutual Information: Fast and Flexible Approximations,accept,"The paper is relevant as it proposes new variational estimators based on Normalizing Flows for approximating Mutual Information, which is a fundamental concept in LLM/AI/ML. The method is novel as it extends previous Gaussian-based variational estimators with more expressive flow-based distributions. The abstract indicates the method can be implemented using PyTorch/XLA and is compatible with Trainium's FP16/BF16 support.",no,yes
wehHZpoBclM7MZc3-pNj,Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning,accept,The paper is relevant as it proposes a new MISL method (contrastive successor features) that is relevant to LLM training efficiency and optimization. The method is novel as it claims to retain the performance of METRA with fewer moving parts. The method appears compatible with Trainium as it is expressed in PyTorch and uses common transformer-compatible ops.,no,yes
8uhTW5oBclM7MZc3MJK_,Beyond Next Token Prediction: Patch-Level Training for Large Language Models,accept,"The paper is relevant as it focuses on improving the training efficiency of large language models, a key aspect of LLM research. The proposed patch-level training approach is novel compared to the state-of-the-art techniques mentioned in the context, and the method can be realistically implemented on Trainium as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",no,yes
_-hTW5oBclM7MZc3dpJy,Towards a Unified and Verified Understanding of Group-Operation Networks,reject,"The paper is primarily about mechanistic interpretability of neural networks trained on group operations, which is not directly relevant to current LLM/AI/ML work. The methods described do not appear to be novel beyond the state of the art, and the abstract lacks sufficient detail to judge Trainium compatibility.",no,unclear
fOhHZpoBclM7MZc3opMu,Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks,accept,"The paper is relevant as it focuses on training general reinforcement learning agents for physical control tasks, which is an important area of LLM/AI/ML research. The proposed Kinetix framework and the trained agent's strong physical reasoning capabilities are novel beyond the state of the art. The method appears compatible with AWS Trainium as it uses PyTorch/XLA and standard transformer-compatible operations.",no,yes
M-hUW5oBclM7MZc3jJPz,MAP: Multi-Human-Value Alignment Palette,accept,"The paper is relevant as it addresses the challenge of aligning generative AI systems with multiple human values, which is a key concern in current LLM/AI/ML work. The proposed MAP approach is novel in formulating the alignment problem as an optimization task with user-defined constraints. However, the abstract lacks details on the specific technical implementation, so the Trainium compatibility is unclear.",no,unclear
6ehJW5oBclM7MZc3W5JQ,Learning Extrapolative Sequence Transformations from Markov Chains,accept,"The paper is relevant as it proposes a novel autoregressive model for learning extrapolative sequence transformations, which is applicable to LLM/AI/ML work. The method is novel compared to known techniques and can be implemented on Trainium as it uses standard PyTorch/XLA operations.",no,yes
5uhIZpoBclM7MZc3MpMp,Forte : Finding Outliers with Representation Typicality Estimation,accept,"The paper is relevant as it focuses on improving out-of-distribution (OOD) detection, which is an important problem in LLM/AI/ML. The proposed approach of using representation learning and manifold estimation to estimate typical sets is novel compared to existing OOD detection methods. The abstract indicates the method can be expressed in PyTorch/XLA and uses common transformer-compatible operations, suggesting it is compatible with AWS Trainium.",no,yes
RehHZpoBclM7MZc3TpMX,NExUME: Adaptive Training and Inference for DNNs under Intermittent Power Environments,accept,"The paper is relevant as it focuses on training and inference efficiency for DNNs under intermittent power environments, which is a key concern for LLM/AI/ML work. The proposed NExUME method is novel as it dynamically adjusts training parameters like dropout and quantization based on energy availability, which is a new approach beyond known techniques. The method is compatible with Trainium as it uses standard DNN operations and can be expressed in PyTorch/XLA.",no,yes
3uhIZpoBclM7MZc3JZO2,MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine,accept,"The paper is relevant as it focuses on improving the mathematical capabilities of large language models, which is a key area of LLM research. It proposes a novel training pipeline, MAVIS, that uses an automatic data engine to create a large-scale dataset for mathematical visual instruction tuning. The method is compatible with Trainium as it uses PyTorch/XLA and standard transformer-compatible operations.",no,yes
w-hIZpoBclM7MZc3AZMe,Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense,accept,"The paper is relevant as it proposes a novel dynamic neural network architecture for defending against model extraction attacks, which is a key challenge in LLM/AI/ML work. The method introduces a dynamic early-exit mechanism to reduce computational cost for attack queries while preserving model utility for benign queries, which is a novel contribution beyond the state of the art. The method is compatible with AWS Trainium as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",no,yes
FehTW5oBclM7MZc38ZPe,Decoupled Subgraph Federated Learning,reject,"The paper is about federated learning on graph-structured data, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training/inference efficiency methods, or evaluation/safety techniques.",no,unclear
AehTW5oBclM7MZc3gpNP,Multi-domain Distribution Learning for De Novo Drug Design,accept,"The paper is relevant as it proposes a new generative model, DrugFlow, for structure-based drug design that integrates continuous flow matching and discrete Markov bridges. It is novel as it introduces an uncertainty estimate and a joint preference alignment scheme, which are not present in the retrieved papers. The abstract indicates the method can be expressed in PyTorch/XLA and is compatible with Trainium.",no,yes
s-hIW5oBclM7MZc3LZJ7,Faster Stochastic Optimization with Arbitrary Delays via Adaptive Asynchronous Mini-Batching,reject,"This paper is about asynchronous stochastic optimization, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques. The abstract lacks details on the specific methods used, so it's unclear if they are compatible with AWS Trainium.",no,unclear
ZOhHZpoBclM7MZc3f5Pu,Competing Large Language Models in Multi-Agent Gaming Environments,accept,"The paper is relevant as it focuses on evaluating LLM decision-making abilities in multi-agent gaming environments, which is a key aspect of current LLM/AI/ML work. The proposed GAMA-Bench framework is novel as it introduces a new benchmark for comprehensive LLM evaluation, going beyond existing two-player scenarios. The methods described in the paper appear compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",no,yes
t-hHZpoBclM7MZc37pMO,SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process,accept,"The paper is relevant as it focuses on gradient compression techniques for training large language models, which is a key challenge in LLM research. The proposed SEPARATE method is novel as it leverages the low-rank properties of gradients and Hessians, and the paper provides theoretical and experimental evidence of its effectiveness. The method appears compatible with Trainium as it uses standard PyTorch/XLA operations without any proprietary hardware requirements.",no,yes
c03b86be12ac30b75fbeeda5c74a2dec46e335af4ae2ce84a86566055338e122,Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models,reject,"The paper is relevant as it investigates the conspiratorial tendencies and biases of large language models, which is an important aspect of LLM research. However, the methods described do not appear to be novel beyond the state of the art. The paper also seems compatible with AWS Trainium as it does not mention any specialized hardware requirements.",no,yes
8ehTW5oBclM7MZc3LJJi,Framer: Interactive Frame Interpolation,reject,"The paper is about interactive frame interpolation, which is a narrow end-user application without method advances in LLM/AI/ML. The abstract does not provide enough details to judge Trainium compatibility.",no,unclear
COhTW5oBclM7MZc3r5PO,StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces,accept,"The paper is relevant as it proposes a novel method (StochSync) for generating images in arbitrary spaces using a pretrained diffusion model, which is relevant to LLM/AI/ML work. The method is novel as it combines the strengths of previous approaches (Diffusion Synchronization and Score Distillation Sampling) to enable effective performance with weak conditioning. The paper also indicates that the method can be expressed in PyTorch/XLA and uses FP16/BF16, suggesting compatibility with AWS Trainium.",no,yes
8uhIZpoBclM7MZc3RJN2,MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses,accept,"The paper is relevant as it focuses on using LLMs to discover novel chemistry hypotheses, which is a key application of LLMs. The proposed method is novel as it breaks down the task into three smaller questions and develops a multi-agent framework to address them. The method is compatible with Trainium as it is described in PyTorch/XLA and uses transformer-compatible ops.",no,yes
R-hHZpoBclM7MZc3UZNO,Time-to-Event Pretraining for 3D Medical Imaging,accept,"The paper is relevant as it focuses on pretraining 3D medical imaging models to capture temporal context from longitudinal electronic health records, which is important for LLM/AI/ML work. The method is novel as it introduces a new pretraining framework that leverages time-to-event distributions, which is beyond the state of the art. However, the abstract lacks details on the specific model architecture and implementation, so the Trainium compatibility is unclear.",no,unclear
1uhIZpoBclM7MZc3G5P9,In-Context Editing: Learning Knowledge from Self-Induced Distributions,reject,Bedrock throttling - max retries exceeded,unknown,unknown
tuhIW5oBclM7MZc3PZJk,NeuronTune: Towards Self-Guided Spurious Bias Mitigation,accept,"The paper is relevant as it addresses the issue of spurious bias in deep neural networks, which is a key challenge in current LLM/AI/ML work. The proposed method, NeuronTune, is novel as it introduces a self-guided approach to mitigate spurious bias without requiring external annotations. The method is also compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
6OhIZpoBclM7MZc3M5Pe,What to align in multimodal contrastive learning?,accept,"The paper is relevant as it proposes a new multimodal contrastive learning strategy (CoMM) that goes beyond learning just shared or redundant information between modalities. The abstract claims this is a novel approach that can capture synergistic and unique information, which is a credible advance over existing multimodal contrastive learning methods. The method appears compatible with Trainium as it is described in PyTorch-friendly terms without any mention of custom CUDA-only kernels.",no,yes
xOhIW5oBclM7MZc3ipK4,Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation,accept,"The paper is relevant as it proposes a novel pruning strategy for text encoders in text-to-image diffusion models, which is a key component of large language models. The method, called Skrr, reduces memory usage without compromising performance, and is compatible with PyTorch/XLA and Trainium requirements. The paper makes a clear material improvement over existing blockwise pruning methods.",no,yes
nuhHZpoBclM7MZc3z5NF,HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes,reject,"The paper is about novel view synthesis for degraded 3D scenes, which is not directly related to LLM/AI/ML work. It does not propose any new algorithms or architectures for LLMs, training efficiency, or inference efficiency.",no,unclear
wuhHZpoBclM7MZc3_pPy,A Statistical Framework for Ranking LLM-based Chatbots,accept,"The paper proposes a novel statistical framework for ranking LLMs in open-ended conversational tasks, which is relevant to current LLM/AI/ML work. The framework introduces key advancements such as a factored tie model and modeling covariance between competitors, demonstrating substantial improvements over existing methods. The methods are expressed in Python and do not require any proprietary hardware, making them compatible with AWS Trainium.",no,yes
cehHZpoBclM7MZc3kZNR,From Commands to Prompts: LLM-based Semantic File System for AIOS,accept,"The paper is relevant as it proposes an LLM-based Semantic File System (LSFS) for prompt-driven file management in LLM Agent Operating Systems, which is directly related to LLM architectures and inference efficiency. The method is novel as it introduces a comprehensive API set for semantic file management functionalities, which is not a minor packaging/tuning of known techniques. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",no,unclear
O-hHZpoBclM7MZc3NZN7,Adversarial Mixup Unlearning,accept,"The paper is relevant as it proposes a novel adversarial mixup-based approach for machine unlearning, which is a critical technique for safeguarding data privacy in LLMs. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations. The paper makes a clear material improvement over prior work on unlearning, as evidenced by its strong performance on benchmark datasets.",no,yes
uuhHZpoBclM7MZc385Nu,Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization,reject,"The paper is about crowd counting and localization, which is a narrow end-user application without method advances in LLM/AI/ML. The abstract does not provide enough detail to judge Trainium compatibility.",no,unclear
xehIW5oBclM7MZc3mZIp,Concept Reachability in Diffusion Models: Beyond Dataset Constraints,accept,"The paper is relevant as it focuses on improving the reachability of concepts in diffusion models, which is an important aspect of LLM and AI research. The proposed method of directly steering intermediate model activations to reach concepts in latent space is novel beyond the state of the art. However, the abstract does not provide enough detail to judge the Trainium compatibility of the method.",no,unclear
d-hHZpoBclM7MZc3mZPR,The Geometry of Categorical and Hierarchical Concepts in Large Language Models,accept,"The paper is relevant as it focuses on the representation of semantic concepts in large language models, which is a key aspect of LLM architectures and training. The proposed formalization of the linear representation hypothesis for categorical and hierarchical concepts is novel beyond the state of the art. The methods described in the paper can be implemented using PyTorch/XLA and are compatible with the Trainium platform.",no,yes
8OhTW5oBclM7MZc3JpJd,SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal,accept,"The paper is relevant as it focuses on evaluating the safety refusal capabilities of large language models, which is crucial for practical deployment. It proposes a novel benchmark, SORRY-Bench, that addresses limitations of existing evaluation methods. However, the abstract does not provide enough detail to judge the Trainium compatibility of the proposed methods.",no,unclear
I-hUW5oBclM7MZc3PJN6,Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping,accept,"The paper is relevant as it proposes new optimization methods for training LLMs under heavy-tailed noise conditions, which is an important consideration for practical LLM training. The method is novel as it achieves optimal convergence rates without requiring gradient clipping, which is a common technique. The method appears compatible with Trainium as it is expressed in standard PyTorch/XLA operations without custom CUDA-only kernels.",no,yes
puhHZpoBclM7MZc315O2,Scaling Transformers for Low-Bitrate High-Quality Speech Coding,accept,"The paper is relevant as it focuses on scaling Transformer architectures for efficient speech coding, which is a key task in LLM/AI/ML. The proposed approach using a large Transformer model with a Finite Scalar Quantization bottleneck is novel compared to traditional low-parameter architectures. The method can be implemented on Trainium as it uses standard Transformer components and quantization, which are compatible with PyTorch/XLA and Trainium's capabilities.",no,yes
jehHZpoBclM7MZc3u5P4,Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data,accept,"The paper is relevant as it focuses on improving the robustness of neural networks through adversarial training, which is a key topic in LLM/AI/ML research. The proposed method is novel as it provides a theoretical understanding of how adversarial training can provably improve the learning of robust features, going beyond just empirical results. The method appears compatible with AWS Trainium as it uses standard neural network operations like convolutional layers and ReLU activations.",no,yes
xehIZpoBclM7MZc3A5PC,CR-CTC: Consistency regularization on CTC for improved speech recognition,accept,"The paper is relevant as it proposes a novel consistency regularization method (CR-CTC) for improving speech recognition performance using CTC. The method is novel as it introduces a self-distillation approach and learns contextual representation through masked prediction, which are not widely known techniques. The method is compatible with Trainium as it is expressed in PyTorch and uses standard transformer-compatible operations.",no,yes
ouhFW5oBclM7MZc3-ZJs,On the Alignment between Fairness and Accuracy: from the Perspective of Adversarial Robustness,accept,"The paper is relevant as it focuses on adversarial robustness and its connection to fairness, which are important topics in LLM/AI/ML work. The proposed method for unifying adversarial training for fairness and accuracy is novel beyond the state of the art. However, the abstract lacks details on the specific techniques used, so the Trainium compatibility is unclear.",no,unclear
tehHZpoBclM7MZc37JNW,Discrete Diffusion Schrödinger Bridge Matching for Graph Transformation,reject,"The paper is about a discrete diffusion model for graph transformation, which is not directly related to LLMs, training/inference efficiency, or evaluation/safety methods. The abstract does not provide enough details to judge Trainium compatibility.",no,unclear
uOhIW5oBclM7MZc3SZK6,Neural Guided Diffusion Bridges,accept,"The paper is relevant as it proposes a novel method for simulating conditioned diffusion processes, which is a key technique in LLM/AI/ML work. The method is novel as it eliminates the need for computationally intensive MCMC methods or reverse-process modeling, and is compatible with Trainium as it is expressed in a neural network that can run on PyTorch/XLA. The paper is distinct from the retrieved work on diffusion bridge models, as it proposes a flexible variational family for approximating the diffusion bridge path measure, which is a clear material improvement over existing methods.",no,yes
EehTW5oBclM7MZc33pMJ,Learning mirror maps in policy mirror descent,accept,"The paper is relevant as it focuses on improving policy optimization in reinforcement learning, which is an important aspect of LLM training and deployment. The proposed method of learning better mirror maps is novel compared to the commonly used negative entropy mirror map. The abstract indicates the method is compatible with PyTorch and standard RL operations, suggesting it can be implemented on AWS Trainium.",no,yes
tOhIW5oBclM7MZc3MpLO,Gradient Aligned Regression via Pairwise Losses,reject,The paper is about a regression method and does not focus on LLM/AI/ML work. It does not propose any novel architectures or training/inference methods beyond the state of the art.,no,yes
uehHZpoBclM7MZc38ZNf,Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model,accept,"The paper is relevant as it proposes a novel ECG language model for learning representations from ECG signals, which is relevant to LLM and ML work. The method is novel as it treats ECG signals as 'words' and 'sentences', which is a new perspective beyond existing eSSL methods. The method is compatible with Trainium as it is based on PyTorch and uses standard transformer-compatible operations.",no,yes
g-hHZpoBclM7MZc3q5Pz,Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation,accept,"The paper is relevant as it focuses on improving LLM performance through a novel KG-based RAG framework. The authors introduce SubgraphRAG, which uses a lightweight MLP and parallel triple-scoring mechanism for efficient and flexible subgraph retrieval, enhancing retrieval effectiveness. This is a novel approach beyond the state of the art. The method is compatible with Trainium as it is expressed in PyTorch/XLA and uses transformer-compatible operations.",no,yes
DehTW5oBclM7MZc3zJMk,Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws,accept,"The paper is relevant as it proposes a new algorithm, Adaptive Data Optimization (ADO), for optimizing data distributions during model training, which is directly applicable to LLM/AI/ML work. The method is novel as it uses scaling laws to estimate the learning potential of each data domain, without requiring external knowledge or proxy models. The abstract indicates the method is compatible with Trainium as it does not require any proprietary hardware or CUDA-only custom kernels.",no,yes
yOhIW5oBclM7MZc3oZIj,Online Differentially Private Conformal Prediction for Uncertainty Quantification,reject,"This paper is primarily about differentially private conformal prediction, which is not directly related to LLM architectures, training efficiency, or inference efficiency. The abstract does not provide enough detail to judge Trainium compatibility.",no,unclear
zuhIZpoBclM7MZc3EpOI,Three Mechanisms of Feature Learning in a Linear Network,reject,"This paper is about the learning dynamics of a linear network, which is not directly relevant to current LLM/AI/ML work. The abstract does not provide enough details to judge the Trainium compatibility.",no,unclear
_ehIZpoBclM7MZc3T5O2,Animate-X: Universal Character Image Animation with Enhanced Motion Representation,accept,"The paper is relevant as it focuses on LLM-related topics like character image animation and motion representation. It proposes a novel framework called Animate-X that enhances motion representation using techniques like Pose Indicator. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",no,unclear
3ehIZpoBclM7MZc3JJO7,LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations,accept,"The paper is relevant as it focuses on understanding the internal representations of LLMs and their ability to detect and predict errors, which are important for improving LLM safety and reliability. The proposed methods for leveraging the truthfulness information encoded in LLM representations are novel compared to existing techniques. The paper also suggests that the methods can be implemented using standard transformer-based operations, making them compatible with AWS Trainium.",no,yes
_OhIZpoBclM7MZc3TpPU,Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words,accept,"The paper is relevant as it focuses on improving the interpretability of large language models by evaluating sparse autoencoders on their ability to capture polysemous word representations. The proposed evaluation method is novel beyond the state of the art. However, the abstract lacks details on the specific methods used, so the Trainium compatibility is unclear.",no,unclear
yOhIZpoBclM7MZc3CZPo,Affine Steerable Equivariant Layer for Canonicalization of Neural Networks,accept,"The paper proposes a new steerable EquivarLayer that achieves affine equivariance, which is relevant to LLM architectures and scaling. The method is novel compared to existing techniques and appears compatible with Trainium based on the use of standard neural network operations.",no,yes
AuhIZpoBclM7MZc3VJTJ,SimPER: A Minimalist Approach to Preference  Alignment without Hyperparameters,accept,"The paper is relevant as it proposes a novel preference optimization algorithm for language model alignment, which is a key aspect of LLM training efficiency and safety. The method, SimPER, is described as hyperparameter-free and computationally efficient, making it compatible with AWS Trainium.",no,yes
9-hIZpoBclM7MZc3S5ME,ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains,accept,"The paper is relevant as it focuses on evaluating the chronological knowledge of large language models, which is an important aspect of current LLM/AI/ML work. The proposed ChroKnowBench benchmark and ChroKnowledge framework are novel approaches to assessing temporal knowledge beyond the state of the art. However, the abstract lacks details on the specific methods and implementation, so the Trainium compatibility is unclear.",no,unclear
1OhIZpoBclM7MZc3GJPJ,Number Cookbook: Number Understanding of Language Models and How to Improve It,accept,"The paper is relevant as it focuses on improving the numerical understanding and processing ability (NUPA) of large language models, which is essential for complex reasoning tasks. The paper proposes a comprehensive benchmark covering various numerical tasks and evaluates the effectiveness of existing and potential techniques for enhancing NUPA, which is novel beyond the state of the art. The methods described in the paper appear to be compatible with AWS Trainium as they are expressed in PyTorch/XLA and use FP16/BF16 operations.",no,yes
vuhHZpoBclM7MZc395N6,Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel,accept,"The paper is relevant as it proposes a new algorithm for efficient diffusion sampling, which is a key component of LLM inference. The algorithm claims to achieve better dimension dependence than prior work, indicating novelty. However, the abstract lacks details on the implementation, so Trainium compatibility is unclear.",no,unclear
IehUW5oBclM7MZc3M5O0,Retrieval Head Mechanistically Explains Long-Context Factuality,accept,"The paper is relevant as it focuses on understanding the internal mechanisms of long-context language models, specifically the role of 'retrieval heads' in retrieving relevant information. The proposed insights on the properties of retrieval heads are novel beyond the state of the art. The methods described appear compatible with Trainium as they do not require any specialized hardware or custom kernels.",no,yes
iuhHZpoBclM7MZc3tpNs,VLMaterial: Procedural Material Generation with Large Vision-Language Models,accept,"The paper is relevant as it proposes a method to generate procedural materials from input images using large vision-language models, which is relevant to current LLM/AI/ML work. The method is novel as it leverages the ability to convert procedural materials into Python programs and fine-tunes a pre-trained VLM to generate such programs, which is not a widely known technique. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",no,unclear
A-hTW5oBclM7MZc3jZMt,Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers,accept,"The paper is relevant as it proposes a novel approach to Inverse Constraint Learning (ExICL) for aligning policies with environmental constraints, which is an important problem in safe control. The abstract indicates the method uses a generative diffusion verifier, which is a novel technique beyond the state of the art. However, the abstract lacks details on the implementation, so Trainium compatibility is unclear.",no,unclear
QuhHZpoBclM7MZc3R5Ok,CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale,accept,"The paper is relevant as it proposes a multi-modal approach combining image and DNA data for taxonomic classification, which is relevant to LLM/AI/ML work. The method is novel as it uses contrastive learning to fuse the two modalities, which is beyond the state of the art. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",no,unclear
aehHZpoBclM7MZc3hpMU,Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement,reject,"The paper is relevant as it focuses on efficient image classification models for few-class tasks, which is relevant to current LLM/AI/ML work. However, the proposed methods are mostly packaging/ablations of known techniques, so the novelty is limited. The methods described appear compatible with Trainium as they use standard PyTorch/XLA operations.",no,yes
DOhIZpoBclM7MZc3X5RP,Unsupervised Model Tree Heritage Recovery,reject,Bedrock throttling - max retries exceeded,unknown,unknown
CehTW5oBclM7MZc3tJP0,Data Unlearning in Diffusion Models,accept,"The paper is relevant as it focuses on unlearning specific datapoints from diffusion models, which is a key challenge in LLM/AI/ML work. The proposed SISS method is novel as it provides theoretical guarantees and achieves Pareto optimality in preserving model quality and unlearning strength, going beyond existing unlearning techniques. However, the abstract lacks details on the technical implementation, so the Trainium compatibility is unclear.",no,unclear
BehIZpoBclM7MZc3WZQ9,Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,accept,"The paper is relevant as it focuses on the reasoning capabilities of large language models, a key aspect of current LLM/AI/ML work. It proposes a novel approach to studying the generalization strategies employed by LLMs, which is distinct from the retrieved papers. The methods described appear compatible with AWS Trainium as they do not require specialized hardware or CUDA-only custom ops.",no,yes
qehHZpoBclM7MZc325Oc,ImageFolder: Autoregressive Image Generation with Folded Tokens,accept,The paper is relevant as it proposes a new image tokenizer called ImageFolder that improves both image reconstruction and generation quality. It is novel as it introduces a flexible solution to the trade-off between token length and generation quality. The method appears compatible with Trainium as it is described in PyTorch-compatible terms without any hardware-specific requirements.,no,yes
4ehJW5oBclM7MZc3L5JD,Preserving AUC Fairness in Learning with Noisy Protected Groups,accept,"The paper is relevant as it focuses on optimizing AUC fairness, which is crucial for LLM/AI/ML applications. It proposes a novel robust AUC fairness approach under noisy protected groups, which is a novel contribution beyond the state of the art. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
JehUW5oBclM7MZc3R5ND,"ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels",accept,"The paper is relevant as it focuses on improving the performance of AI kernels, which is crucial for LLM/AI/ML work. The proposed ThunderKittens framework introduces novel abstractions and techniques that outperform prior art, indicating novelty. The paper also suggests the framework is compatible with Trainium as it uses PyTorch/XLA and relies on transformer-compatible ops.",no,yes
a-hHZpoBclM7MZc3iJPb,Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity,accept,"The paper is relevant as it addresses training efficiency and inference efficiency for LLMs, specifically through a novel zeroth-order fine-tuning approach with transferable static sparsity. The method is compatible with Trainium as it uses PyTorch/XLA, FP16/BF16, and transformer-compatible ops.",no,yes
h-hHZpoBclM7MZc3spOd,Beyond Autoregression: Fast LLMs via Self-Distillation Through Time,accept,"The paper is relevant as it focuses on improving the efficiency of large language models, specifically by exploring diffusion models as an alternative to autoregressive models. The paper claims to propose a novel distillation method for discrete diffusion models that can generate at least 32 tokens simultaneously, outperforming autoregressive models in text quality and on the LAMBADA benchmark. The method appears compatible with Trainium as it is described in PyTorch/XLA and uses FP16/BF16 without any proprietary hardware requirements. The paper is distinct from the provided references, which focus on different aspects of diffusion models.",no,yes
5-hIZpoBclM7MZc3MpP3,Reconciling Model Multiplicity for Downstream Decision Making,reject,"The paper is not about LLM architectures, training/inference efficiency, or evaluation/safety methods. It focuses on a general problem of model multiplicity in downstream decision-making, which is not directly relevant to current LLM/AI/ML work.",no,unclear
SehHZpoBclM7MZc3U5Pt,Discrete Latent Plans via Semantic Skill Abstractions,accept,"The paper is relevant as it focuses on learning language-conditioned discrete latent plans for skill learning, which is relevant to LLM and AI/ML work. The method is novel as it introduces a hierarchical approach that decouples the learning of the latent plan space from the language-conditioned high-level policy, which is not a widely known 2024-2025 technique. The method is compatible with AWS Trainium as it is expressible in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
xuhIW5oBclM7MZc3nZJE,Delay-DSGN: A Dynamic Spiking Graph Neural Network with Delay Mechanisms for Evolving Graph,accept,"The paper is relevant as it proposes a novel dynamic spiking graph neural network with a learnable delay mechanism to capture the impact of latency in information propagation, which is relevant to LLM/AI/ML work. The method is novel as it introduces a Gaussian delay kernel to adaptively delay historical information, which is beyond the state of the art. However, the abstract does not provide enough detail to determine if the method can be realistically implemented on AWS Trainium.",no,unclear
1ehIZpoBclM7MZc3GpPc,TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting,reject,"This paper is about a spiking neural network architecture for time series forecasting, which is not directly relevant to current LLM/AI/ML work. The method is not novel beyond the state of the art in spiking neural networks. The abstract lacks sufficient details to determine Trainium compatibility.",no,unclear
ruhIW5oBclM7MZc3FJLi,Not all solutions are created equal: An analytical dissociation of functional and representational similarity in deep linear neural networks,reject,"This paper is a theoretical analysis of the relationship between function and representation in linear neural networks, which is not directly relevant to current LLM/AI/ML work. The abstract does not provide enough technical details to assess Trainium compatibility.",no,unclear
5OhJW5oBclM7MZc3PpJ9,Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer,accept,"The paper is relevant as it addresses key challenges in learning to defer with multiple experts, which is critical for natural language generation and other fields. The novel surrogate loss functions and efficient algorithms with strong theoretical learning guarantees are beyond the state of the art. The methods appear compatible with Trainium as they are expressible in PyTorch/XLA and use transformer-compatible ops.",no,yes
bOhHZpoBclM7MZc3iZOk,PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS,reject,"The paper is relevant as it focuses on evaluating the performance and resource efficiency of compressed LLMs on mobile devices, which is an important aspect of LLM deployment. However, the methods described are not novel beyond the state of the art. The paper indicates the techniques used are compatible with Trainium, as they involve quantization and other optimizations suitable for mobile deployment.",no,yes
pOhHZpoBclM7MZc31pM7,Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment,accept,"The paper is relevant as it proposes a novel skill learning framework (DCSL) for improving reinforcement learning in long-horizon tasks, which is relevant to current LLM/AI/ML work. The abstract describes new technical contributions in skill representation, similarity learning, and dynamic skill length adjustment, which appear novel beyond the state of the art. However, the abstract lacks details on the specific implementation, so Trainium compatibility is unclear.",no,unclear
BehTW5oBclM7MZc3mZPn,Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance,accept,"The paper is relevant as it discusses the context reliance of LLMs and how instruction finetuning can decrease it, which is a key issue in LLM research. The phenomenon of 'context-parametric inversion' is novel and the paper provides a theoretical analysis of it. The methods described appear to be compatible with Trainium as they are expressed in PyTorch/XLA and use transformer-compatible ops.",no,yes
4OhJW5oBclM7MZc3K5Iq,Rank-One Modified Value Iteration,reject,"The paper is about a novel algorithm for solving Markov decision processes, which is not directly related to LLM/AI/ML work. The abstract does not provide enough detail to judge Trainium compatibility.",no,unclear
sOhHZpoBclM7MZc345PF,Perm: A Parametric Representation for Multi-Style 3D Hair Modeling,reject,"This paper is about 3D hair modeling, which is not directly related to LLMs, AI/ML architectures, training, or inference. The abstract does not mention any concrete technical contributions to these areas.",no,unclear
XOhHZpoBclM7MZc3dZMP,MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents,accept,"The paper is relevant as it focuses on improving multimodal retrieval for embodied agents, which is an important aspect of LLM/AI/ML work. The proposed MART method is novel as it utilizes interaction data to fine-tune an MLLM retriever, which is beyond the state of the art. The method is compatible with AWS Trainium as it is expressible in PyTorch/XLA and uses transformer-compatible ops.",no,yes
9ehIZpoBclM7MZc3SZMU,Exploring the Camera Bias of Person Re-identification,reject,"The paper is primarily about camera bias in person re-identification models, which is not directly related to LLM/AI/ML work. It does not propose any novel algorithms or architectures, and the abstract lacks details on the technical implementation to judge Trainium compatibility.",no,unclear
lOhHZpoBclM7MZc3xJMS,Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression,reject,"This paper is about instrumental variable regression using deep neural networks, which is not directly related to LLM architectures, training efficiency, or inference efficiency. The abstract does not provide enough detail to judge Trainium compatibility.",no,unclear
n-hHZpoBclM7MZc30JMd,Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective,accept,"The paper is relevant as it focuses on understanding the spectral properties of GNNs, which is important for LLM/AI/ML work. It proposes a novel benchmark to evaluate GNNs' ability to capture and leverage different frequency components of the input graph data, which is a material improvement over existing methods. The methods described appear compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",no,yes
9OhTW5oBclM7MZc3OZK_,"Aligned Better, Listen Better for Audio-Visual Large Language Models",accept,"The paper is relevant as it focuses on improving audio-visual understanding in large language models, which is an important aspect of LLM research. The proposed Dolphin model with audio-visual multi-scale adapter and interleaved merging is novel beyond the state of the art. The methods described appear to be compatible with AWS Trainium as they are expressed in PyTorch/XLA and use transformer-compatible operations.",no,yes
wuhIW5oBclM7MZc3f5J2,Computing Voting Rules with Improvement Feedback,reject,"This paper is about preference aggregation and voting rules, which is not directly related to LLM/AI/ML work. It does not propose any new algorithms or architectures for LLMs or ML models.",no,unclear
uOhHZpoBclM7MZc38JM3,Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping,reject,"The paper is primarily about 3D head and body avatar reconstruction from videos, which is not directly relevant to LLM/AI/ML work. The method seems to build on prior Gaussian splatting techniques, without clear novel algorithmic contributions.",yes,unclear
FOhTW5oBclM7MZc37JNV,"Bridging the Data Provenance Gap Across Text, Speech, and Video",reject,"This paper is a survey and analysis of dataset sourcing and attributes across text, speech, and video, without proposing any new technical methods for LLMs or AI. The abstract does not mention any specific algorithms or architectures relevant to current LLM/AI/ML work.",no,unclear
oehHZpoBclM7MZc30pPF,Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM,accept,"The paper is relevant as it focuses on improving the translation capabilities of LLMs, particularly for low-resource languages, which is a key area of interest in current LLM/AI/ML work. The proposed Mufu method, which leverages automatically generated multilingual candidates and prompting to improve translation quality, appears to be a novel approach beyond the state of the art. The abstract indicates the method can be expressed in PyTorch/XLA and uses FP16/BF16, suggesting it is compatible with AWS Trainium.",no,yes
pehGW5oBclM7MZc3ApIn,Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization,accept,"The paper is relevant as it proposes a new optimization algorithm (Stacey) for training deep neural networks, including LLMs. The abstract claims the method is novel and demonstrates improved performance over existing techniques like SGD and AdamW. The method appears compatible with Trainium as it is expressed in PyTorch and uses standard operations.",no,yes
pOhGW5oBclM7MZc3AZIA,One-Pass Feature Evolvable Learning with Theoretical Guarantees,reject,"The paper is about feature evolvable learning in data streams, which is not directly relevant to LLM/AI/ML work. The method uses online kernel learning and random Fourier features, which may be compatible with Trainium, but the abstract lacks enough detail to judge.",no,unclear
kehHZpoBclM7MZc3wZOa,Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework,accept,"The paper is relevant as it proposes a novel one-shot pruning framework for reducing the cost of vision network inference, which is relevant to LLM/AI/ML work. The method is novel as it optimizes a more global reconstruction objective to preserve deep feature representations, using a specialized second-order optimization framework. The method is compatible with Trainium as it is expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
eehHZpoBclM7MZc3m5N6,Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference,accept,"The paper is relevant as it focuses on understanding how to subvert large language models (LLMs) from following prompt-specified rules, which is an important topic in LLM research. The paper proposes a novel logic-based framework for analyzing LLMs in rule-based settings, which is novel beyond the state of the art. However, the abstract lacks details on the specific methods used, so it's unclear if the approach is compatible with AWS Trainium.",no,unclear
8OhIZpoBclM7MZc3PJNo,MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations,accept,"The paper is relevant as it proposes a novel contrastive learning approach (MIM-Refiner) to improve the performance of pre-trained Masked Image Modeling (MIM) models, which is directly applicable to current LLM/AI/ML work. The method is novel as it leverages intermediate layer representations to boost the performance of MIM models, going beyond simple parameter tuning. The paper also indicates that the approach is compatible with Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
JuhUW5oBclM7MZc3S5OO,Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization,accept,"The paper is relevant as it proposes a novel communication-efficient algorithm for federated learning, which is a key aspect of large language model training and deployment. The algorithm reduces the communication cost from O(d) to O(1) by using zeroth-order optimization, which is a novel contribution beyond the state of the art. The method is compatible with Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 without relying on proprietary hardware or CUDA-only custom kernels.",no,yes
D-hIZpoBclM7MZc3YZTW,Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits,accept,"The paper is relevant as it focuses on improving the regret bounds for batch linear contextual bandits, which is a key problem in efficient training and inference for LLMs. The proposed algorithm and analysis are novel compared to the state-of-the-art techniques. The method is compatible with Trainium as it is based on linear contextual bandits, which can be expressed in PyTorch/XLA. The paper is distinct from the provided references, which focus on different aspects of contextual bandits.",no,yes
D-hTW5oBclM7MZc31ZMb,AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents,accept,"The paper is relevant as it focuses on evaluating the robustness of LLM agents to jailbreak attacks, which is an important aspect of LLM safety and security. The benchmark proposed, AgentHarm, is novel in its scope and approach. However, the abstract lacks details on the specific technical methods used, so Trainium compatibility is unclear.",no,unclear
wOhHZpoBclM7MZc3-ZN9,Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research,accept,"The paper is relevant as it focuses on improving training efficiency and stability of goal-conditioned reinforcement learning, which is a key technique for LLM/AI/ML. The novel contributions include a high-performance codebase and benchmark that can accelerate training by up to 22x, and an assessment of design choices to stabilize and enhance training performance. The methods are compatible with Trainium as they are implemented in JAX, which can run on Trainium.",no,yes
dehHZpoBclM7MZc3l5Mu,Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint,reject,"The paper is about offline reinforcement learning and cross-domain policy adaptation, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques.",no,unclear
G-hUW5oBclM7MZc3E5Ps,Diffusion Bridge Implicit Models,accept,"The paper is relevant as it proposes a new method for fast sampling of denoising diffusion bridge models, which are a powerful variant of diffusion models. The method, called diffusion bridge implicit models (DBIMs), is novel and offers a significant speedup over the vanilla sampler. The abstract indicates the method can be implemented using PyTorch/XLA, making it compatible with AWS Trainium.",no,yes
nOhHZpoBclM7MZc3zJOy,3DIS: Depth-Driven Decoupled Image Synthesis for Universal Multi-Instance Generation,accept,"The paper is relevant as it proposes a novel framework (3DIS) for multi-instance generation, which is a key aspect of current LLM/AI/ML work. The method decouples the generation process into depth map generation and attribute rendering, which is a novel approach beyond the state of the art. The abstract indicates the method is compatible with PyTorch/XLA and can run on Trainium.",no,yes
zehIW5oBclM7MZc3upKN,FedSSI: Rehearsal-Free Continual Federated Learning with Synergistic  Synaptic Intelligence,accept,"The paper is relevant as it proposes a new regularization algorithm (FedSSI) for continual federated learning, which addresses the data heterogeneity issue. The method is novel as it tailors the synaptic intelligence technique for the CFL setting. The abstract indicates the method is compatible with Trainium as it uses standard regularization techniques without requiring rehearsal or custom hardware.",no,yes
0OhIW5oBclM7MZc3yZIS,Is Noise Conditioning Necessary for Denoising Generative Models?,accept,"The paper is relevant as it explores a novel approach to denoising generative models without noise conditioning, which is a key component of current LLM/AI/ML work. The proposed method achieves competitive performance on CIFAR-10, suggesting a material improvement over the state of the art. The abstract indicates the method can be implemented using standard transformer-compatible operations, making it compatible with AWS Trainium.",no,yes
K-hUW5oBclM7MZc3Y5N4,A Coefficient Makes SVRG Effective,reject,Claude evaluation failed: Invalid \escape: line 6 column 81 (char 189),unknown,unknown
ROhHZpoBclM7MZc3SpM4,Neural Functions for Learning Periodic Signal,accept,"The paper is relevant as it proposes a novel neural network architecture for learning periodic signals, which is applicable to LLM/AI/ML tasks. The method is novel as it introduces a new approach to enhance generalization and extrapolation performance beyond existing coordinate-based MLPs. The paper indicates the method can be expressed in PyTorch/XLA and uses FP16/BF16, suggesting compatibility with AWS Trainium.",no,yes
juhHZpoBclM7MZc3vZPG,Spurious Forgetting in Continual Learning of Language Models,accept,"The paper is relevant as it focuses on addressing the issue of 'spurious forgetting' in continual learning of language models, which is a key challenge in LLM architectures and training. The proposed Freezing strategy is a novel method that shows substantial improvements in continual learning scenarios. The method is compatible with Trainium as it is described in a framework that can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.",no,yes
iOhHZpoBclM7MZc3s5Od,Accelerating 3D Molecule Generation via Jointly Geometric Optimal Transport,accept,"The paper is relevant as it proposes a new 3D molecule generation framework based on geometric optimal transport, which is a novel approach beyond the state of the art. The method appears compatible with Trainium as it uses equivariant networks and does not mention any CUDA-only or proprietary hardware requirements. The paper is distinct from the provided reference work, which focuses on drug design rather than 3D molecule generation.",no,yes
dOhHZpoBclM7MZc3lJOx,Conditional Testing based on Localized Conformal $p$-values,reject,"The paper is about conditional testing and conformal inference, which are not directly related to LLM/AI/ML work. It does not propose any new LLM architectures, training/inference efficiency methods, or safety techniques.",no,unclear
Oug2ZpoBclM7MZc30pNS,Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models,reject,"The paper is primarily about a new analytical framework for studying the internal structure of large models, which is not directly related to LLM architectures, training efficiency, inference efficiency, or evaluation/safety methods. The abstract does not provide enough detail to assess Trainium compatibility.",no,unclear
9OhIZpoBclM7MZc3RpN3,ADAM Optimization with Adaptive Batch Selection,accept,"The paper is relevant as it proposes a new optimization method (AdamCB) that improves training efficiency, which is a key aspect of LLM/AI/ML work. The method is novel as it integrates combinatorial bandit techniques into Adam, going beyond known optimizer tweaks. The abstract indicates the method is expressible in PyTorch/XLA and uses standard operations, suggesting compatibility with AWS Trainium.",no,yes
r-hHZpoBclM7MZc34pP5,3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting,reject,"The paper is primarily about a scene editing framework for 2D and 3D objects, which is not directly related to LLM/AI/ML work. It does not propose any novel algorithms or architectures for LLMs, training efficiency, or inference efficiency.",no,unclear
F-hTW5oBclM7MZc3_pM2,Generative Classifiers Avoid Shortcut Solutions,accept,"The paper is relevant as it proposes a new generative classifier approach to address the shortcut problem in discriminative models, which is a key challenge in LLM/AI/ML. The method is novel as it introduces a new class-conditional generative model approach that outperforms existing techniques on distribution shift benchmarks. The abstract indicates the method is compatible with PyTorch/XLA and uses standard transformer-compatible operations, suggesting it can be implemented on AWS Trainium.",no,yes
VuhHZpoBclM7MZc3bZPq,Wavelet-based Positional Representation for Long Context,accept,The paper is relevant as it addresses a key challenge in LLMs - representing long-range dependencies. The proposed wavelet-based position encoding method is novel compared to existing approaches like RoPE and ALiBi. The method appears compatible with Trainium as it is based on standard transformer operations without requiring custom CUDA-only kernels.,no,yes
HehUW5oBclM7MZc3HJPQ,Beyond Worst-Case Dimensionality Reduction for Sparse Vectors,reject,"This paper is about dimensionality reduction for sparse vectors, which is not directly relevant to current LLM/AI/ML work. It does not propose any new LLM architectures, training methods, or inference techniques.",no,unclear
zuhIW5oBclM7MZc3v5JN,The Generalized Skew Spectrum of Graphs,reject,"This paper is about a new family of graph embeddings, which is not directly relevant to current LLM/AI/ML work. The abstract does not provide enough details to assess Trainium compatibility.",no,unclear
-OhIZpoBclM7MZc3S5PI,Attention with Markov: A Curious Case of Single-layer Transformers,accept,"The paper is relevant as it studies the behavior of transformers, a key LLM architecture, on Markov chains, which is relevant to understanding their sequential modeling capabilities. The findings on the contrasting behavior of single-layer transformers compared to deeper models are novel. The methods appear compatible with Trainium as they are based on standard transformer operations.",no,yes
4uhIZpoBclM7MZc3KpMH,VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control,accept,"The paper is relevant as it focuses on camera control for transformer-based video diffusion models, which is an important aspect of LLM/AI/ML work. The proposed method is novel as it enables camera control for new transformer-based video diffusion models, which is not addressed by existing approaches. However, the abstract does not provide enough detail to assess the Trainium compatibility of the method.",no,unclear
2ehJW5oBclM7MZc3A5JZ,Understanding the Limits of Deep Tabular Methods with Temporal Shift,accept,"The paper is relevant as it addresses issues with deep tabular models under temporal distribution shifts, which is an important problem in LLM/AI/ML work. The proposed solutions, including a new data splitting protocol and a temporal embedding, are novel beyond the state of the art. The methods are compatible with AWS Trainium as they are expressed in PyTorch and rely on standard transformer-compatible operations.",no,yes
Z-hHZpoBclM7MZc3hJMy,Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,accept,"The paper is relevant as it focuses on evaluating the capabilities of spoken language models, which is a key area of LLM/AI/ML research. The benchmark is novel as it expands on the previous Dynamic-SUPERB benchmark with 125 new tasks, making it the largest benchmark for speech and audio evaluation. However, the abstract does not provide enough details to determine if the methods are compatible with AWS Trainium.",no,unclear
MehUW5oBclM7MZc3gZOf,InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation,accept,"The paper is relevant as it focuses on real-time portrait editing, which is a key task in LLM/AI/ML. It proposes a novel diffusion-based model, IPNet, that addresses the trade-off between identity preservation, editing fidelity, and inference speed. The abstract indicates the method can be expressed in PyTorch/XLA and uses FP16/BF16, suggesting compatibility with AWS Trainium.",no,yes
qehHW5oBclM7MZc3-pI0,Unifying Knowledge from Diverse Datasets to Enhance Spatial-Temporal Modeling: A Granularity-Adaptive Geographical Embedding Approach,reject,"The paper is primarily about spatio-temporal forecasting and geographical data representation, which are not directly related to LLM/AI/ML work. It does not propose any novel algorithms or architectures for LLMs or other AI/ML methods.",no,unclear
kOhHZpoBclM7MZc3wJO4,Reinforcement learning with combinatorial actions for coupled restless bandits,accept,"The paper is relevant as it focuses on reinforcement learning with combinatorial actions, which is an important problem in LLM/AI/ML work. The proposed SEQUOIA algorithm is novel as it directly optimizes for long-term reward over the feasible action space. However, the abstract lacks details on the specific implementation, so the Trainium compatibility is unclear.",no,unclear
WuhHZpoBclM7MZc3cZNa,Teaching Human Behavior Improves Content Understanding Abilities Of VLMs,accept,"The paper is relevant as it focuses on improving the content understanding abilities of vision-language models (VLMs) by training them on receiver behavior data like likes, comments, and replay graphs. The proposed approach is novel as it introduces a new training technique that leverages readily available internet data to enhance VLM performance across a broad range of downstream tasks. The method appears compatible with Trainium as it can be expressed in PyTorch/XLA and uses standard transformer-compatible operations.",no,yes
c-hHZpoBclM7MZc3k5Pm,GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack,accept,"The paper is relevant as it proposes a novel score-based black-box attack method (GSBA^K) for crafting top-K adversarial examples, which is applicable to LLM/AI/ML work. The method introduces novel gradient-based techniques for finding effective initial boundary points and exploiting the geometry of the decision boundary, which appears novel beyond the state of the art. The abstract suggests the method can be implemented using standard ML primitives and is compatible with AWS Trainium.",no,yes
muhHZpoBclM7MZc3ypP1,Revisiting Large-Scale Non-convex Distributionally Robust Optimization,reject,"The paper is about distributionally robust optimization, which is not directly related to LLM/AI/ML work. It does not propose any novel LLM architectures, training methods, or inference techniques.",no,unclear
BOhTW5oBclM7MZc3k5PP,Federated Domain Generalization with Data-free On-server Matching Gradient,accept,"The paper is relevant as it proposes a novel federated domain generalization method (FedOMG) that leverages distributed model gradients to learn domain-invariant representations. The method is novel compared to existing federated learning and domain generalization techniques. The abstract indicates the method is compatible with PyTorch/XLA and uses standard transformer-compatible operations, suggesting it can be implemented on AWS Trainium.",no,yes
IOhUW5oBclM7MZc3LpMt,Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis,accept,"The paper is relevant as it focuses on the theoretical aspects of score-based discrete diffusion models, which are a key component of modern LLM and AI/ML architectures. The authors introduce a novel discrete-time sampling algorithm and provide convergence bounds, which represents a material advance beyond the state of the art.",no,unclear
u-hIW5oBclM7MZc3WZJi,Monte-Carlo Tree Search with Uncertainty Propagation via Optimal Transport,accept,"The paper is relevant as it proposes a novel backup strategy for Monte-Carlo Tree Search (MCTS) that models value and action-value nodes as Gaussian distributions and propagates uncertainty across the tree. The method is compatible with Trainium as it uses standard PyTorch/XLA operations. The paper is distinct from the provided context as it introduces a new MCTS algorithm, rather than just packaging or tuning known techniques.",no,yes
sehHZpoBclM7MZc35ZN0,Aligning Human Motion Generation with Human Perceptions,accept,"The paper is relevant as it focuses on improving the quality of generated human motions by aligning them with human perceptions, which is a key aspect of LLM/AI/ML work. The proposed MotionCritic model is a novel approach that offers a more accurate metric for assessing motion quality. The method is compatible with AWS Trainium as it can be expressed in PyTorch/XLA and uses FP16/BF16 operations.",no,yes
Oeg2ZpoBclM7MZc3z5Pj,How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?,accept,"The paper is relevant as it examines the impact of vision-language adaptation on the safety of vision-language models, which is a key concern for LLM/AI/ML work. The proposed weight merging approach to address the divergent objectives of VL adaptation and safety tuning is novel. However, the abstract lacks details on the specific techniques used, so Trainium compatibility is unclear.",no,unclear
_OhTW5oBclM7MZc3ZpIG,Faster Cascades via Speculative Decoding,accept,The paper is relevant as it focuses on improving language model inference efficiency through speculative decoding and cascading techniques. It proposes a novel approach that combines the benefits of both methods. The abstract indicates the method is compatible with Trainium as it does not mention any hardware-specific requirements.,no,yes
GOhUW5oBclM7MZc3BJMH,AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials,accept,"The paper is relevant as it focuses on training GUI agents, which is a key aspect of LLM/AI/ML work. It proposes a novel data synthesis pipeline using web tutorials, which is a material improvement over existing approaches. The method appears compatible with Trainium as it uses visual-language models and does not mention any CUDA-only custom kernels.",no,yes
