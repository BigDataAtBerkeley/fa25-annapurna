{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "332dbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f4e612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_relevance_analysis(df, y_proba, best_threshold):\n",
    "    df_adjusted = df.copy()\n",
    "    df_adjusted['relevance_probability'] = y_proba\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df_adjusted['threshold'] = best_threshold\n",
    "    df_adjusted['relevance_decision'] = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42213239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1601 pages\n",
      "Class distribution: {0: 1322, 1: 279}\n",
      "Applied positive-class oversampling. New train distribution: {0: 925, 1: 925}\n",
      "Extracting TF-IDF features...\n",
      "Training with C=1.0, max_iter=1000\n",
      "Using 'balanced' class weights\n",
      "Training classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarunshah/Desktop/Venv VS/Annapurna Fall 2025/WebScraping Engine/fa25-annapurna/.venv/lib/python3.13/site-packages/xgboost/training.py:199: UserWarning: [17:51:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8482\n",
      "\n",
      "Classification Report (threshold=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Relevant       0.89      0.94      0.91       397\n",
      "    Relevant       0.59      0.43      0.50        84\n",
      "\n",
      "    accuracy                           0.85       481\n",
      "   macro avg       0.74      0.68      0.70       481\n",
      "weighted avg       0.83      0.85      0.84       481\n",
      "\n",
      "\n",
      "ğŸ“Š Optimal threshold for 'Relevant' class: 0.30\n",
      "   Precision: 0.523, Recall: 0.536, F1: 0.529\n",
      "\n",
      "ğŸ“ˆ Metrics at different thresholds:\n",
      "   Threshold 0.4: Precision=0.563, Recall=0.476, F1=0.516\n",
      "   Threshold 0.5: Precision=0.590, Recall=0.429, F1=0.497\n",
      "   Threshold 0.6: Precision=0.623, Recall=0.393, F1=0.482\n",
      "   Threshold 0.7: Precision=0.674, Recall=0.345, F1=0.457\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.3\n",
    "random_state = 42\n",
    "oversample_positive = True\n",
    "unigram_max_features = 5000\n",
    "trigram_max_features = 2000\n",
    "c_regularization = 1.0\n",
    "max_iter = 1000\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"pdf_classifications.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df = df[(df['keep'] == '0') | (df['keep'] == '1')]\n",
    "df['keep'] = df['keep'].astype(int)\n",
    "df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "\n",
    "print(f\"Training on {len(df)} pages\")\n",
    "print(f\"Class distribution: {df['keep'].value_counts().to_dict()}\")\n",
    "\n",
    "X = df['text'].fillna('').astype(str).values\n",
    "y = df['keep'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Oversample positive class\n",
    "def oversample_positive_pages(texts, labels, random_state):\n",
    "    \"\"\"Oversample positive class to match negatives in the training split.\"\"\"\n",
    "    positives = np.where(labels == 1)[0]\n",
    "    negatives = np.where(labels == 0)[0]\n",
    "    \n",
    "    if len(positives) == 0 or len(negatives) == 0:\n",
    "        print(\"Warning: Skipping oversampling due to missing class in training split.\")\n",
    "        return texts, labels\n",
    "    \n",
    "    if len(positives) >= len(negatives):\n",
    "        return texts, labels\n",
    "    \n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    sampled_pos = rng.choice(positives, size=len(negatives), replace=True)\n",
    "    new_indices = np.concatenate([negatives, sampled_pos])\n",
    "    rng.shuffle(new_indices)\n",
    "    \n",
    "    return texts[new_indices], labels[new_indices]\n",
    "\n",
    "if oversample_positive:\n",
    "    X_train, y_train = oversample_positive_pages(X_train, y_train, random_state)\n",
    "    print(f\"Applied positive-class oversampling. New train distribution: \"\n",
    "          f\"{{0: {int((y_train == 0).sum())}, 1: {int((y_train == 1).sum())}}}\")\n",
    "\n",
    "# Extract TF-IDF statistics\n",
    "def extract_tfidf_statistics(texts, tfidf_matrix):\n",
    "    \"\"\"Extract TF-IDF-based statistical features from text.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text_lower = text.lower()\n",
    "        text_words = text_lower.split()\n",
    "        \n",
    "        # Get TF-IDF vector for this document\n",
    "        doc_tfidf = tfidf_matrix[i].toarray().flatten()\n",
    "        \n",
    "        # Statistical features from TF-IDF\n",
    "        stats = [\n",
    "            np.sum(doc_tfidf),\n",
    "            np.mean(doc_tfidf),\n",
    "            np.std(doc_tfidf),\n",
    "            np.max(doc_tfidf),\n",
    "            np.percentile(doc_tfidf, 75),\n",
    "            np.percentile(doc_tfidf, 50),\n",
    "            np.percentile(doc_tfidf, 25),\n",
    "            np.sum(doc_tfidf > 0),\n",
    "            len(text_words),\n",
    "            len(text) / max(len(text_words), 1),\n",
    "        ]\n",
    "        \n",
    "        # Additional text-based features\n",
    "        stats.extend([\n",
    "            text.count('\\n'),\n",
    "            text.count('.'),\n",
    "            text.count('='),\n",
    "            text.count('('),\n",
    "            text.count('['),\n",
    "            len([w for w in text_words if len(w) > 10]),\n",
    "        ])\n",
    "        \n",
    "        features.append(stats)\n",
    "    \n",
    "    return csr_matrix(features)\n",
    "\n",
    "# Create TF-IDF vectorizers\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=unigram_max_features,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "trigram_vectorizer = TfidfVectorizer(\n",
    "    max_features=trigram_max_features,\n",
    "    ngram_range=(3, 3),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "# Fit vectorizers and transform training data\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_trigram = trigram_vectorizer.fit_transform(X_train)\n",
    "X_train_stats = extract_tfidf_statistics(X_train, X_train_tfidf)\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_trigram, X_train_stats])\n",
    "\n",
    "# Transform test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_test_trigram = trigram_vectorizer.transform(X_test)\n",
    "X_test_stats = extract_tfidf_statistics(X_test, X_test_tfidf)\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_trigram, X_test_stats])\n",
    "\n",
    "# Normalize statistical features\n",
    "scaler = StandardScaler()\n",
    "n_stats = X_train_stats.shape[1]\n",
    "stats_start_idx = X_train_combined.shape[1] - n_stats\n",
    "\n",
    "X_train_stats_dense = X_train_combined[:, stats_start_idx:].toarray()\n",
    "X_test_stats_dense = X_test_combined[:, stats_start_idx:].toarray()\n",
    "\n",
    "X_train_stats_scaled = scaler.fit_transform(X_train_stats_dense)\n",
    "X_test_stats_scaled = scaler.transform(X_test_stats_dense)\n",
    "\n",
    "# Recombine with scaled stats\n",
    "X_train_final = hstack([\n",
    "    X_train_combined[:, :stats_start_idx],\n",
    "    csr_matrix(X_train_stats_scaled)\n",
    "])\n",
    "X_test_final = hstack([\n",
    "    X_test_combined[:, :stats_start_idx],\n",
    "    csr_matrix(X_test_stats_scaled)\n",
    "])\n",
    "\n",
    "# Train logistic regression classifier\n",
    "print(f\"Training with C={c_regularization}, max_iter={max_iter}\")\n",
    "print(\"Using 'balanced' class weights\")\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    class_weight={0: 1.0, 1: 5.0},\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state,\n",
    "    solver='lbfgs',\n",
    "    C=c_regularization\n",
    ")\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "    scale_pos_weight=3.0,  # Penalize false negatives\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "classifier.fit(X_train_final, y_train)\n",
    "\n",
    "# Evaluate with default threshold (0.5)\n",
    "y_pred = classifier.predict(X_test_final)\n",
    "y_proba = classifier.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (threshold=0.5):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Relevant', 'Relevant']))\n",
    "\n",
    "# Find optimal threshold\n",
    "thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "print(f\"\\nğŸ“Š Optimal threshold for 'Relevant' class: {best_threshold:.2f}\")\n",
    "print(f\"   Precision: {best_precision:.3f}, Recall: {best_recall:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "# Show metrics at different thresholds\n",
    "print(\"\\nğŸ“ˆ Metrics at different thresholds:\")\n",
    "for thresh in [0.4, 0.5, 0.6, 0.7]:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    print(f\"   Threshold {thresh:.1f}: Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Save model\n",
    "model_data = {\n",
    "    'vectorizer': vectorizer,\n",
    "    'trigram_vectorizer': trigram_vectorizer,\n",
    "    'scaler': scaler,\n",
    "    'classifier': classifier\n",
    "}\n",
    "\n",
    "#joblib.dump(model_data, 'pdf_classifier.pkl')\n",
    "#print(f\"\\nModel saved to pdf_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17ad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keep\n",
       "0      1322\n",
       "1       279\n",
       "]0        3\n",
       "0 0       1\n",
       "o         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keep'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['keep'] == '0') | (df['keep'] == '1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad822b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ad32df4723703816a30c7c6995b646ed4779f0bbad499...</td>\n",
       "      <td>Diffusion_Bridge_Implicit_Models.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ad32df4723703816a30c7c6995b646ed4779f0bbad499...</td>\n",
       "      <td>Diffusion_Bridge_Implicit_Models.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9ad32df4723703816a30c7c6995b646ed4779f0bbad499...</td>\n",
       "      <td>Diffusion_Bridge_Implicit_Models.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ad32df4723703816a30c7c6995b646ed4779f0bbad499...</td>\n",
       "      <td>Diffusion_Bridge_Implicit_Models.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9ad32df4723703816a30c7c6995b646ed4779f0bbad499...</td>\n",
       "      <td>Diffusion_Bridge_Implicit_Models.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...</td>\n",
       "      <td>SpinQuant__LLM_Quantization_with_Learned_Rotat...</td>\n",
       "      <td>21</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...</td>\n",
       "      <td>SpinQuant__LLM_Quantization_with_Learned_Rotat...</td>\n",
       "      <td>22</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...</td>\n",
       "      <td>SpinQuant__LLM_Quantization_with_Learned_Rotat...</td>\n",
       "      <td>23</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...</td>\n",
       "      <td>SpinQuant__LLM_Quantization_with_Learned_Rotat...</td>\n",
       "      <td>24</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>a1a727a93fda146e773f15386f9ecf746c222ac2ca9f03...</td>\n",
       "      <td>NL-Eye__Abductive_NLI_For_Images.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper_id  \\\n",
       "0     9ad32df4723703816a30c7c6995b646ed4779f0bbad499...   \n",
       "1     9ad32df4723703816a30c7c6995b646ed4779f0bbad499...   \n",
       "2     9ad32df4723703816a30c7c6995b646ed4779f0bbad499...   \n",
       "3     9ad32df4723703816a30c7c6995b646ed4779f0bbad499...   \n",
       "4     9ad32df4723703816a30c7c6995b646ed4779f0bbad499...   \n",
       "...                                                 ...   \n",
       "1604  61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...   \n",
       "1605  61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...   \n",
       "1606  61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...   \n",
       "1607  61552bb2cd5d7c1e879a0cf1d4461834daa30b4c506f69...   \n",
       "1608  a1a727a93fda146e773f15386f9ecf746c222ac2ca9f03...   \n",
       "\n",
       "                                               pdf_name page_number  \\\n",
       "0                  Diffusion_Bridge_Implicit_Models.pdf           1   \n",
       "1                  Diffusion_Bridge_Implicit_Models.pdf           2   \n",
       "2                  Diffusion_Bridge_Implicit_Models.pdf           3   \n",
       "3                  Diffusion_Bridge_Implicit_Models.pdf           4   \n",
       "4                  Diffusion_Bridge_Implicit_Models.pdf           5   \n",
       "...                                                 ...         ...   \n",
       "1604  SpinQuant__LLM_Quantization_with_Learned_Rotat...          21   \n",
       "1605  SpinQuant__LLM_Quantization_with_Learned_Rotat...          22   \n",
       "1606  SpinQuant__LLM_Quantization_with_Learned_Rotat...          23   \n",
       "1607  SpinQuant__LLM_Quantization_with_Learned_Rotat...          24   \n",
       "1608               NL-Eye__Abductive_NLI_For_Images.pdf           1   \n",
       "\n",
       "                                                   text keep  \n",
       "0     Published as a conference paper at ICLR 2025\\n...    0  \n",
       "1     Published as a conference paper at ICLR 2025\\n...    1  \n",
       "2     Published as a conference paper at ICLR 2025\\n...    1  \n",
       "3     Published as a conference paper at ICLR 2025\\n...    1  \n",
       "4     Published as a conference paper at ICLR 2025\\n...    1  \n",
       "...                                                 ...  ...  \n",
       "1604  Published as a conference paper at ICLR 2025\\n...    0  \n",
       "1605  Published as a conference paper at ICLR 2025\\n...    0  \n",
       "1606  Published as a conference paper at ICLR 2025\\n...    0  \n",
       "1607  Published as a conference paper at ICLR 2025\\n...    0  \n",
       "1608  Published as a conference paper at ICLR 2025\\n...    0  \n",
       "\n",
       "[1601 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ecc2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Published as a conference paper at ICLR 2025 Here, we additionally use a scheduler Îº(t), that satisfies Îº(0) = 1 and Îº(1) = 0, to control the rate at which the geodesic distance d between x0 and x1 decreases (Chen & Lipman, 2023): d(xt, x1) = Îº(t)d(x0, x1). (15) Thus, we obtain the loss function LÏ‡(Î¸) = Et,q(x1),p(x0)âˆ¥vÎ¸(xt, t) âˆ’Ë™xtâˆ¥2 (16) with Ë™xt = âˆ’Ë™Îº(t) logx0(x1). (17) Here, the learned vector field vÎ¸(xt, t) represents vectors on the tangent plane. A.1.3 MARKOV BRIDGE MODEL The molecular graph consists of discrete entities (node and edge types) and can therefore not be easily modeled in the flow matching framework. While discrete diffusion formulations (Austin et al., 2021; Vignac et al., 2022) can be used in principle, we decided to employ the Markov bridge model (Igashov et al., 2023) instead which is conceptually more similar to the flow matching scheme used for the continuous variables as it does not require a closed-form prior. The Markov bridge model captures the stochastic dependency between two discrete-valued spaces X and Y. It defines a Markov process between fixed start and end points z0 = x and z1 = y, respectively, through a sequence of N + 1 random variables (zt=i/N)N i=0 for which p(zt|z0, z0+âˆ†t, ..., ztâˆ’âˆ†t, z1 = y) = p(zt|ztâˆ’âˆ†t, z1 = y) (18) with âˆ†t = 1/N. Additionally, since the process is pinned at its end point, we have p(z1 = y|z1âˆ’âˆ†t, y) = 1. (19) Each transition is given by p(zt+âˆ†t|zt, z1 = y) = Cat(zt+âˆ†t; Qtzt) (20) where zt âˆˆ{0, 1}K is a one-hot representation of the current category and Qt is a transition matrix parameterised as Qt := Qt(y) = Î²tI + (1 âˆ’Î²t)y1T K. (21) Any intermediate state of the Markov chain can be probed in closed form: p(zt|z0, z1) = Cat(zt; Â¯Qtâˆ’âˆ†tz0) (22) with Â¯Qt = QtQtâˆ’âˆ†t...Q0 = Â¯Î²tI + (1 âˆ’Â¯Î²t)y1T K. (23) In this work, we choose a linear schedule for Â¯Î² = 1 âˆ’t which implies Î²t = Â¯Î²t/Â¯Î²tâˆ’âˆ†t = (1 âˆ’ t)/(1 âˆ’t + âˆ†t). The neural network Î¸ approximates y so that we can sample from the Markov bridge without know- ing the true final state. It is trained by maximizing the following lower bound on the log-likelihood qÎ¸ of the end point y given the start point x log qÎ¸(y|x) â‰¥âˆ’T Â· Et,ztâˆ¼p(zt|x,y)DKL(p(zt+âˆ†t|zt, y)||qÎ¸(zt+âˆ†t|zt)) =: âˆ’LMBM(Î¸). (24) A.1.4 TRAINING LOSS Our overall loss function is a weighted sum of the previously introduced loss terms: L = Î»coordLcoord + Î»Ï‡LÏ‡ + Î»aLMBM, atom + Î»bLMBM, bond. (25) 18 ',\n",
       "       'Published as a conference paper at ICLR 2025 1 3 5 7 9 11 13 15 17 Multi-Property Controllability (Ranking) 1 3 5 7 9 11 13 15 17 Retrosynthesis Success Rate (Ranking) Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-3-70B Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B In-Context Learning Supervised Fine-Tuning Llamole (a) LLM for Drug (Small Molecule) Design 1 3 5 7 9 11 13 15 17 Multi-Property Controllability (Ranking) 1 3 5 7 9 11 13 15 17 Retrosynthesis Success Rate (Ranking) Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Llama-3-70B Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B Mistral-7B Qwen2-7B Llama-3.1-8B In-Context Learning Supervised Fine-Tuning Llamole (b) LLM for Material (Polymer) Design Figure 5: Overall Comparison of LLMs for Controllability and Synthesizability: Performance is ranked by averaged BA/MAE (x-axis) and retrosynthesis success rate (y-axis). Circle size indicates model size. LLMs with ICL, SFT, and Llamole are highlighted in blue, orange, and red, respectively. Table 2: Retrosynthetic Success Rate: Best results are in bold , best baseline results are in italic . In-Context Learning Llama-2-7B Mistral-7B Qwen2-7B Llama-3-8B Flan-T5-XXL Granite-13B Llama-2-13B Mistral-8x7B Llama-2-70B Drug (%) 0.1 0.2 0.0 5.5 0.4 0.6 1.2 1.6 1.0 Material (%) 0.3 0.4 0.0 4.8 0.8 1.6 1.2 1.7 0.8 Supervised Fine-tuning BioNavi for Llamole Mistral-7B Qwen2-7B Llama-3-8B Llama-3.1-8B DiGress Mistral-7B Qwen2-7B Llama-3.1-8B Drug (%) 1.5 0.2 0.6 0.8 18.0 29.9 33.7 35.1 Material (%) 0.8 0.1 0.7 0.8 15.4 14.3 17.9 17.6 (3) Larger models without domain-specific adaptation do not necessarily perform better in molecular designs. We calculate the average Pearson correlation coefficient between model size and molecular design metrics, yielding a value of 0.366, indicating a weak correlation (below 0.5) between size and performance. We also compare LLM performance with GraphGA, which has been shown to be simple yet powerful (Gao et al., 2022; Liu et al., 2024c). Our observations confirm that GraphGA serves as a strong molecular design baseline, challenging most LLM models with ICL and SFT in generating molecules with precise multi-condition control. 5.2 RQ2: DISCUSSION ON CONTROLLABLE MOLECULAR GENERATION 5.2.1 ABLATION STUDIES ON LLM AND GRAPH DIT SYNERGY We investigate the synergy effect of Graph DiT and LLM in Llamole for molecule controllability. We first remove text conditions ctext. In this case, Graph DiT uses a learned â€œnullâ€ embedding to represent the dropped condition ctext = âˆ…. Next, we remove the drug or material property conditions {ci}M i associated with the question. Results in Figure 6 show that text instructions enhance the chemical structure understanding ability of Graph DiT, while Llamole leverages Graph DiTâ€™s capabilities with property inputs to generate molecules with desirable properties. 5.2.2 CASE STUDIES FOR PROPERTY AND STRUCTURE CONTROLLABILITY In Figure 7, Llamole can design a satisfactory molecule that meets both functional and structural constraints. Functionally, the oracle function confirms that the properties of BACE and HIV align with the criteria. Structurally, all key criteria are satisfied, including molecular weight, â€œtwo aromatic rings,â€ and â€œconnected to aliphatic chains.â€ Llamole also adds details for structure design, such as a carboxyl (â€“COOH) group and an amino group (â€“NH2). While the amino group is present in the 8 ',\n",
       "       'Published as a conference paper at ICLR 2025 Task Metric Whisper- LLaMA LTU-AS SALMONN 7B SALMONN 13B Qwen- Audio- Chat Qwen2- Audio- 7B-Inst. WavLLM MU- LLaMA GAMA- IT Multi-Speaker Detection (LibriSpeech- TestClean) LLM-Câ†‘ 49.00% 31.50% 16.50% 21.50% 50.50% 52.00% 20.50% 33.50% 48.50% Speech Command Recognition (AudioMNIST) NARâ†“ 4.93% 76.80% 2.93% 0.00% 0.93% 0.00% 0.53% 5.20% 75.87% Speech Command Recognition (AudioMNIST) ACCâ†‘ 0.8890 0.8330 0.9670 0.7410 0.9620 0.7710 0.9340 0.1000 0.2430 Speech Text Matching (LibriSpeech- TestClean) LLM-Câ†‘ 87.00% 32.00% 58.00% 57.00% 68.00% 84.50% 50.50% 41.50% 18.50% Speech Text Matching (LibriSpeech- TestOther) LLM-Câ†‘ 82.00% 34.00% 57.00% 56.50% 72.00% 87.50% 55.50% 45.00% 18.50% Speech Text Matching (LJSpeech) LLM-Câ†‘ 82.50% 32.00% 59.50% 55.50% 64.50% 76.00% 49.00% 47.50% 16.00% Spoken Term Detection (LibriSpeech- TestClean) LLM-Câ†‘ 77.00% 29.50% 54.50% 53.50% 52.50% 62.00% 57.00% 48.50% 39.50% Spoken Term Detection (LibriSpeech- TestOther) LLM-Câ†‘ 77.00% 27.50% 50.00% 50.00% 48.50% 66.00% 55.50% 40.00% 39.50% Spoken Term Detection (LJSpeech) LLM-Câ†‘ 83.50% 28.50% 53.00% 51.50% 47.00% 79.50% 50.00% 40.00% 31.00% SUPERB Keyword Spotting (Speech Commands V1-Test) LLM-Câ†‘ 75.50% 4.50% 30.50% 31.50% 30.00% 59.00% 47.50% 27.50% 34.00% SUPERB Query by Example (Quesst14-Eval) LLM-Câ†‘ 48.50% 33.50% 49.00% 51.50% 37.50% 53.50% 49.00% 52.00% 31.00% Stuttering Detection (SEP28k) LLM-Câ†‘ 49.50% 45.40% 50.50% 50.30% 50.20% 52.60% 58.90% 49.80% 48.50% Voice Disorder Classification (VOICED) LLM-Câ†‘ 16.30% 13.50% 13.50% 17.30% 19.70% 17.30% 20.20% 22.10% 13.50% Conversation Matching LLM-Câ†‘ 77.80% 9.30% 57.40% 37.00% 51.90% 61.10% 61.10% 1.90% 18.50% Dialogue Act Classification (SLUE-HVB) LLM-Câ†‘ 21.80% 18.90% 22.80% 3.30% 9.40% 20.60% 17.30% 4.90% 4.10% Dialogue Act Classification (Daily Talk) LLM-Câ†‘ 29.00% 14.50% 37.00% 44.00% 43.50% 30.50% 42.00% 20.50% 11.00% Dialogue Act Pairing (Daily Talk) LLM-Câ†‘ 51.00% 16.00% 53.50% 48.00% 43.00% 48.50% 39.50% 50.00% 39.00% SUPERB Intent Classification (SLURP) LLM-Câ†‘ 33.00% 7.50% 13.50% 3.00% 31.00% 26.50% 20.50% 4.00% 2.00% Continued on next page 58 ',\n",
       "       ...,\n",
       "       'Published as a conference paper at ICLR 2025 Jiatong Shi, Dan Berrebbi, William Chen, En-Pei Hu, Wei-Ping Huang, Ho-Lam Chung, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung yi Lee, and Shinji Watanabe. Ml-superb: Multilingual speech universal performance benchmark. In INTERSPEECH 2023, pp. 884â€“888, 2023. doi: 10.21437/Interspeech.2023-1316. Jiatong Shi, Yueqian Lin, Xinyi Bai, Keyi Zhang, Yuning Wu, Yuxun Tang, Yifeng Yu, Qin Jin, and Shinji Watanabe. Singing voice data scaling-up: An introduction to ace-opencpop and ace-kising. arXiv preprint arXiv:2401.17619, 2024a. Jiatong Shi, Shih-Heng Wang, William Chen, Martijn Bartelds, Vanya Bannihatti Kumar, Jinchuan Tian, Xuankai Chang, Dan Jurafsky, Karen Livescu, Hung yi Lee, and Shinji Watanabe. Ml- superb 2.0: Benchmarking multilingual speech models across modeling constraints, languages, and datasets. In Interspeech 2024, pp. 1230â€“1234, 2024b. doi: 10.21437/Interspeech.2024-2248. Kazuki Shimada, Archontis Politis, Parthasaarathy Sudarsanam, Daniel A Krause, Kengo Uchida, Sharath Adavanne, Aapo Hakala, Yuichiro Koyama, Naoya Takahashi, Shusuke Takahashi, et al. Starss23: An audio-visual dataset of spatial recordings of real scenes with spatiotemporal anno- tations of sound events. Advances in Neural Information Processing Systems, 36, 2024. Suwon Shon, Ankita Pasad, Felix Wu, Pablo Brusco, Yoav Artzi, Karen Livescu, and Kyu J Han. Slue: New benchmark tasks for spoken language understanding evaluation on natural speech. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7927â€“7931. IEEE, 2022. Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-Yi Lee, Karen Livescu, and Shinji Watanabe. Slue phase-2: A benchmark suite of diverse spoken language understanding tasks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8906â€“8937, 2023. David Snyder, Guoguo Chen, and Daniel Povey. Musan: A music, speech, and noise corpus. arXiv preprint arXiv:1510.08484, 2015. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Fabian-Robert StÂ¨oter, Soumitro Chakrabarty, EmanuÂ¨el Habets, and Bernd Edler. LibriCount, a dataset for speaker count estimation, 2018. URL https://doi.org/10.5281/zenodo. 1216072. Dan Stowell, Michael D Wood, Hanna PamuÅ‚a, Yannis Stylianou, and HervÂ´e Glotin. Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge. Methods in Ecology and Evolution, 10(3):368â€“380, 2019. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024a. Yuxun Tang, Jiatong Shi, Yuning Wu, and Qin Jin. Singmos: An extensive open-source singing voice dataset for mos prediction. arXiv preprint arXiv:2406.10911, 2024b. Mi Tian, Ajay Srinivasamurthy, Mark Sandler, and Xavier Serra. A study of instrument-wise on- set detection in beijing opera percussion ensembles. In 2014 ieee international conference on acoustics, speech and signal processing (icassp), pp. 2159â€“2163. IEEE, 2014. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÂ´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 18 ',\n",
       "       'Published as a conference paper at ICLR 2025 E PROOFS E.1 PROOF OF PROPOSITION 1 Lemma 1. The following statements are equivalent. (a) Î¸ minimizes LFM(Î¸; Q01, xt, t). (b) Î¸ minimizes LGFM(Î¸; Q01, xt, t). (c) DÎ¸(xt, t) = Ex0âˆ¼Q0|t(Â·|xt)[x0]. Proof. We first observe that (writing DÎ¸ in place of DÎ¸(xt, t) for brevity) âˆ‡DÎ¸LGFM(Î¸; Q01, xt, t) = Ï•âŠ¤Ï•{âˆ‡DÎ¸LFM(Î¸; Q01, xt, t)} (43) and since Ï• is invertible, Ï•âŠ¤Ï• is invertible as well, which implies âˆ‡DÎ¸LGFM(Î¸; Q01, xt, t) = 0 â‡â‡’âˆ‡DÎ¸LFM(Î¸; Q01, xt, t) = 0. (44) Because both LGFM(Î¸; Q01, xt, t) and LFM(Î¸; Q01, xt, t) are strongly convex w.r.t. DÎ¸, this means Î¸ minimizes LGFM(Î¸; Q01, xt, t) iff Î¸ minimizes LFM(Î¸; Q01, xt, t) iff DÎ¸(xt, t) = Ex0âˆ¼Q0|t(Â·|xt)[x0]. (45) This establishes the equivalence of the three claims. Lemma 2. Let Âµ be a Ïƒ-finite measure. If f > g on a set A with Âµ(A) > 0, R A f dÂµ > R A g dÂµ. Proof. By linearity of integrals, we can assume g = 0. Since f > 0 on A, we may express A = âˆªâˆ n=1An, An := {x âˆˆA : f(x) > 1/n}. (46) Since Âµ(A) > 0, there is n such that Âµ(An) > 0. Otherwise, by subadditivity of measures, Âµ(A) â‰¤Pâˆ n=1 Âµ(An) = 0 (47) which contradicts the assumption Âµ(A) > 0. It follows that R A f dÂµ â‰¥ R An f dÂµ â‰¥ R An 1 n dÂµ = Âµ(An) n > 0. (48) This establishes the claim. Proof of Proposition 1. Denote the measure of (t, xt) where t âˆ¼unif(0, 1) and xt âˆ¼Qt as Âµ. (Assuming DÎ¸ can approximate a sufficiently large set of functions), define Î¸âˆ—as the neural net parameter which satisfies DÎ¸âˆ—(xt, t) = Ex0âˆ¼Q0|t(Â·|xt)[x0] (49) for any (xt, t) such that LGFM(Î¸; Q01, xt, t) â‰¥LGFM(Î¸âˆ—; Q01, xt, t) (50) or equivalently, LFM(Î¸; Q01, xt, t) â‰¥LFM(Î¸âˆ—; Q01, xt, t) (51) for any (xt, t) and Î¸ by Lemma 1. We now show that a minimizer of Eq. (13) minimizes Eq. (1). Suppose Î¸ minimizes Eq. (13), but there is a set A with positive measure, i.e., Âµ(A) > 0, such that DÎ¸(xt, t) Ì¸= Ex0âˆ¼Q0|t(Â·|xt)[x0] (52) for all (xt, t) âˆˆA. By Lemma 1, LGFM(Î¸; Q01, xt, t) > LGFM(Î¸âˆ—; Q01, xt, t) (53) 20 ',\n",
       "       'Published as a conference paper at ICLR 2025 Table 12: Examples of questions where Mistral-7b-Instruct consistently provided incorrect answers but occasionally generated the correct one. In these instances, the probe successfully identified the right answer. For each question, the model was sampled 30 times. Question Wrong Answer Count Correct Answer Count Which town in southeast Wales became a UNESCO World Heritage Site in 2000? Caerleon 29 Blaenavon 1 From her first US film musical â€Down Argentina Wayâ€ (1940), who became famous for extravagant hats, jewellery and dresses? Betty Grable 27 Carmen Miranda 1 Men Against the Sea and Pitcairnâ€™s Island were two sequels to what famous novel? Robinson Crusoe 18 Mutiny on the Bounty 2 Which is the only property on a traditional UK Monopoly board which s south of the River Thames? Coventry Street 17 Old Kent Road 3 Which French Canadian became Prime Minister of Canada in 1968? Jean ChrÂ´etien 21 Pierre Elliott Trudeau 4 Table 13: Various answer choice strategies, non-instruct models. Mistral-7b TriviaQA Math Winobias Error type Greedy Random Majority Probing Greedy Random Majority Probing Greedy Random Majority Probing All 0.63 Â±0.003 0.54 Â±0.004 0.65 Â±0.002 0.62 Â±0.003 0.25 Â±0.018 0.36 Â±0.022 0.49 Â±0.019 0.60 Â±0.017 0.69 Â±0.016 0.58 Â±0.009 0.62 Â±0.009 0.83 Â±0.006 (A) Refuses to answer 0.08 Â±0.015 0.04 Â±0.009 0.00 Â±0.000 0.13 Â±0.007 0.01 Â±0.009 0.04 Â±0.019 0.00 Â±0.000 0.22 Â±0.033 - - - - (B1) All 1.00 Â±0.000 1.00 Â±0.000 1.00 Â±0.000 1.00 Â±0.000 - - - - - - - - (B2) Most 0.98 Â±0.001 0.84 Â±0.009 1.00 Â±0.000 0.91 Â±0.002 0.96 Â±0.024 0.84 Â±0.031 1.00 Â±0.000 0.86 Â±0.041 0.96 Â±0.004 0.73 Â±0.009 0.95 Â±0.003 0.91 Â±0.009 (C) Consistently incorrect (C1) All 0.00 Â±0.003 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 - - - - - - - - (C2) Most 0.03 Â±0.014 0.20 Â±0.008 0.00 Â±0.000 0.27 Â±0.036 - - - - 0.19 Â±0.010 0.30 Â±0.026 0.00 Â±0.000 0.70 Â±0.007 (D) Two competing 0.48 Â±0.006 0.36 Â±0.008 0.52 Â±0.015 0.54 Â±0.016 - - - - 0.73 Â±0.018 0.54 Â±0.022 0.47 Â±0.030 0.85 Â±0.019 (E) Many answers (E1) Non correct 0.01 Â±0.004 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.01 Â±0.010 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 - - - - (E2) Correct appears 0.38 Â±0.009 0.21 Â±0.006 0.42 Â±0.015 0.38 Â±0.009 0.09 Â±0.010 0.17 Â±0.034 0.36 Â±0.020 0.62 Â±0.035 - - - - Llama-8b TriviaQA Math Winobias Error type Greedy Sampling Majority Probing Greedy Sampling Majority Probing Greedy Sampling Majority Probing All 0.66 Â±0.002 0.58 Â±0.003 0.68 Â±0.003 0.68 Â±0.002 0.30 Â±0.023 0.47 Â±0.022 0.62 Â±0.014 0.70 Â±0.021 0.73 Â±0.011 0.61 Â±0.005 0.66 Â±0.016 0.84 Â±0.006 (A) Refuses to answer 0.08 Â±0.005 0.07 Â±0.011 0.00 Â±0.000 0.16 Â±0.011 0.00 Â±0.007 0.04 Â±0.015 0.00 Â±0.000 0.25 Â±0.025 - - - - (B) Consistently correct (B1) All 1.00 Â±0.000 1.00 Â±0.000 1.00 Â±0.000 1.00 Â±0.000 - - - - - - - - (B2) Most 0.98 Â±0.001 0.87 Â±0.002 1.00 Â±0.000 0.95 Â±0.002 0.77 Â±0.024 0.88 Â±0.025 1.00 Â±0.000 0.97 Â±0.014 0.98 Â±0.005 0.75 Â±0.004 1.00 Â±0.000 0.94 Â±0.003 (C) Consistently incorrect (C1) All 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 - - - - - - - - (C2) Most 0.06 Â±0.013 0.18 Â±0.009 0.00 Â±0.000 0.35 Â±0.043 - - - - 0.25 Â±0.026 0.29 Â±0.023 0.00 Â±0.000 0.65 Â±0.022 (D) Two competing 0.44 Â±0.029 0.42 Â±0.035 0.53 Â±0.020 0.66 Â±0.030 - - - - 0.73 Â±0.025 0.47 Â±0.019 0.41 Â±0.037 0.86 Â±0.014 (E) Many answers (E1) Non correct 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 0.00 Â±0.000 - - - - (E2) Correct appears 0.46 Â±0.009 0.34 Â±0.009 0.53 Â±0.007 0.54 Â±0.005 0.14 Â±0.015 0.17 Â±0.025 0.44 Â±0.047 0.65 Â±0.031 - - - - 33 '],\n",
       "      shape=(1850,), dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec80d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1601 pages\n",
      "Class distribution: {0: 1322, 1: 279}\n",
      "\n",
      "Loading transformer model: all-MiniLM-L6-v2\n",
      "This may take a moment on first run (downloads model)...\n",
      "\n",
      "Generating embeddings for training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:02<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:01<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1120, 384)\n",
      "\n",
      "Training with C=1.0, max_iter=1000\n",
      "Using custom class weights: {0: 1.0, 1: 5.0} to penalize false negatives\n",
      "Training classifier...\n",
      "\n",
      "Test accuracy: 0.7152\n",
      "\n",
      "Classification Report (threshold=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Relevant       0.89      0.74      0.81       397\n",
      "    Relevant       0.32      0.58      0.42        84\n",
      "\n",
      "    accuracy                           0.72       481\n",
      "   macro avg       0.61      0.66      0.61       481\n",
      "weighted avg       0.79      0.72      0.74       481\n",
      "\n",
      "\n",
      "ğŸ“Š Optimal threshold for 'Relevant' class: 0.45\n",
      "   Precision: 0.304, Recall: 0.690, F1: 0.422\n",
      "\n",
      "ğŸ“ˆ Metrics at different thresholds:\n",
      "   Threshold 0.3: Precision=0.245, Recall=0.893, F1=0.385\n",
      "   Threshold 0.3: Precision=0.269, Recall=0.857, F1=0.409\n",
      "   Threshold 0.4: Precision=0.290, Recall=0.750, F1=0.419\n",
      "   Threshold 0.4: Precision=0.304, Recall=0.690, F1=0.422\n",
      "   Threshold 0.5: Precision=0.325, Recall=0.583, F1=0.417\n",
      "   Threshold 0.5: Precision=0.312, Recall=0.464, F1=0.373\n",
      "   Threshold 0.6: Precision=0.340, Recall=0.393, F1=0.365\n",
      "   Threshold 0.6: Precision=0.413, Recall=0.310, F1=0.354\n",
      "   Threshold 0.7: Precision=0.463, Recall=0.226, F1=0.304\n",
      "   Threshold 0.7: Precision=0.571, Recall=0.143, F1=0.229\n",
      "\n",
      "ğŸ“‹ Confusion Matrix at optimal threshold (0.45):\n",
      "   True Negatives:   264  |  False Positives:  133\n",
      "   False Negatives:   26  |  True Positives:    58\n",
      "\n",
      "   False Negative Rate: 0.310 (proportion of relevant pages missed)\n",
      "   False Positive Rate: 0.335 (proportion of irrelevant pages included)\n",
      "\n",
      "âœ… Training complete!\n",
      "\n",
      "To use this model for predictions:\n",
      "1. Load the model: model_data = joblib.load('pdf_classifier_transformer.pkl')\n",
      "2. Generate embeddings: embeddings = model_data['embedding_model'].encode(new_texts)\n",
      "3. Get predictions: probs = model_data['classifier'].predict_proba(embeddings)[:, 1]\n",
      "4. Apply threshold: predictions = (probs >= 0.45).astype(int)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, f1_score, \n",
    "    precision_score, recall_score\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "\n",
    "# Configuration\n",
    "test_size = 0.3\n",
    "random_state = 42\n",
    "oversample_positive = False\n",
    "c_regularization = 1.0\n",
    "max_iter = 1000\n",
    "embedding_model_name = 'all-MiniLM-L6-v2'  # Fast and efficient\n",
    "# Alternative: 'all-mpnet-base-v2' for better quality but slower\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"pdf_classifications.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df = df[(df['keep'] == '0') | (df['keep'] == '1')]\n",
    "df['keep'] = df['keep'].astype(int)\n",
    "df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "\n",
    "print(f\"Training on {len(df)} pages\")\n",
    "print(f\"Class distribution: {df['keep'].value_counts().to_dict()}\")\n",
    "\n",
    "X = df['text'].astype(str).values\n",
    "y = df['keep'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Oversample positive class\n",
    "def oversample_positive_pages(texts, labels, random_state):\n",
    "    \"\"\"Oversample positive class to match negatives in the training split.\"\"\"\n",
    "    positives = np.where(labels == 1)[0]\n",
    "    negatives = np.where(labels == 0)[0]\n",
    "    \n",
    "    if len(positives) == 0 or len(negatives) == 0:\n",
    "        print(\"Warning: Skipping oversampling due to missing class in training split.\")\n",
    "        return texts, labels\n",
    "    \n",
    "    if len(positives) >= len(negatives):\n",
    "        return texts, labels\n",
    "    \n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    sampled_pos = rng.choice(positives, size=len(negatives), replace=True)\n",
    "    new_indices = np.concatenate([negatives, sampled_pos])\n",
    "    rng.shuffle(new_indices)\n",
    "    \n",
    "    return texts[new_indices], labels[new_indices]\n",
    "\n",
    "if oversample_positive:\n",
    "    X_train, y_train = oversample_positive_pages(X_train, y_train, random_state)\n",
    "    print(f\"Applied positive-class oversampling. New train distribution: \"\n",
    "          f\"{{0: {int((y_train == 0).sum())}, 1: {int((y_train == 1).sum())}}}\")\n",
    "\n",
    "# Load transformer model and generate embeddings\n",
    "print(f\"\\nLoading transformer model: {embedding_model_name}\")\n",
    "print(\"This may take a moment on first run (downloads model)...\")\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "print(\"\\nGenerating embeddings for training data...\")\n",
    "X_train_embeddings = embedding_model.encode(\n",
    "    X_train, \n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"Generating embeddings for test data...\")\n",
    "X_test_embeddings = embedding_model.encode(\n",
    "    X_test,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Embedding shape: {X_train_embeddings.shape}\")\n",
    "\n",
    "# Train logistic regression classifier with custom class weights\n",
    "print(f\"\\nTraining with C={c_regularization}, max_iter={max_iter}\")\n",
    "print(\"Using custom class weights: {0: 1.0, 1: 5.0} to penalize false negatives\")\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    class_weight={0: 1.0, 1: 5.0},  # Penalize false negatives 3x more\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state,\n",
    "    solver='lbfgs',\n",
    "    C=c_regularization\n",
    ")\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "classifier.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Evaluate with default threshold (0.5)\n",
    "y_pred = classifier.predict(X_test_embeddings)\n",
    "y_proba = classifier.predict_proba(X_test_embeddings)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (threshold=0.5):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Relevant', 'Relevant']))\n",
    "\n",
    "# Find optimal threshold\n",
    "thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "print(f\"\\nğŸ“Š Optimal threshold for 'Relevant' class: {best_threshold:.2f}\")\n",
    "print(f\"   Precision: {best_precision:.3f}, Recall: {best_recall:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "# Show metrics at different thresholds\n",
    "print(\"\\nğŸ“ˆ Metrics at different thresholds:\")\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "    prec = precision_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, pos_label=1, zero_division=0)\n",
    "    print(f\"   Threshold {thresh:.1f}: Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Calculate and display confusion matrix at optimal threshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_optimal = (y_proba >= best_threshold).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nğŸ“‹ Confusion Matrix at optimal threshold ({best_threshold:.2f}):\")\n",
    "print(f\"   True Negatives:  {tn:4d}  |  False Positives: {fp:4d}\")\n",
    "print(f\"   False Negatives: {fn:4d}  |  True Positives:  {tp:4d}\")\n",
    "print(f\"\\n   False Negative Rate: {fn/(fn+tp):.3f} (proportion of relevant pages missed)\")\n",
    "print(f\"   False Positive Rate: {fp/(fp+tn):.3f} (proportion of irrelevant pages included)\")\n",
    "\n",
    "# Save model\n",
    "model_data = {\n",
    "    'embedding_model_name': embedding_model_name,\n",
    "    'embedding_model': embedding_model,\n",
    "    'classifier': classifier,\n",
    "    'optimal_threshold': best_threshold\n",
    "}\n",
    "\n",
    "# Uncomment to save:\n",
    "# joblib.dump(model_data, 'pdf_classifier_transformer.pkl')\n",
    "# print(f\"\\nModel saved to pdf_classifier_transformer.pkl\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"\\nTo use this model for predictions:\")\n",
    "print(f\"1. Load the model: model_data = joblib.load('pdf_classifier_transformer.pkl')\")\n",
    "print(f\"2. Generate embeddings: embeddings = model_data['embedding_model'].encode(new_texts)\")\n",
    "print(f\"3. Get predictions: probs = model_data['classifier'].predict_proba(embeddings)[:, 1]\")\n",
    "print(f\"4. Apply threshold: predictions = (probs >= {best_threshold:.2f}).astype(int)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d84948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
