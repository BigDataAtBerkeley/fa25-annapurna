
âœ… Cluster Health:
{
  "cluster_name": "478852001205:research-papers",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 1,
  "number_of_data_nodes": 1,
  "discovered_master": true,
  "discovered_cluster_manager": true,
  "active_primary_shards": 15,
  "active_shards": 15,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 2,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 88.23529411764706
}

Indices:
- .plugins-ml-config (1 docs, status=green)
- .opensearch-observability (0 docs, status=green)
- research-papers-v2 (130 docs, status=yellow)
- .kibana_1 (1 docs, status=green)
- .opendistro_security (10 docs, status=green)
- research-papers (2 docs, status=yellow)

Documents from 'research-papers-v2':

================================================================================
Document #1 (ID: ROikD5oBclM7MZc3xo-n)
================================================================================
  abstract: Modern language models have demonstrated remarkable reasoning capabilities by using chain-of-thought (CoT). One hypothesis about the inner workings of CoT is that it breaks down originally complex tasks into smaller subtasks that are more amenable to learning. We formalize this notion by showing possibility and impossibility results of learning from in-context demonstrations with and without CoT. In particular, with CoT, we examine a family of learning algorithms that learn a task step-by-step, capable of composing simpler functions from individual reasoning steps to form an overall complex function. This process reduces the difficulty of learning a task to that of the hardest reasoning step in the chain. Moreover, we prove Transformers can express this algorithm and thus they can efficiently in-context learn arbitrary tasks as long as these tasks can be decomposed into a finite number of subtasks, each of which are efficiently learnable. In contrast, without CoT, we demonstrate that there exist tasks that are inherently unlearnable by the same algorithm. Overall, our results suggest several provably effective ways for decomposing target problems to instantiate CoT. Empirically, we demonstrate our proposed CoT construction significantly enhances the reasoning capabilities of real-world LLMs in solving challenging arithmetic reasoning tasks, including learning polynomials and Boolean formulas.
  authors: ['Chenxiao Yang', 'Zhiyuan Li', 'David Wipf']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199048350
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Chain-of-Thought_Provably_Enables_Learning_the__Otherwise__Unlearnable.pdf
  sha_abstract: bfd8f4faa8a534eac651d564ba4e8c06e23f674cc628d719487541d41f4e6e89
  title: Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable
  title_normalized: chainofthought_provably_enables_learning_the_otherwise_unlearnable

================================================================================
Document #2 (ID: QuikD5oBclM7MZc3vo9L)
================================================================================
  abstract: The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse capabilities of GPUs suggests we might we need a wide variety of techniques to achieve high performance. However, our work explores if a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like operations, (2) at the thread-block level, we provide templates for asynchronously overlapping operations, and (3) at the grid-level, TK helps hide block launch, tear-down, and memory costs. We show the value of TK by providing simple & diverse kernels that match or outperform prior art. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40$\% on attention backwards, $8\times$ on state space models, and $14\times$ on linear attention.
  authors: ['Benjamin Frederick Spector', 'Simran Arora', 'Aaryan Singhal']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199046210
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: ThunderKittens__Simple__Fast__and___textit_Adorable___Kernels.pdf
  sha_abstract: b51d3b3a548104606f5960672c19f35f3003ca696c500df8c77d196f1fed3abf
  title: ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels
  title_normalized: thunderkittens_simple_fast_and_textitadorable_kernels

================================================================================
Document #3 (ID: P-ikD5oBclM7MZc3rY9_)
================================================================================
  abstract: Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. 
Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. 
Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models.
To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions.
Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. 
The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.
  authors: ['Juyeon Heo', 'Miao Xiong', 'Christina Heinze-Deml']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199041910
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Do_LLMs_estimate_uncertainty_well_in_instruction-following_.pdf
  sha_abstract: b76bd4af926c2820594d770badbcca82d202a958509c80a74e267c99e086fe7c
  title: Do LLMs estimate uncertainty well in instruction-following?
  title_normalized: do_llms_estimate_uncertainty_well_in_instructionfollowing

================================================================================
Document #4 (ID: R-ikD5oBclM7MZc31I9k)
================================================================================
  abstract: We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.
  authors: ['Ilya Loshchilov', 'Cheng-Ping Hsieh', 'Simeng Sun']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199051870
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: nGPT__Normalized_Transformer_with_Representation_Learning_on_the_Hypersphere.pdf
  sha_abstract: 9c456b374f8847aab6ebe1d4ef3e2027b4eee0cdf458dc29c93701a128c4c5d9
  title: nGPT: Normalized Transformer with Representation Learning on the Hypersphere
  title_normalized: ngpt_normalized_transformer_with_representation_learning_on_the_hypersphere

================================================================================
Document #5 (ID: T-ikD5oBclM7MZc3_o9s)
================================================================================
  abstract: Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.
  authors: ['Jaeseong Lee', 'Taewoong Kang', 'Marcel Buehler']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199062630
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: SurFhead__Affine_Rig_Blending_for_Geometrically_Accurate_2D_Gaussian_Surfel_Head_Avatars.pdf
  sha_abstract: 35510a1106bed9af79c5d43b4c845d70c609807d591f74717aa0757c90b1dc20
  title: SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars
  title_normalized: surfhead_affine_rig_blending_for_geometrically_accurate_2d_gaussian_surfel_head_avatars

================================================================================
Document #6 (ID: SuikD5oBclM7MZc35I-p)
================================================================================
  abstract: Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.
  authors: ['Nikunj Saunshi', 'Nishanth Dikkala', 'Zhiyuan Li']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199056010
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Reasoning_with_Latent_Thoughts__On_the_Power_of_Looped_Transformers.pdf
  sha_abstract: 117b4a8d098d1d18e19701927e217ce52282f3b874faf73c968c79e3a9335147
  title: Reasoning with Latent Thoughts: On the Power of Looped Transformers
  title_normalized: reasoning_with_latent_thoughts_on_the_power_of_looped_transformers

================================================================================
Document #7 (ID: TuikD5oBclM7MZc3-Y-8)
================================================================================
  abstract: Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc. However, real-time portrait editing presents three significant challenges: identity preservation, fidelity to editing instructions, and fast model inference. Given that these aspects often present a trade-off, concurrently addressing them poses an even greater challenge. While diffusion-based image editing methods have shown promising capabilities in personalized image editing in recent years, they lack a dedicated focus on portrait editing and thus suffer from the aforementioned problems as well. To address the gap, this paper introduces an Instant-Portrait Network (IPNet), the first one-step diffusion-based model for portrait editing. We train the network in two stages. We first employ an annealing identity loss to train an Identity Enhancement Network (IDE-Net), to ensure robust identity preservation. We then train the IPNet using a novel diffusion Multi-Objective Distillation approach that integrates adversarial loss, identity distillation loss, and a novel Facial-Style Enhancing loss. The Diffusion Multi-Objective Distillation approach efficiently reduces inference steps, ensures identity consistency, and enhances the precision of instruction-based editing. Extensive comparison with prior models demonstrates IPNet as a superior model in terms of identity preservation, text fidelity, and inference speed.
  authors: ['Zhixin Lai', 'Keqiang Sun', 'Fu-Yun Wang']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199061430
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: InstantPortrait__One-Step_Portrait_Editing_via_Diffusion_Multi-Objective_Distillation.pdf
  sha_abstract: a96a8d7ce5169f7c148e3963356c95e372419b327c7c667c395ace49f4ed295b
  title: InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation
  title_normalized: instantportrait_onestep_portrait_editing_via_diffusion_multiobjective_distillation

================================================================================
Document #8 (ID: UOilD5oBclM7MZc3BI92)
================================================================================
  abstract: Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.
  authors: ['Xinran Wang', 'Qi Le', 'Ammar Ahmed']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199064170
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: MAP__Multi-Human-Value_Alignment_Palette.pdf
  sha_abstract: a65c1b15145123c0b86f1dbd80ffb6423b7733a839681a30f91c9c5deaebdce8
  title: MAP: Multi-Human-Value Alignment Palette
  title_normalized: map_multihumanvalue_alignment_palette

================================================================================
Document #9 (ID: S-ikD5oBclM7MZc36Y9Y)
================================================================================
  abstract: Despite extensive research, recovering PDE expressions from experimental observations often involves symbolic regression. This method generally lacks the incorporation of meaningful physical insights, resulting in outcomes lacking clear physical interpretations. Recognizing that the primary interest of Machine Learning for Science (ML4Sci) often lies in understanding the underlying physical mechanisms or even discovering new physical laws rather than simply obtaining mathematical expressions, this paper introduces a novel ML4Sci task paradigm. This paradigm focuses on interpreting experimental data within the framework of prior physical hypotheses and theories, thereby guiding and constraining the discovery of PDE expressions. We have formulated this approach as a nonlinear mixed-integer programming (MIP) problem, addressed through an efficient search scheme developed for this purpose. Our experiments on newly designed Fluid Mechanics and Laser Fusion datasets demonstrate the interpretability and feasibility of this method.
  authors: ['Mingquan Feng', 'Yixin Huang', 'Yizhou Liu']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199057205
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: PhysPDE__Rethinking_PDE_Discovery_and_a_Physical_HYpothesis_Selection_Benchmark.pdf
  sha_abstract: 33c054c02e54d51c654da4c978d0c79c569da73345e9225f21cf57f089d47316
  title: PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark
  title_normalized: physpde_rethinking_pde_discovery_and_a_physical_hypothesis_selection_benchmark

================================================================================
Document #10 (ID: SeikD5oBclM7MZc33o9B)
================================================================================
  abstract: Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset.
  authors: ['Linus Bao', 'Emily Jin', 'Michael M. Bronstein']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199054377
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Homomorphism_Counts_as_Structural_Encodings_for_Graph_Learning.pdf
  sha_abstract: aabb8fb183e66f16ec0357e532f2beb59779f15090a469d651cfcfca639d9150
  title: Homomorphism Counts as Structural Encodings for Graph Learning
  title_normalized: homomorphism_counts_as_structural_encodings_for_graph_learning

================================================================================
Document #11 (ID: TOikD5oBclM7MZc37Y8v)
================================================================================
  abstract: Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically --  previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\ell_\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\ell_\infty$-geometry rather than the more common $\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\ell_\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions.
  authors: ['Shuo Xie', 'Mohamad Amin Mohamadi', 'Zhiyuan Li']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199058213
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Adam_Exploits___ell__infty_-geometry_of_Loss_Landscape_via_Coordinate-wise_Adaptivity.pdf
  sha_abstract: 2882a305d1a330b405e2214343e56b0bf3d6e093dcac7a434d7f79d153a7c913
  title: Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity
  title_normalized: adam_exploits_ellinftygeometry_of_loss_landscape_via_coordinatewise_adaptivity

================================================================================
Document #12 (ID: TeikD5oBclM7MZc38Y_K)
================================================================================
  abstract: We present a novel framework, StochastIc Network Graph Evolving operatoR (SINGER), for learning the evolution operator of high-dimensional partial differential equations (PDEs). The framework uses a sub-network to approximate the solution at the initial time step and stochastically evolves the sub-network parameters over time by a graph neural network to approximate the solution at later time steps. The framework is designed to inherit the desirable properties of the parametric solution operator, including graph topology, semigroup, and stability, with a theoretical guarantee. Numerical experiments on 8 evolution PDEs of 5,10,15,20-dimensions show that our method outperforms existing baselines in almost all cases (31 out of 32), and that our method generalizes well to unseen initial conditions, equation dimensions, sub-network width, and time steps.
  authors: ['Mingquan Feng', 'Yixin Huang', 'Weixin Liao']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199059385
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: SINGER__Stochastic_Network_Graph_Evolving_Operator_for_High_Dimensional_PDEs.pdf
  sha_abstract: b04c6c4799ed42a5aae0b811473dfe01f0a9bac59403e97dd062a9b07ba0afa5
  title: SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs
  title_normalized: singer_stochastic_network_graph_evolving_operator_for_high_dimensional_pdes

================================================================================
Document #13 (ID: SOikD5oBclM7MZc32I_1)
================================================================================
  abstract: Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our empirical analysis finds that, for deeper neural networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\alpha$-SVRG. Our results show $\alpha$-SVRG better optimizes models, consistently reducing training loss compared to the baseline and standard SVRG across various model architectures and multiple image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at github.com/davidyyd/alpha-SVRG.
  authors: ['Yida Yin', 'Zhiqiu Xu', 'Zhiyuan Li']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199053012
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: A_Coefficient_Makes_SVRG_Effective.pdf
  sha_abstract: 239e70df6a9d37c53d43744e790606c1a9a9cc55401e7c5e7658c9167d3edd5d
  title: A Coefficient Makes SVRG Effective
  title_normalized: a_coefficient_makes_svrg_effective

================================================================================
Document #14 (ID: XeilD5oBclM7MZc3Qo8o)
================================================================================
  abstract: Multi-modal Large Language Models (MLLMs) have recently showcased superior proficiency in general visual scenarios. However, we identify their mathematical capabilities remain under-explored with three areas to be improved: visual encoding of math diagrams, diagram-language alignment, and chain-of-thought (CoT) reasoning. This draws forth an urgent demand for an effective training paradigm and a large-scale, comprehensive dataset with detailed CoT rationales, which is challenging to collect and costly to annotate manually. To tackle this issue, we propose MAVIS, a MAthematical VISual instruction tuning pipeline for MLLMs, featuring an automatic data engine to efficiently create mathematical visual datasets.
We design the data generation process to be entirely independent of human intervention or GPT API usage, while ensuring the diagram-caption correspondence, question-answer correctness, and CoT reasoning quality. With this approach, we curate two datasets, MAVIS-Caption (558K diagram-caption pairs) and MAVIS-Instruct (834K visual math problems with CoT rationales), and propose four progressive stages for training MLLMs from scratch.
First, we utilize MAVIS-Caption to fine-tune a math-specific vision encoder (CLIP-Math) through contrastive learning, tailored for improved diagram visual encoding. Second, we also leverage MAVIS-Caption to align the CLIP-Math with a large language model (LLM) by a projection layer, enhancing vision-language alignment in mathematical domains. Third, we adopt MAVIS-Instruct to perform the instruction tuning for robust problem-solving skills, and term the resulting model as MAVIS-7B. Fourth, we apply Direct Preference Optimization (DPO) to enhance the CoT capabilities of our model, further refining its step-wise reasoning performance.
On various mathematical benchmarks, our MAVIS-7B achieves leading results among open-source MLLMs, e.g., surpassing other 7B models by +9.3% and the second-best LLaVA-NeXT (110B) by +6.9%, demonstrating the effectiveness of our method.
  authors: ['Renrui Zhang', 'Xinyu Wei', 'Dongzhi Jiang']... (11 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199079970
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: MAVIS__Mathematical_Visual_Instruction_Tuning_with_an_Automatic_Data_Engine.pdf
  sha_abstract: 99aa00182297997a995df4d07a06b462ca20e0eb0d5ec13d53bd965d1dfdb059
  title: MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine
  title_normalized: mavis_mathematical_visual_instruction_tuning_with_an_automatic_data_engine

================================================================================
Document #15 (ID: XOilD5oBclM7MZc3PY8B)
================================================================================
  abstract: Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as $9.11 > 9.9$). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear.
Through the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work provides a more detailed and comprehensive understanding of NUPA in LLMs.
  authors: ['Haotong Yang', 'Yi Hu', 'Shijia Kang']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199078650
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Number_Cookbook__Number_Understanding_of_Language_Models_and_How_to_Improve_It.pdf
  sha_abstract: b9e4ca76fd7602c48b4e36f277cc0d07c939879f4fa018983059cad21dd4f05d
  title: Number Cookbook: Number Understanding of Language Models and How to Improve It
  title_normalized: number_cookbook_number_understanding_of_language_models_and_how_to_improve_it

================================================================================
Document #16 (ID: YeilD5oBclM7MZc3Vo8V)
================================================================================
  abstract: Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena.
  authors: ['Ashok Vardhan Makkuva', 'Marco Bondaschi', 'Adway Girish']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199085040
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Attention_with_Markov__A_Curious_Case_of_Single-layer_Transformers.pdf
  sha_abstract: 2357b4d197c3643af13069e590f421b7f5ad04ba8f25a9154360ae40f7c955b0
  title: Attention with Markov: A Curious Case of Single-layer Transformers
  title_normalized: attention_with_markov_a_curious_case_of_singlelayer_transformers

================================================================================
Document #17 (ID: XuilD5oBclM7MZc3R49B)
================================================================================
  abstract: The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters.
The effectiveness of MoE models depends primarily on their routing mechanisms, with the widely adopted Top-K routing scheme used for activating experts.
However, the Top-K scheme has notable limitations,
including unnecessary activations and underutilization of experts.
In this work, 
rather than modifying the routing mechanism as done in previous studies,
we propose the Ternary Choice MoE (TC-MoE),
a novel approach that expands the expert space by applying the ternary set {-1, 0, 1} to each expert.
This expansion allows more efficient and effective expert activations without incurring significant computational costs.
Additionally, 
given the unique characteristics of the expanded expert space,
we introduce a new load balance loss and reward loss to ensure workload balance and achieve a flexible trade-off between effectiveness and efficiency.
Extensive experiments demonstrate that TC-MoE achieves an average improvement of over 1.1% compared with traditional approaches,
while reducing the average number of activated experts by up to 9%.
These results confirm that TC-MoE effectively addresses the inefficiencies of conventional routing schemes,
offering a more efficient and scalable solution for MoE-based large language models.
Code and models are available at https://github.com/stiger1000/TC-MoE.
  authors: ['Shen Yan', 'Xingyan Bin', 'Sijun Zhang']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199081270
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: TC-MoE__Augmenting_Mixture_of_Experts_with_Ternary_Expert_Choice.pdf
  sha_abstract: d32dbbcc50ddf04339c857e4e8da7fd11a98f664f1005d0ccf3cc26e450e25bf
  title: TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice
  title_normalized: tcmoe_augmenting_mixture_of_experts_with_ternary_expert_choice

================================================================================
Document #18 (ID: YuilD5oBclM7MZc3Wo8P)
================================================================================
  abstract: Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations which lose their relevance as the iterate moves along the loss landscape. This work questions the use of a single EMA to accumulate past gradients and empirically demonstrates how this choice can be sub-optimal: a single EMA cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients. Building on this observation, we propose AdEMAMix, a simple modification of the Adam optimizer with a mixture of two EMAs to better take advantage of past gradients. Our experiments on language modeling and image classification show---quite surprisingly---that gradients can stay relevant for tens of thousands of steps. They help to converge faster, and often to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B tokens performs comparably to an AdamW model trained on $197$B tokens ($+95\%$). Moreover, our method significantly slows-down model forgetting during training. Our work motivates further exploration of different types of functions to leverage past gradients, beyond EMAs.
  authors: ['Matteo Pagliardini', 'Pierre Ablin', 'David Grangier']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199086090
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: The_AdEMAMix_Optimizer__Better__Faster__Older.pdf
  sha_abstract: 721eabf7ce68db7817d060b77be44e0509751c9f60cee971170ebcba5bcda532
  title: The AdEMAMix Optimizer: Better, Faster, Older
  title_normalized: the_ademamix_optimizer_better_faster_older

================================================================================
Document #19 (ID: X-ilD5oBclM7MZc3S491)
================================================================================
  abstract: Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing *Leverage SHAP*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just $O(n\log n)$ model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing *leverage score sampling*, a powerful regression tool. Beyond theoretical guarantees, we find that Leverage SHAP achieves an approximately 50% reduction in error compared to the highly optimized implementation of Kernel SHAP in the widely used SHAP library [Lundberg & Lee, 2017].
  authors: ['Christopher Musco', 'R. Teal Witter']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199082350
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Provably_Accurate_Shapley_Value_Estimation_via_Leverage_Score_Sampling.pdf
  sha_abstract: 9e69e05be24c6d20901f02235207284766d329e802d9836918f501fadd3ebc8a
  title: Provably Accurate Shapley Value Estimation via Leverage Score Sampling
  title_normalized: provably_accurate_shapley_value_estimation_via_leverage_score_sampling

================================================================================
Document #20 (ID: W-ilD5oBclM7MZc3N4-c)
================================================================================
  abstract: Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamics of a one-hidden-layer linear network, with one-dimensional data, across any finite width, uniquely exhibiting both kernel and feature learning phases. This study marks a technical advancement by enabling the analysis of the training trajectory from any initialization and a detailed phase diagram under varying common hyperparameters such as width, layer-wise learning rates, and scales of output and initialization. We identify three novel prototype mechanisms specific to the feature learning regime: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling, which contrast starkly with the dynamics observed in the kernel regime. Our theoretical findings are substantiated with empirical evidence showing that these mechanisms also manifest in deep nonlinear networks handling real-world tasks, enhancing our understanding of neural network training dynamics and guiding the design of more effective learning strategies.
  authors: ['Yizhou Xu', 'Liu Ziyin']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199077270
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Three_Mechanisms_of_Feature_Learning_in_a_Linear_Network.pdf
  sha_abstract: 99beab18f7836b2ca1f5fa6582ca15725abd17e754b71086917059351d14b44d
  title: Three Mechanisms of Feature Learning in a Linear Network
  title_normalized: three_mechanisms_of_feature_learning_in_a_linear_network

================================================================================
Document #21 (ID: WuilD5oBclM7MZc3M49Q)
================================================================================
  abstract: In the field of equivariant networks, achieving affine equivariance, particularly for general group representations, has long been a challenge.
In this paper, we propose the steerable EquivarLayer, a generalization of InvarLayer (Li et al., 2024), by building on the concept of equivariants beyond invariants.
The steerable EquivarLayer supports affine equivariance with arbitrary input and output representations, marking the first model to incorporate steerability into networks for the affine group.
To integrate it with canonicalization, a promising approach for making pre-trained models equivariant, we introduce a novel Det-Pooling module, expanding the applicability of EquivarLayer and the range of groups suitable for canonicalization.
We conduct experiments on image classification tasks involving group transformations to validate the steerable EquivarLayer in the role of a canonicalization function, demonstrating its effectiveness over data augmentation.
  authors: ['Yikang Li', 'Yeqing Qiu', 'Yuxuan Chen']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199076170
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Affine_Steerable_Equivariant_Layer_for_Canonicalization_of_Neural_Networks.pdf
  sha_abstract: 15d91b02b33bcd5c155838d6780930a3599d966f38a973ea5011e4d83997f6bf
  title: Affine Steerable Equivariant Layer for Canonicalization of Neural Networks
  title_normalized: affine_steerable_equivariant_layer_for_canonicalization_of_neural_networks

================================================================================
Document #22 (ID: YOilD5oBclM7MZc3UY8A)
================================================================================
  abstract: When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a ``collapse." Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We first prove two concrete mechanisms through which symmetries lead to reduced capacities and ignored features during training and inference. We then propose a simple and theoretically justified algorithm, \textit{syre}, to remove almost all symmetry-induced low-capacity states in neural networks. When this type of entrapment is especially a concern, removing symmetries with the proposed method is shown to correlate well with improved optimization or performance. A remarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge of the symmetry.
  authors: ['Liu Ziyin', 'Yizhou Xu', 'Isaac L. Chuang']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199083770
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Remove_Symmetries_to_Control_Model_Expressivity_and_Improve_Optimization.pdf
  sha_abstract: e02bb0c60f183294ca6248d9fa69673eebaa4d73f2314c74c829747d4bf9d07c
  title: Remove Symmetries to Control Model Expressivity and Improve Optimization
  title_normalized: remove_symmetries_to_control_model_expressivity_and_improve_optimization

================================================================================
Document #23 (ID: buioD5oBclM7MZc3dI9X)
================================================================================
  abstract: Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning methods. Our analysis reveals that safety degradation occurs during VL adaptation, even when the training data is safe. While safety tuning techniques like supervised fine-tuning with safety datasets or reinforcement learning from human feedback mitigate some risks, they still lead to safety degradation and a reduction in helpfulness due to over-rejection issues. Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels. Additionally, our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal. To address this, we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness. These insights help guide the development of more reliable and secure LVLMs for real-world applications.
  authors: ['Seongyun Lee', 'Geewook Kim', 'Jiyeon Kim']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199289410
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: How_Does_Vision-Language_Adaptation_Impact_the_Safety_of_Vision_Language_Models_.pdf
  sha_abstract: b38cf3e78d39c9ddbed4a9d7f0b96028f03f425c0cbd46165357b1f203366a9d
  title: How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?
  title_normalized: how_does_visionlanguage_adaptation_impact_the_safety_of_vision_language_models

================================================================================
Document #24 (ID: beilD5oBclM7MZc3lY_d)
================================================================================
  abstract: Generative models lack rigorous statistical guarantees with respect to their predictions. In this work, we propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a sequential conformal prediction method producing prediction sets that satisfy a rigorous statistical guarantee called conformal admissibility control. This guarantee means that the prediction sets contain at least one admissible (or valid) example, with high probability. To this end, our method first samples an initial set of i.i.d. examples from a black box generative model. Then, this set is iteratively pruned via so-called greedy filters. As a consequence of the iterative generation procedure, admissibility of the final prediction set factorizes as a Markov chain, where each factor can be controlled separately, using conformal prediction. In comparison to prior work, our method demonstrates a large reduction in the number of admissibility evaluations during calibration. This is crucial e.g. in safety-critical applications, where these evaluations must be conducted manually by domain experts and are therefore costly and time consuming. We highlight the advantages of our method in terms of admissibility evaluations and cardinality of the prediction set through experiments in natural language generation and molecular graph extension tasks.
  authors: ['Klaus-Rudolf Kladny', 'Bernhard SchÃ¶lkopf', 'Michael Muehlebach']
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199101397
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Conformal_Generative_Modeling_with_Improved_Sample_Efficiency_through_Sequential_Greedy_Filtering.pdf
  sha_abstract: 84f6528c37fb81b3321f92c2bbe029ff9ec1497d1eadd737a7215a5b52b87662
  title: Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering
  title_normalized: conformal_generative_modeling_with_improved_sample_efficiency_through_sequential_greedy_filtering

================================================================================
Document #25 (ID: bOilD5oBclM7MZc3kY8D)
================================================================================
  abstract: Accurate prediction of thermodynamic properties is essential in drug discovery and materials science. Molecular dynamics (MD) simulations provide a principled approach to this task, yet they typically rely on prohibitively long sequential simulations. Implicit Transfer Operator (ITO) Learning offers a promising approach to address this limitation by enabling stable simulation with time steps orders of magnitude larger than MD. However, to train ITOs, we need extensive, unbiased MD data, limiting the scope of this framework. Here, we introduce Boltzmann Priors for ITO (BoPITO) to enhance ITO learning in two ways. First, BoPITO enables more efficient data generation, and second, it embeds inductive biases for long-term dynamical behavior, simultaneously improving sample efficiency by one order of magnitude and guaranteeing asymptotically unbiased equilibrium statistics. Furthermore, we showcase the use of BoPITO in a new tunable sampling protocol interpolating between ITOs trained on off-equilibrium simulations and an equilibrium model by incorporating unbiased correlation functions. Code is available at https://github.com/olsson-group/bopito.
  authors: ['Juan Viguera Diez', 'Mathias Jacob Schreiner', 'Ola Engkvist']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1761199100157
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Boltzmann_priors_for_Implicit_Transfer_Operators.pdf
  sha_abstract: 90396f6e0f64a54399caf017a2ae9275b223e16c1741405a6b430255cad1f04f
  title: Boltzmann priors for Implicit Transfer Operators
  title_normalized: boltzmann_priors_for_implicit_transfer_operators

================================================================================
Document #26 (ID: cOjHD5oBclM7MZc3Eo__)
================================================================================
  abstract: Research paper from unknown conference
  authors: ['Unknown']
  date: 2025-10-22
  decision: accept
  ingested_at: 1761201296114
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: AI_Metropolis__Scaling_Large_Language_Model-based_Multi-Agent_Simulation_with_Out-of-order_Execution.pdf
  sha_abstract: 6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c
  title: AI Metropolis  Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution
  title_normalized: ai_metropolis__scaling_large_language_modelbased_multiagent_simulation_with_outoforder_execution

================================================================================
Document #27 (ID: b-jHD5oBclM7MZc3Eo9y)
================================================================================
  abstract: Research paper from unknown conference
  authors: ['Unknown']
  date: 2025-10-22
  decision: accept
  ingested_at: 1761201295873
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: AIOpsLab__A_Holistic_Framework_to_Evaluate_AI_Agents_for_Enabling_Autonomous_Clouds.pdf
  sha_abstract: 6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c
  title: AIOpsLab  A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds
  title_normalized: aiopslab__a_holistic_framework_to_evaluate_ai_agents_for_enabling_autonomous_clouds

================================================================================
Document #28 (ID: cejHD5oBclM7MZc3FY9Z)
================================================================================
  abstract: Research paper from unknown conference
  authors: ['Unknown']
  date: 2025-10-22
  decision: accept
  ingested_at: 1761201296703
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: APOLLO__SGD-like_Memory__AdamW-level_Performance.pdf
  sha_abstract: 6ea31c3f92d0d87847e1ba57b94a0645ffa81283bf71eb15f722866c3263301c
  title: APOLLO  SGD-like Memory  AdamW-level Performance
  title_normalized: apollo__sgdlike_memory__adamwlevel_performance

================================================================================
Document #29 (ID: 7ej63JkBP8oloYi_9iKE)
================================================================================
  abstract: Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.30%. Furthermore, combining
self-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.
  authors: ['Vithursan Thangarasa', 'Ganesh Venkatesh', 'Mike Lasby']... (5 items)
  code_generated: True
  code_generated_at: 2025-10-23T06:50:20.635670
  code_metadata_s3_key: 7ej63JkBP8oloYi_9iKE/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: 7ej63JkBP8oloYi_9iKE/code.py
  date: 2025-05-13
  decision: accept
  ingested_at: 1760349058668
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Self-Data_Distillation_for_Recovering_Quality_in_Pruned_Large_Language_Models.pdf
  sha_abstract: 2370ca0c910e3bafb58ba2dd5e1a760fda1ea07ca040951f66aabd1547975ced
  test_results: {'error_message': 'Unknown error', 'tested_at': '2025-10-23T06:51:24.595159', 'stdout_s3_key': '7ej63JkBP8oloYi_9iKE/outputs/stdout.log', 'tested': True, 'success': False, 'error_type': 'unknown', 'stdout_s3_bucket': 'papers-test-outputs', 'return_code': 1, 'artifacts_s3_prefix': '7ej63JkBP8oloYi_9iKE/outputs/', 'timeout': False, 'execution_time': 0.019766, 'stderr_s3_key': '7ej63JkBP8oloYi_9iKE/outputs/stderr.log'}
  title: Self-Data Distillation for Recovering Quality in Pruned Large Language Models
  title_normalized: selfdata_distillation_for_recovering_quality_in_pruned_large_language_models

================================================================================
Document #30 (ID: 8Oj73JkBP8oloYi_AiJV)
================================================================================
  abstract: Language models for scientific tasks are trained on text from scientific publications---most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive MLâ€‘driven systems (for complex or degraded ones). The choice of the ``best'' parser for a particular document depends on 1) its computational cost and 2) the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and (aligned) predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by 17$\times$ while still achieving comparable accuracy (actually, 0.2\% better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at \url{https://github.com/7shoe/AdaParse/}.
  authors: ['Carlo Siebenschuh', 'Kyle Hippe', 'Ozan Gokdemir']... (13 items)
  code_generated: True
  code_generated_at: 2025-10-23T06:50:54.967804
  code_metadata_s3_key: 8Oj73JkBP8oloYi_AiJV/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: 8Oj73JkBP8oloYi_AiJV/code.py
  date: 2025-05-13
  decision: accept
  ingested_at: 1760349061643
  novelty: yes
  reason: testing purposes
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: AdaParse__An_Adaptive_Parallel_PDF_Parsing_and_Resource_Scaling_Engine.pdf
  sha_abstract: ed9925d65ae5198fd29febb4c3948661944222120467d53e05d377de17b18f0d
  test_results: {'error_message': 'Unknown error', 'tested_at': '2025-10-23T06:50:56.119098', 'stdout_s3_key': '8Oj73JkBP8oloYi_AiJV/outputs/stdout.log', 'tested': True, 'success': False, 'error_type': 'unknown', 'stdout_s3_bucket': 'papers-test-outputs', 'return_code': 1, 'artifacts_s3_prefix': '8Oj73JkBP8oloYi_AiJV/outputs/', 'timeout': False, 'execution_time': 0.032474, 'stderr_s3_key': '8Oj73JkBP8oloYi_AiJV/outputs/stderr.log'}
  title: AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine
  title_normalized: adaparse_an_adaptive_parallel_pdf_parsing_and_resource_scaling_engine
