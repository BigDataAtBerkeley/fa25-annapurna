
✅ Cluster Health:
{
  "cluster_name": "478852001205:research-papers",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 1,
  "number_of_data_nodes": 1,
  "discovered_master": true,
  "discovered_cluster_manager": true,
  "active_primary_shards": 15,
  "active_shards": 15,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 2,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 88.23529411764706
}

Indices:
- .plugins-ml-config (1 docs, status=green)
- .opensearch-observability (0 docs, status=green)
- research-papers-v2 (83 docs, status=yellow)
- .kibana_1 (1 docs, status=green)
- .opendistro_security (10 docs, status=green)
- research-papers (2 docs, status=yellow)

Documents from 'research-papers-v2':

================================================================================
Document #1 (ID: XOhHWJoBclM7MZc3IZJh)
================================================================================
  abstract: Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT (VPT-perturb) designed to limit the effectiveness of "shortcut" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines.
  abstract_embedding: [0.65234375, 0.341796875, 0.259765625]... (1536 items)
  authors: ['Drew Linsley', 'Peisen Zhou', 'Alekh Karkada Ashok']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417647958
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: The_3D-PC__a_benchmark_for_visual_perspective_taking_in_humans_and_machines.pdf
  sha_abstract: 9585d404d528dfa529c1117d0589fd23f599b5c8abee4464b3f10bd7ef74450b
  title: The 3D-PC: a benchmark for visual perspective taking in humans and machines
  title_normalized: the_3dpc_a_benchmark_for_visual_perspective_taking_in_humans_and_machines

================================================================================
Document #2 (ID: YOhHWJoBclM7MZc3O5Kz)
================================================================================
  abstract: Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose $\textbf{Di}$rectional $\textbf{Gra}$dient $\textbf{P}$rojection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.
  abstract_embedding: [0.34375, 0.5390625, 0.1337890625]... (1536 items)
  authors: ['Chengyue Huang', 'Junjiao Tian', 'Brisa Maneechotesuwan']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417654698
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Directional_Gradient_Projection_for_Robust_Fine-Tuning_of_Foundation_Models.pdf
  sha_abstract: ba5d0249766995fe9964cf3837984acec094b1a550edbb523f9d42f97e68d001
  title: Directional Gradient Projection for Robust Fine-Tuning of Foundation Models
  title_normalized: directional_gradient_projection_for_robust_finetuning_of_foundation_models

================================================================================
Document #3 (ID: WuhHWJoBclM7MZc3FZLn)
================================================================================
  abstract: Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT).
However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents.
In this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations.
DelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components.
Experimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average.
DelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method.
Furthermore, DelTA improves pronoun and context-dependent translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks.
The code and data of our approach are released at https://github.com/YutongWang1216/DocMTAgent.
  abstract_embedding: [0.89453125, 0.0247802734375, -0.04833984375]... (1536 items)
  authors: ['Yutong Wang', 'Jiali Zeng', 'Xuebo Liu']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417645001
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: DelTA__An_Online_Document-Level_Translation_Agent_Based_on_Multi-Level_Memory.pdf
  sha_abstract: f5aee9f167eb47888090f3c8ed664c0fea08c93410f1296da9a2eaf3e0521126
  title: DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory
  title_normalized: delta_an_online_documentlevel_translation_agent_based_on_multilevel_memory

================================================================================
Document #4 (ID: YehHWJoBclM7MZc3QJI9)
================================================================================
  abstract: A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.
  abstract_embedding: [0.0166015625, 0.392578125, 0.056640625]... (1536 items)
  authors: ['Wilson Wu', 'Louis Jaburi', 'jacob drori']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417655858
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Towards_a_Unified_and_Verified_Understanding_of_Group-Operation_Networks.pdf
  sha_abstract: d475afb9f3dd8840f600b5793dbe4d0b8a4fcf5d77c57facc38018254f2708d0
  title: Towards a Unified and Verified Understanding of Group-Operation Networks
  title_normalized: towards_a_unified_and_verified_understanding_of_groupoperation_networks

================================================================================
Document #5 (ID: W-hHWJoBclM7MZc3GpLV)
================================================================================
  abstract: In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While previous approaches leveraging discrete latent spaces, such as DreamerV3, have demonstrated strong performance in discrete action settings and visual control tasks, their comparative performance in state-based continuous control remains underexplored. In contrast, methods with continuous latent spaces, such as TD-MPC2, have shown notable success in state-based continuous control benchmarks. In this paper, we demonstrate that modeling discrete latent states has benefits over continuous latent states and that discrete codebook encodings are more effective representations for continuous control, compared to alternative encodings, such as one-hot and label-based encodings. Based on these insights, we introduce DCWM: Discrete Codebook World Model, a self-supervised world model with a discrete and stochastic latent space, where latent states are codes from a codebook. We combine DCWM with decision-time planning to get our model-based RL algorithm, named DC-MPC: Discrete Codebook Model Predictive Control, which performs competitively against recent state-of-the-art algorithms, including TD-MPC2 and DreamerV3, on continuous control benchmarks.
  abstract_embedding: [0.63671875, 0.341796875, 0.4453125]... (1536 items)
  authors: ['Aidan Scannell', 'Mohammadreza Nakhaeinezhadfard', 'Kalle Kujanpää']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417646279
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Discrete_Codebook_World_Models_for_Continuous_Control.pdf
  sha_abstract: 4ecb5992b96ab009e0644e81a836cf294143da8a5d51aac50000e3381dd5587d
  title: Discrete Codebook World Models for Continuous Control
  title_normalized: discrete_codebook_world_models_for_continuous_control

================================================================================
Document #6 (ID: X-hHWJoBclM7MZc3M5Jq)
================================================================================
  abstract: Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\texttt{miniCTX}$, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. $\texttt{miniCTX}$ contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for $\texttt{miniCTX}$, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as $\texttt{miniF2F}$. Alongside $\texttt{miniCTX}$, we offer $\texttt{ntp-toolkit}$ for automatically extracting and annotating theorem proving data, making it easy to add new projects into $\texttt{miniCTX}$ to ensure that contexts are not seen during training. $\texttt{miniCTX}$ offers a challenging and realistic evaluation of neural theorem provers.
  abstract_embedding: [0.66796875, 0.5625, 0.1298828125]... (1536 items)
  authors: ['Jiewen Hu', 'Thomas Zhu', 'Sean Welleck']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417652519
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: miniCTX__Neural_Theorem_Proving_with__Long-_Contexts.pdf
  sha_abstract: 2ecffffc4e68527d96a2c53287f70d7f192e37ae36cefe300b2529ef89d3f3ca
  title: miniCTX: Neural Theorem Proving with (Long-)Contexts
  title_normalized: minictx_neural_theorem_proving_with_longcontexts

================================================================================
Document #7 (ID: WehHWJoBclM7MZc3EZI0)
================================================================================
  abstract: Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors have different influences on the likelihood of memorization depending on the taxonomic category.
  abstract_embedding: [0.30859375, 0.30859375, -0.263671875]... (1536 items)
  authors: ['USVSN Sai Prashanth', 'Alvin Deng', "Kyle O'Brien"]... (12 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417643788
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Recite__Reconstruct__Recollect__Memorization_in_LMs_as_a_Multifaceted_Phenomenon.pdf
  sha_abstract: f478eb2ea9eaebe040e47758bd9f448039cb741aa313358df9d9c28a00a004be
  title: Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon
  title_normalized: recite_reconstruct_recollect_memorization_in_lms_as_a_multifaceted_phenomenon

================================================================================
Document #8 (ID: XuhHWJoBclM7MZc3LZIP)
================================================================================
  abstract: Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while  speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.
  abstract_embedding: [0.259765625, 0.275390625, 0.11962890625]... (1536 items)
  authors: ['Harikrishna Narasimhan', 'Wittawat Jitkrittum', 'Ankit Singh Rawat']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417650950
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Faster_Cascades_via_Speculative_Decoding.pdf
  sha_abstract: c1740e4314d3ff87a4c6b53014889f0eada36cfb0e6ecf2e8582608fb1d7f6db
  title: Faster Cascades via Speculative Decoding
  title_normalized: faster_cascades_via_speculative_decoding

================================================================================
Document #9 (ID: U-hGWJoBclM7MZc37ZKC)
================================================================================
  abstract: We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an "autopilot" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, model, and interface are publicly accessible at https://github.com/aim-uofa/Framer.
  abstract_embedding: [0.31640625, 0.375, 0.130859375]... (1536 items)
  authors: ['Wen Wang', 'Qiuyu Wang', 'Kecheng Zheng']... (9 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:47.566528
  code_metadata_s3_key: U-hGWJoBclM7MZc37ZKC/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: U-hGWJoBclM7MZc37ZKC/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417634648
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Framer__Interactive_Frame_Interpolation.pdf
  sha_abstract: 79f7cda9d3e369c5f40cc1f1859cffb263308a12d1f02167e673f07e665edb37
  title: Framer: Interactive Frame Interpolation
  title_normalized: framer_interactive_frame_interpolation

================================================================================
Document #10 (ID: UuhGWJoBclM7MZc355LM)
================================================================================
  abstract: Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more --- which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through [https://sorry-bench.github.io](https://sorry-bench.github.io).
  abstract_embedding: [0.6171875, 0.388671875, -0.03564453125]... (1536 items)
  authors: ['Tinghao Xie', 'Xiangyu Qi', 'Yi Zeng']... (16 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:47.891306
  code_metadata_s3_key: UuhGWJoBclM7MZc355LM/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: UuhGWJoBclM7MZc355LM/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417633218
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: SORRY-Bench__Systematically_Evaluating_Large_Language_Model_Safety_Refusal.pdf
  sha_abstract: 1132bdc951c96fa4b830c6af27985f86b76112fa1a30d90fe1911cc4c157e5ad
  title: SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal
  title_normalized: sorrybench_systematically_evaluating_large_language_model_safety_refusal

================================================================================
Document #11 (ID: V-hHWJoBclM7MZc3ApJY)
================================================================================
  abstract: Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. 
While effective in fine-tuning, KD during pre-training faces efficiency, flexibility, and effectiveness issues. 
Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data.
In this work, we propose **MiniPLM**, a KD framework for pre-training LMs by refining the training data distribution with the teacher LM's knowledge.
For efficiency, MiniPLM performs offline teacher inference, allowing KD for multiple student LMs without adding training costs.
For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families.
For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the training data difficulty and diversity, helping student LMs acquire versatile and sophisticated knowledge.
Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 common downstream tasks, improves language modeling capabilities, and reduces pre-training computation. 
The benefit of MiniPLM extends to larger training scales, evidenced by the scaling curve extrapolation.
Further analysis reveals that MiniPLM supports KD across model families and enhances the pre-training data utilization. Our code, data, and models can be found at https://github.com/thu-coai/MiniPLM.
  abstract_embedding: [0.4296875, 0.44921875, 0.392578125]... (1536 items)
  authors: ['Yuxian Gu', 'Hao Zhou', 'Fandong Meng']... (5 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:50.943990
  code_metadata_s3_key: V-hHWJoBclM7MZc3ApJY/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: V-hHWJoBclM7MZc3ApJY/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417639919
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: MiniPLM__Knowledge_Distillation_for_Pre-training_Language_Models.pdf
  sha_abstract: 23c4a07f43020622c025e4aecec48dd5be474aeb05ee40c87ab7d2d8cc02dfee
  title: MiniPLM: Knowledge Distillation for Pre-training Language Models
  title_normalized: miniplm_knowledge_distillation_for_pretraining_language_models

================================================================================
Document #12 (ID: behHWJoBclM7MZc3i5L_)
================================================================================
  abstract: Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process.  Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.
  abstract_embedding: [0.97265625, 0.146484375, 0.4140625]... (1536 items)
  authors: ['Claas A Voelcker', 'Marcel Hussing', 'Eric Eaton']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417675225
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: MAD-TD__Model-Augmented_Data_stabilizes_High_Update_Ratio_RL.pdf
  sha_abstract: de33890c223c008f9657b46ae8a0f14091ca069c4b024347c2f3b8034ec28770
  title: MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL
  title_normalized: madtd_modelaugmented_data_stabilizes_high_update_ratio_rl

================================================================================
Document #13 (ID: auhHWJoBclM7MZc3d5J6)
================================================================================
  abstract: We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360◦ panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization–performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space–generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling–gradually updating the target space data through gradient descent–results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360◦ panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods.
  abstract_embedding: [0.1611328125, 0.6171875, 0.33984375]... (1536 items)
  authors: ['Kyeongmin Yeo', 'Jaihoon Kim', 'Minhyuk Sung']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417669998
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: StochSync__Stochastic_Diffusion_Synchronization_for_Image_Generation_in_Arbitrary_Spaces.pdf
  sha_abstract: f7c538d27a0658b43507adc170d31e1f3acf1466e49103e6c84c570b4b9faf96
  title: StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces
  title_normalized: stochsync_stochastic_diffusion_synchronization_for_image_generation_in_arbitrary_spaces

================================================================================
Document #14 (ID: b-hHWJoBclM7MZc3l5Jv)
================================================================================
  abstract: The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.
  abstract_embedding: [0.40625, 0.2041015625, 0.474609375]... (1536 items)
  authors: ['Yiding Jiang', 'Allan Zhou', 'Zhili Feng']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417678182
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Adaptive_Data_Optimization__Dynamic_Sample_Selection_with_Scaling_Laws.pdf
  sha_abstract: c4902c4850a377de50bb4fd6ba922c5296f41298277455585e6aafe05e879c19
  title: Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws
  title_normalized: adaptive_data_optimization_dynamic_sample_selection_with_scaling_laws

================================================================================
Document #15 (ID: cOhHWJoBclM7MZc3ppJ0)
================================================================================
  abstract: Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way.
  abstract_embedding: [0.0849609375, 0.2490234375, 0.1728515625]... (1536 items)
  authors: ['Marc Anton Finzi', 'Sanyam Kapoor', 'Diego Granziol']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417681992
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Compute-Optimal_LLMs_Provably_Generalize_Better_with_Scale.pdf
  sha_abstract: b1f48346893cc97894134708442d6e2b754391da8abe3e023129a45c250fd82b
  title: Compute-Optimal LLMs Provably Generalize Better with Scale
  title_normalized: computeoptimal_llms_provably_generalize_better_with_scale

================================================================================
Document #16 (ID: a-hHWJoBclM7MZc3fJLv)
================================================================================
  abstract: Recent work has shown that diffusion models memorize and reproduce training data examples. At the same time, large copyright lawsuits and legislation such as GDPR have highlighted the need for erasing datapoints from diffusion models. However, retraining from scratch is often too expensive. This motivates the setting of data unlearning, i.e., the study of efficient techniques for unlearning specific datapoints from the training set. Existing concept unlearning techniques require an anchor prompt/class/distribution to guide unlearning, which is not available in the data unlearning setting. General-purpose machine unlearning techniques were found to be either unstable or failed to unlearn data. We therefore propose a family of new loss functions called Subtracted Importance Sampled Scores (SISS) that utilize importance sampling and are the first method to unlearn data with theoretical guarantees. SISS is constructed as a weighted combination between simpler objectives that are responsible for preserving model quality and unlearning the targeted datapoints. When evaluated on CelebA-HQ and MNIST, SISS achieved Pareto optimality along the quality and unlearning strength dimensions. On Stable Diffusion, SISS successfully mitigated memorization on nearly 90% of the prompts we tested. We release our code online.
  abstract_embedding: [0.5625, 0.451171875, 0.33984375]... (1536 items)
  authors: ['Silas Alberti', 'Kenan Hasanaliyev', 'Manav Shah']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417671359
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Data_Unlearning_in_Diffusion_Models.pdf
  sha_abstract: f9809c3aa8412392e95ce3b1d7f9c31a490e00be541cbe489bbfc9e7a0c44abe
  title: Data Unlearning in Diffusion Models
  title_normalized: data_unlearning_in_diffusion_models

================================================================================
Document #17 (ID: buhHWJoBclM7MZc3kpK5)
================================================================================
  abstract: We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of a pairwise probability distribution over the space of sequences, capturing the amino acid covariances observed in Multiple Sequence Alignments (MSA) of homologous proteins. This allows for the efficient generation of highly diverse protein sequences while preserving structural and functional integrity.
We demonstrate that this increased diversity in sampled sequences translates into greater variability in biochemical properties, highlighting the exciting potential of our method for applications such as protein design. The orders of magnitude improvement in sampling speed compared to existing methods unlocks new possibilities for high-throughput in virtual screening.
  abstract_embedding: [0.0771484375, 0.8359375, 0.115234375]... (1536 items)
  authors: ['luca alessandro silva', 'Barthelemy Meynard-Piganeau', 'Carlo Lucibello']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417676978
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Fast_Uncovering_of_Protein_Sequence_Diversity_from_Structure.pdf
  sha_abstract: 8eb22f03bc71b0f57c61fa23d160455d7c6749d88e318cd896a068819c33f4b2
  title: Fast Uncovering of Protein Sequence Diversity from Structure
  title_normalized: fast_uncovering_of_protein_sequence_diversity_from_structure

================================================================================
Document #18 (ID: fOhHWJoBclM7MZc30pIA)
================================================================================
  abstract: Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 44 potentially unsafe topics, and 440 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more --- which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive safety refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient manner. Benchmark demo, data, code, and models are available through [https://sorry-bench.github.io](https://sorry-bench.github.io).
  abstract_embedding: [0.6171875, 0.388671875, -0.03564453125]... (1536 items)
  authors: ['Tinghao Xie', 'Xiangyu Qi', 'Yi Zeng']... (16 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417693136
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: SORRY-Bench__Systematically_Evaluating_Large_Language_Model_Safety_Refusal.pdf
  sha_abstract: 1132bdc951c96fa4b830c6af27985f86b76112fa1a30d90fe1911cc4c157e5ad
  title: SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal
  title_normalized: sorrybench_systematically_evaluating_large_language_model_safety_refusal

================================================================================
Document #19 (ID: gehHWJoBclM7MZc34pIq)
================================================================================
  abstract: As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms embedded within their structures to explain their behavior. This work systematically examines a fundamental question: for a fixed behavior to explain, and under the criteria that MI sets for itself, are we guaranteed a unique explanation? Drawing an analogy with the concept of identifiability in statistics, which ensures the uniqueness of parameters inferred from data under specific modeling assumptions, we speak about the identifiability of explanations produced by MI.

We identify two broad strategies to produce MI explanations: (i) "where-then-what", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) "what-then-where", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. 

We systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons small enough to allow a complete enumeration of candidate explanations. Our experiments reveal overwhelming evidence of non-identifiability in all cases: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and a single algorithm can be causally aligned with different subspaces of the network.

We discuss whether the unicity intuition is necessary. One could adopt a pragmatic stance, requiring explanations only to meet predictive and/or manipulability standards. However, if unicity is considered essential, e.g., to provide a sense of understanding, we also discuss less permissive criteria. Finally, we also refer to the inner interpretability framework that demands explanations to be validated by multiple complementary criteria. This work aims to contribute constructively to the ongoing effort to formalize what we expect from explanations in AI.
  abstract_embedding: [0.2734375, 0.2236328125, -0.0400390625]... (1536 items)
  authors: ['Maxime Méloux', 'Silviu Maniu', 'François Portet']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417697314
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Everything__Everywhere__All_at_Once__Is_Mechanistic_Interpretability_Identifiable_.pdf
  sha_abstract: ed9823162141df36378ad1db13a29f50a5126df24f7e0c9a309d4d2787cb911c
  title: Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?
  title_normalized: everything_everywhere_all_at_once_is_mechanistic_interpretability_identifiable

================================================================================
Document #20 (ID: f-hHWJoBclM7MZc325Kt)
================================================================================
  abstract: Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.
  abstract_embedding: [0.69921875, -0.035400390625, 0.337890625]... (1536 items)
  authors: ['Yiheng Xu', 'Dunjie Lu', 'Zhennan Shen']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417695651
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: AgentTrek__Agent_Trajectory_Synthesis_via_Guiding_Replay_with_Web_Tutorials.pdf
  sha_abstract: 56be0f23c7ad987c4a2e633f8d54477e9ccb8f319e4c95ddbd6721f34c5ca9f3
  title: AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials
  title_normalized: agenttrek_agent_trajectory_synthesis_via_guiding_replay_with_web_tutorials

================================================================================
Document #21 (ID: fuhHWJoBclM7MZc32JKS)
================================================================================
  abstract: We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an "autopilot" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, model, and interface are publicly accessible at https://github.com/aim-uofa/Framer.
  abstract_embedding: [0.31640625, 0.375, 0.130859375]... (1536 items)
  authors: ['Wen Wang', 'Qiuyu Wang', 'Kecheng Zheng']... (9 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417694835
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: Framer__Interactive_Frame_Interpolation.pdf
  sha_abstract: 79f7cda9d3e369c5f40cc1f1859cffb263308a12d1f02167e673f07e665edb37
  title: Framer: Interactive Frame Interpolation
  title_normalized: framer_interactive_frame_interpolation

================================================================================
Document #22 (ID: euhHWJoBclM7MZc3zZIR)
================================================================================
  abstract: The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce port-Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.
  abstract_embedding: [0.59765625, 0.318359375, -0.0189208984375]... (1536 items)
  authors: ['Simon Heilig', 'Alessio Gravina', 'Alessandro Trenta']... (5 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417691897
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: Port-Hamiltonian_Architectural_Bias_for_Long-Range_Propagation_in_Deep_Graph_Networks.pdf
  sha_abstract: d9b47761e54c4140622ea15eb058b1db477fc309e9ef0d92b3597e3d7e1f5b0d
  title: Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks
  title_normalized: porthamiltonian_architectural_bias_for_longrange_propagation_in_deep_graph_networks

================================================================================
Document #23 (ID: gOhHWJoBclM7MZc34ZJL)
================================================================================
  abstract: The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\times$, without compromising the model performance compared to token-level training. Source code: \url{https://github.com/shaochenze/PatchTrain}.
  abstract_embedding: [0.291015625, 0.1767578125, 0.162109375]... (1536 items)
  authors: ['Chenze Shao', 'Fandong Meng', 'Jie Zhou']
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417697070
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: Beyond_Next_Token_Prediction__Patch-Level_Training_for_Large_Language_Models.pdf
  sha_abstract: 9708cfcf22e205df0997b86176d4e0ecbd09b2f0c1216a1a4d6e21eb6fdb2ced
  title: Beyond Next Token Prediction: Patch-Level Training for Large Language Models
  title_normalized: beyond_next_token_prediction_patchlevel_training_for_large_language_models

================================================================================
Document #24 (ID: e-hHWJoBclM7MZc3z5KA)
================================================================================
  abstract: Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.
  abstract_embedding: [0.5390625, 0.51953125, 0.11865234375]... (1536 items)
  authors: ['Zekun Wang', 'Mingyang Yi', 'Shuchen Xue']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417692521
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Improved_Diffusion-based_Generative_Model_with_Better_Adversarial_Robustness.pdf
  sha_abstract: ea82003bd7cecc49176a65db9f5b12ca2995cbc9664ce440bf8c5aee0ea62660
  title: Improved Diffusion-based Generative Model with Better Adversarial Robustness
  title_normalized: improved_diffusionbased_generative_model_with_better_adversarial_robustness

================================================================================
Document #25 (ID: fehHWJoBclM7MZc315Iv)
================================================================================
  abstract: Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.
  abstract_embedding: [-0.232421875, 0.578125, 0.1318359375]... (1536 items)
  authors: ['Alexander Cong Li', 'Ananya Kumar', 'Deepak Pathak']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417694498
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Generative_Classifiers_Avoid_Shortcut_Solutions.pdf
  sha_abstract: 03648b7031675eca7d21544ee7e7ad61790aa0104fc114dda073cb5fb81638ad
  title: Generative Classifiers Avoid Shortcut Solutions
  title_normalized: generative_classifiers_avoid_shortcut_solutions

================================================================================
Document #26 (ID: eehHWJoBclM7MZc3ypLy)
================================================================================
  abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.
  abstract_embedding: [0.0186767578125, 0.5078125, 0.17578125]... (1536 items)
  authors: ['Javad Aliakbari', 'Johan Östman', 'Alexandre Graell i Amat']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417691348
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Decoupled_Subgraph_Federated_Learning.pdf
  sha_abstract: e0204095f49e5e036d3c2ef9ffc0589a9eb5e57700f223442f462211d7494a0c
  title: Decoupled Subgraph Federated Learning
  title_normalized: decoupled_subgraph_federated_learning

================================================================================
Document #27 (ID: bOhHWJoBclM7MZc3hZJJ)
================================================================================
  abstract: Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an effective scheme for training CMs that largely improves the efficiency of building such models. Specifically, by expressing CM trajectories via a particular differential equation, we argue that diffusion models can be viewed as a special case of CMs. We can thus fine-tune a consistency model starting from a pretrained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly reduced training times while improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained for hundreds of GPU hours. Owing to this computational efficiency, we investigate the scaling laws of CMs under ECT, showing that they obey the classic power law scaling, hinting at their ability to improve efficiency and performance at larger scales. Our [code](https://github.com/locuslab/ect) is available.
  abstract_embedding: [0.466796875, 0.484375, 0.51171875]... (1536 items)
  authors: ['Zhengyang Geng', 'Ashwini Pokle', 'Weijian Luo']... (5 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:28:29.735035
  code_metadata_s3_key: bOhHWJoBclM7MZc3hZJJ/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: bOhHWJoBclM7MZc3hZJJ/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417673538
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Consistency_Models_Made_Easy.pdf
  sha_abstract: b978c9f23f73106902eda090ff5d0c4fcbb5debeed4bbbaee7fb1f176ae5fdef
  title: Consistency Models Made Easy
  title_normalized: consistency_models_made_easy

================================================================================
Document #28 (ID: aehHWJoBclM7MZc3cJJA)
================================================================================
  abstract: Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations into the token inputs of Large Language Models (LLMs). However, their real-world deployment is often constrained by high latency during inference due to the substantial compute required by the LLM to process the large number of input tokens, predominantly arising from the image. To reduce inference costs, one can either downsize the LLM or reduce the number of input tokens needed to represent the image, the latter of which has been the focus of many recent efforts around token compression. However, it is unclear what the optimal trade-off is given a fixed inference budget. 
We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs is achieved by using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take the first steps toward designing token compression algorithms tailored for high-compression settings, utilizing prompt-based compression of tokens. Our work underscores the performance and efficiency benefits of operating in low visual token regimes and the importance of developing tailored token reduction algorithms for such conditions.
  abstract_embedding: [0.061279296875, 0.263671875, 0.51171875]... (1536 items)
  authors: ['Kevin Li', 'Sachin Goyal', 'João D. Semedo']... (4 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:28:29.948337
  code_metadata_s3_key: aehHWJoBclM7MZc3cJJA/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: aehHWJoBclM7MZc3cJJA/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417668111
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Inference_Optimal_VLMs_Need_Fewer_Visual_Tokens_and_More_Parameters.pdf
  sha_abstract: c88738c13ff825d5d3d77962d25af31795562c13e261e7c4c382f4650d5b6780
  title: Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters
  title_normalized: inference_optimal_vlms_need_fewer_visual_tokens_and_more_parameters

================================================================================
Document #29 (ID: kuhIWJoBclM7MZc3IpIi)
================================================================================
  abstract: Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. 
However, the substantial communication costs associated with FL significantly challenge its efficiency. 
Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. 
Despite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations.
This paper proposes a novel dimension-free communication algorithm - DeComFL, which leverages the zeroth-order optimization techniques and reduces the communication cost from $\mathcal{O}(d)$ to $\mathcal{O}(1)$ by transmitting only a constant number of scalar values between clients and the server in each round, regardless of the dimension $d$ of the model parameters.
Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions. With additional low effective rank assumption, we can further show that the convergence rate is independent of the model dimension $d$ as well.
Empirical evaluations, encompassing both classic deep learning training and large language model fine-tuning, demonstrate significant reductions in communication overhead. 
Notably, DeComFL achieves this by transmitting only around 1MB of data in total between the server and a client to fine-tune a model with billions of parameters. 
The code is available at https://github.com/ZidongLiu/DeComFL.
  abstract_embedding: [0.283203125, 0.43359375, 0.125]... (1536 items)
  authors: ['Zhe Li', 'Bicheng Ying', 'Zidong Liu']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417713659
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Achieving_Dimension-Free_Communication_in_Federated_Learning_via_Zeroth-Order_Optimization.pdf
  sha_abstract: 5ecd56638a7d57fc993d0d2bce6f664dd7917b67b630c7f120846ae99fcc9cd1
  title: Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization
  title_normalized: achieving_dimensionfree_communication_in_federated_learning_via_zerothorder_optimization

================================================================================
Document #30 (ID: j-hIWJoBclM7MZc3EpIN)
================================================================================
  abstract: Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and true gradient) is considered to have only a finite $\mathfrak{p}$-th moment where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different algorithms and proved them to converge at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate for smooth objectives after $T$ iterations. Notably, all these new-designed algorithms are based on the same technique – gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, we prove that Batched NSGDM can achieve the optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the case where the tail index $\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice.
  abstract_embedding: [0.302734375, 0.26171875, 0.203125]... (1536 items)
  authors: ['Zijian Liu', 'Zhengyuan Zhou']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417709574
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Nonconvex_Stochastic_Optimization_under_Heavy-Tailed_Noises__Optimal_Convergence_without_Gradient_Clipping.pdf
  sha_abstract: ef8781698909539c5f457d98666ea16eaffd8d7ba8f0993016ee1423589ace66
  title: Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping
  title_normalized: nonconvex_stochastic_optimization_under_heavytailed_noises_optimal_convergence_without_gradient_clipping

================================================================================
Document #31 (ID: jehIWJoBclM7MZc3CZJO)
================================================================================
  abstract: Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.
  abstract_embedding: [0.2578125, -0.006805419921875, 0.322265625]... (1536 items)
  authors: ['Wenhao Wu', 'Yizhong Wang', 'Guangxuan Xiao']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417707334
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Retrieval_Head_Mechanistically_Explains_Long-Context_Factuality.pdf
  sha_abstract: af70edec3b9426604b3c88ea7e943b7dc09f6c671ab0afd87f478a767fc26e26
  title: Retrieval Head Mechanistically Explains Long-Context Factuality
  title_normalized: retrieval_head_mechanistically_explains_longcontext_factuality

================================================================================
Document #32 (ID: kOhIWJoBclM7MZc3FpLm)
================================================================================
  abstract: Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various adaptation techniques such as prompt engineering, In-Context Learning (ICL), and supervised fine-tuning can further enhance the model’s performance on a downstream task, but they require substantial manual effort to construct effective prompts or labeled examples. In this work, we introduce a joint inference framework for fully unsupervised adaptation, eliminating the need for manual prompt engineering and labeled examples. Unlike zero-shot inference, which makes independent predictions, the joint inference makes predictions simultaneously for all inputs in a given task. Since direct joint inference involves computationally expensive optimization, we develop efficient approximation techniques, leading to two unsupervised adaptation methods: unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness of our methods across diverse tasks and models, including language-only Llama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math on grade school math problems, vision-language OpenFlamingo on vision tasks, and the API-only access GPT-4o model on massive multi-discipline tasks. Our experiments demonstrate substantial improvements over the standard zero-shot approach, including 39% absolute improvement on the challenging GSM8K math reasoning dataset. Remarkably, despite being fully unsupervised, our framework often performs on par with supervised approaches that rely on ground truth labels.
  abstract_embedding: [-0.08935546875, 0.375, 0.197265625]... (1536 items)
  authors: ['Artyom Gadetsky', 'Andrei Atanov', 'Yulun Jiang']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417710814
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Large__Vision__Language_Models_are_Unsupervised_In-Context_Learners.pdf
  sha_abstract: 0f73661c7f6c590e011562660724046231dfa94d1e37e37cddc93dacf4b4526f
  title: Large (Vision) Language Models are Unsupervised In-Context Learners
  title_normalized: large_vision_language_models_are_unsupervised_incontext_learners

================================================================================
Document #33 (ID: kehIWJoBclM7MZc3HZJO)
================================================================================
  abstract: The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse capabilities of GPUs suggests we might we need a wide variety of techniques to achieve high performance. However, our work explores if a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like operations, (2) at the thread-block level, we provide templates for asynchronously overlapping operations, and (3) at the grid-level, TK helps hide block launch, tear-down, and memory costs. We show the value of TK by providing simple & diverse kernels that match or outperform prior art. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40$\% on attention backwards, $8\times$ on state space models, and $14\times$ on linear attention.
  abstract_embedding: [0.1865234375, -0.059814453125, 0.38671875]... (1536 items)
  authors: ['Benjamin Frederick Spector', 'Simran Arora', 'Aaryan Singhal']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417712454
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: ThunderKittens__Simple__Fast__and___textit_Adorable___Kernels.pdf
  sha_abstract: b51d3b3a548104606f5960672c19f35f3003ca696c500df8c77d196f1fed3abf
  title: ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels
  title_normalized: thunderkittens_simple_fast_and_textitadorable_kernels

================================================================================
Document #34 (ID: i-hHWJoBclM7MZc3_pI6)
================================================================================
  abstract: Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce $DS^2$, a **D**iversity-aware **S**core curation method for **D**ata **S**election. By systematically modeling error patterns through a score transition matrix, $DS^2$ corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3\% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that ``more can be less''.
  abstract_embedding: [0.388671875, 0.2041015625, 0.244140625]... (1536 items)
  authors: ['Jinlong Pang', 'Jiaheng Wei', 'Ankit Shah']... (9 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417704494
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Improving_Data_Efficiency_via_Curating_LLM-Driven_Rating_Systems.pdf
  sha_abstract: 3c5115fbf29eb8d1030f0c8b99d76fd2e7c686dced2331392748dac665fed628
  title: Improving Data Efficiency via Curating LLM-Driven Rating Systems
  title_normalized: improving_data_efficiency_via_curating_llmdriven_rating_systems

================================================================================
Document #35 (ID: jOhIWJoBclM7MZc3ApKp)
================================================================================
  abstract: Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored. In this paper, we study the theoretical aspects of score-based discrete diffusion models under the Continuous Time Markov Chain (CTMC) framework. We introduce a discrete-time sampling algorithm in the general state space $[S]^d$ that utilizes score estimators at predefined time points. We derive convergence bounds for the Kullback-Leibler (KL) divergence and total variation (TV) distance between the generated sample distribution and the data distribution, considering both scenarios with and without early stopping under reasonable assumptions. Notably, our KL divergence bounds are nearly linear in the dimension $d$, aligning with state-of-the-art results for diffusion models. Our convergence analysis employs a Girsanov-based method and establishes key properties of the discrete score function, which are essential for characterizing the discrete-time sampling process.
  abstract_embedding: [0.5, 0.53515625, 0.25390625]... (1536 items)
  authors: ['Zikun Zhang', 'Zixiang Chen', 'Quanquan Gu']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417705621
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Convergence_of_Score-Based_Discrete_Diffusion_Models__A_Discrete-Time_Analysis.pdf
  sha_abstract: f70ec12c35a76a20f14fc445be1ff5527602e424c01ef1a7db22730b99960cbe
  title: Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis
  title_normalized: convergence_of_scorebased_discrete_diffusion_models_a_discretetime_analysis

================================================================================
Document #36 (ID: WOhHWJoBclM7MZc3DJKD)
================================================================================
  abstract: In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.

Our system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.
  abstract_embedding: [0.8515625, 0.06103515625, 0.447265625]... (1536 items)
  authors: ['Kaustubh Vyas', 'Damien Graux', 'Yijun Yang']... (11 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417642580
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: From_an_LLM_Swarm_to_a_PDDL-empowered_Hive__Planning_Self-executed_Instructions_in_a_Multi-modal_Jungle.pdf
  sha_abstract: b4a622954b6e7560e33006a2eb89c6174322b980169809f88b9b2a00cd3663dd
  title: From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle
  title_normalized: from_an_llm_swarm_to_a_pddlempowered_hive_planning_selfexecuted_instructions_in_a_multimodal_jungle

================================================================================
Document #37 (ID: VuhGWJoBclM7MZc3_JII)
================================================================================
  abstract: Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption \& instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations.
  abstract_embedding: [0.2314453125, -0.04052734375, 0.4375]... (1536 items)
  authors: ['Yuxin Guo', 'Shuailei Ma', 'Shijie Ma']... (10 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417638355
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Aligned_Better__Listen_Better_for_Audio-Visual_Large_Language_Models.pdf
  sha_abstract: aea160d31c518a004e227ae3e8b13321f328b817bd3feeb19d57c6b24724e83c
  title: Aligned Better, Listen Better for Audio-Visual Large Language Models
  title_normalized: aligned_better_listen_better_for_audiovisual_large_language_models

================================================================================
Document #38 (ID: VehGWJoBclM7MZc395IO)
================================================================================
  abstract: Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity.  These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models.
  abstract_embedding: [0.5078125, -0.0234375, 0.40625]... (1536 items)
  authors: ['Xiaofeng Lin', 'Chenheng Xu', 'Matthew Yang']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417637115
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: CTSyn__A_Foundation_Model_for_Cross_Tabular_Data_Generation.pdf
  sha_abstract: 0e3f1b904ae346acd8fce51a3ae0d89546340f4927c6fa95bf92b06fe5b012d3
  title: CTSyn: A Foundation Model for Cross Tabular Data Generation
  title_normalized: ctsyn_a_foundation_model_for_cross_tabular_data_generation

================================================================================
Document #39 (ID: VOhGWJoBclM7MZc38pId)
================================================================================
  abstract: The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\times$, without compromising the model performance compared to token-level training. Source code: \url{https://github.com/shaochenze/PatchTrain}.
  abstract_embedding: [0.291015625, 0.1767578125, 0.162109375]... (1536 items)
  authors: ['Chenze Shao', 'Fandong Meng', 'Jie Zhou']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417635858
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Beyond_Next_Token_Prediction__Patch-Level_Training_for_Large_Language_Models.pdf
  sha_abstract: 9708cfcf22e205df0997b86176d4e0ecbd09b2f0c1216a1a4d6e21eb6fdb2ced
  title: Beyond Next Token Prediction: Patch-Level Training for Large Language Models
  title_normalized: beyond_next_token_prediction_patchlevel_training_for_large_language_models

================================================================================
Document #40 (ID: UOhGWJoBclM7MZc33pKY)
================================================================================
  abstract: Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. 
Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method.
  abstract_embedding: [0.296875, 0.30078125, -0.0693359375]... (1536 items)
  authors: ['Ruiyi Fang', 'Bingheng Li', 'zhao kang']... (8 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:46.794554
  code_metadata_s3_key: UOhGWJoBclM7MZc33pKY/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: UOhGWJoBclM7MZc33pKY/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417630858
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: On_the_Benefits_of_Attribute-Driven_Graph_Domain_Adaptation.pdf
  sha_abstract: 32639fb627fececb4968bd2121248d5f9edebfae77eceb9bca62788b1b5a51ef
  title: On the Benefits of Attribute-Driven Graph Domain Adaptation
  title_normalized: on_the_benefits_of_attributedriven_graph_domain_adaptation

================================================================================
Document #41 (ID: aOhHWJoBclM7MZc3a5Ln)
================================================================================
  abstract: Optimization in deep learning remains poorly understood.  A key difficulty is that optimizers exhibit complex oscillatory dynamics, referred to as "edge of stability," which cannot be captured by traditional optimization theory.  In this paper, we show that the path taken by an oscillatory optimizer can often be captured by a  _central flow_: a differential equation which directly models the time-averaged (i.e. smoothed) optimization trajectory. We empirically show that these central flows can predict long-term optimization trajectories for generic neural networks with a high degree of numerical accuracy.  By interpreting these flows, we are able to understand  how gradient descent makes progress even as the loss sometimes goes up; how adaptive optimizers ``adapt'' to the local loss landscape; and how adaptive optimizers implicitly seek out regions of weight space where they can take larger steps.  These insights (and others) are not apparent from the optimizers' update rules, but are revealed by the central flows.  Therefore, we believe that central flows constitute a promising tool for reasoning about optimization in deep learning.
  abstract_embedding: [0.2734375, 0.19921875, -0.01153564453125]... (1536 items)
  authors: ['Jeremy Cohen', 'Alex Damian', 'Ameet Talwalkar']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417667008
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Understanding_Optimization_in_Deep_Learning_with_Central_Flows.pdf
  sha_abstract: c5c0a3b3911f0355fdb3889f0811c9f9985f0b99054055705ac877dc709b5494
  title: Understanding Optimization in Deep Learning with Central Flows
  title_normalized: understanding_optimization_in_deep_learning_with_central_flows

================================================================================
Document #42 (ID: T-hGWJoBclM7MZc32ZJO)
================================================================================
  abstract: This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal image restoration pre-training. Unlike the existing self-supervised pre-training methods, DCPT utilizes the degradation type of the input image as an extremely weak supervision, which can be effortlessly obtained, even intrinsic in all image restoration datasets. DCPT comprises two primary stages. Initially, image features are extracted from the encoder. Subsequently, a lightweight decoder, such as ResNet18, is leveraged to classify the degradation type of the input image solely based on the features extracted in the first stage, without utilizing the input image. The encoder is pre-trained with a straightforward yet potent DCPT, which is used to address universal image restoration and achieve outstanding performance. Following DCPT, both convolutional neural networks (CNNs) and transformers demonstrate performance improvements, with gains of up to 2.55 dB in the 10D all-in-one restoration task and 6.53 dB in the mixed degradation scenarios. Moreover, previous self-supervised pretraining methods, such as masked image modeling, discard the decoder after pre-training, while our DCPT utilizes the pre-trained parameters more effectively. This superiority arises from the degradation classifier acquired during DCPT, which facilitates transfer learning between models of identical architecture trained on diverse degradation types. Source code and models are available at \url{https://github.com/MILab-PKU/dcpt}.
  abstract_embedding: [-0.07373046875, 0.0306396484375, -0.08349609375]... (1536 items)
  authors: ['JiaKui Hu', 'Lujia Jin', 'Zhengjian Yao']... (4 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:37.090555
  code_metadata_s3_key: T-hGWJoBclM7MZc32ZJO/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: T-hGWJoBclM7MZc32ZJO/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417629399
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Universal_Image_Restoration_Pre-training_via_Degradation_Classification.pdf
  sha_abstract: 0ffa4f4e1c77c9dfa3cd11b517bf448061d610b00c411635b5103a0d6b234c86
  title: Universal Image Restoration Pre-training via Degradation Classification
  title_normalized: universal_image_restoration_pretraining_via_degradation_classification

================================================================================
Document #43 (ID: UehGWJoBclM7MZc34pKX)
================================================================================
  abstract: The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce port-Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.
  abstract_embedding: [0.59765625, 0.318359375, -0.0189208984375]... (1536 items)
  authors: ['Simon Heilig', 'Alessio Gravina', 'Alessandro Trenta']... (5 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:27:44.194447
  code_metadata_s3_key: UehGWJoBclM7MZc34pKX/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: UehGWJoBclM7MZc34pKX/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417631869
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Port-Hamiltonian_Architectural_Bias_for_Long-Range_Propagation_in_Deep_Graph_Networks.pdf
  sha_abstract: d9b47761e54c4140622ea15eb058b1db477fc309e9ef0d92b3597e3d7e1f5b0d
  title: Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks
  title_normalized: porthamiltonian_architectural_bias_for_longrange_propagation_in_deep_graph_networks

================================================================================
Document #44 (ID: ZehHWJoBclM7MZc3WJK_)
================================================================================
  abstract: An important prerequisite for safe control is aligning the policy with the underlying constraints in the environment. In many real-world applications, due to the difficulty of manually specifying these constraints, existing works have proposed recovering constraints from expert demonstrations by solving the Inverse Constraint Learning (ICL) problem. However, ICL is inherently ill-posed, as multiple constraints can equivalently explain the experts' preferences, making the optimal solutions not uniquely identifiable. In this work, instead of focusing solely on a single constraint, we propose the novel approach of Exploratory ICL (ExICL). The goal of ExICL is to recover a diverse set of feasible constraints, thereby providing practitioners the flexibility to select the most appropriate constraint based on the practical needs of deployment. To achieve this goal, we design a generative diffusion verifier that guides the trajectory generation process using the probabilistic representation of an optimal constrained policy. By comparing these decisions with those made by expert agents, we can efficiently verify a candidate constraint. Driven by the verification feedback, ExICL implements an exploratory constraint update mechanism that strategically facilitates diversity within the collection of feasible constraints. Our empirical results demonstrate that ExICL can seamlessly and reliably generalize across different tasks and environments. The code is available at https://github.com/ZhaoRunyi/ExICL.
  abstract_embedding: [0.57421875, 0.73046875, -0.039306640625]... (1536 items)
  authors: ['Runyi Zhao', 'Sheng Xu', 'Bo Yue']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417662107
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Toward_Exploratory_Inverse_Constraint_Inference_with_Generative_Diffusion_Verifiers.pdf
  sha_abstract: 4d18304d337bc38f84922e8603772c257d14737c3af47ca881dcc542344cdf01
  title: Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers
  title_normalized: toward_exploratory_inverse_constraint_inference_with_generative_diffusion_verifiers

================================================================================
Document #45 (ID: Z-hHWJoBclM7MZc3Y5I7)
================================================================================
  abstract: Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\textit{despite an initial expected increase}$. We call this phenomenon as the $\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. 
We tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains.
  abstract_embedding: [0.443359375, 0.279296875, -0.041748046875]... (1536 items)
  authors: ['Sachin Goyal', 'Christina Baek', 'J Zico Kolter']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417664809
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Context-Parametric_Inversion__Why_Instruction_Finetuning_May_Not_Actually_Improve_Context_Reliance.pdf
  sha_abstract: ddb23dfb5f724a0aa2c4422256a6951b7cea7096a55b22eda400200b1b3bfe64
  title: Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance
  title_normalized: contextparametric_inversion_why_instruction_finetuning_may_not_actually_improve_context_reliance

================================================================================
Document #46 (ID: ZuhHWJoBclM7MZc3XZJ5)
================================================================================
  abstract: Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can efficiently leverage domain information from distributed domains. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome). The reproducible code is publicly available~\footnote[1]{\url{https://github.com/skydvn/fedomg}}.
  abstract_embedding: [0.625, 0.4609375, -0.13671875]... (1536 items)
  authors: ['Trong Binh Nguyen', 'Duong Minh Nguyen', 'Jinsun Park']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417663337
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Federated_Domain_Generalization_with_Data-free_On-server_Matching_Gradient.pdf
  sha_abstract: e5451dd8105d59d1129baa649e484046b9abe7b66cec7d026cd08ddafc3bf941
  title: Federated Domain Generalization with Data-free On-server Matching Gradient
  title_normalized: federated_domain_generalization_with_datafree_onserver_matching_gradient

================================================================================
Document #47 (ID: ZOhHWJoBclM7MZc3U5J2)
================================================================================
  abstract: We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks—including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario—demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments.
  abstract_embedding: [0.6640625, 0.267578125, 0.2734375]... (1536 items)
  authors: ['Wonje Choi', 'Jinwoo Park', 'Sanghyun Ahn']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417660778
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: NeSyC__A_Neuro-symbolic_Continual_Learner_For_Complex_Embodied_Tasks_in_Open_Domains.pdf
  sha_abstract: 174ab84a6e13c0f94b91c0fc30c0c130aabe6c3bb1898c7e106dddef745edae2
  title: NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains
  title_normalized: nesyc_a_neurosymbolic_continual_learner_for_complex_embodied_tasks_in_open_domains

================================================================================
Document #48 (ID: YuhHWJoBclM7MZc3RJLz)
================================================================================
  abstract: Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions.
  abstract_embedding: [0.259765625, 0.380859375, 0.294921875]... (1536 items)
  authors: ['Shansan Gong', 'Shivam Agarwal', 'Yizhe Zhang']... (12 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417657066
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Scaling_Diffusion_Language_Models_via_Adaptation_from_Autoregressive_Models.pdf
  sha_abstract: 78f73b00d77f60394656599f83889b95403bbb4502ce0e3651797efb6939554d
  title: Scaling Diffusion Language Models via Adaptation from Autoregressive Models
  title_normalized: scaling_diffusion_language_models_via_adaptation_from_autoregressive_models

================================================================================
Document #49 (ID: Y-hHWJoBclM7MZc3TJII)
================================================================================
  abstract: We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules.
  abstract_embedding: [0.3515625, 0.361328125, 0.3046875]... (1536 items)
  authors: ['Arne Schneuing', 'Ilia Igashov', 'Adrian W. Dobbelstein']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417658878
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Multi-domain_Distribution_Learning_for_De_Novo_Drug_Design.pdf
  sha_abstract: 7299172cd01a0123aae5f15f4fa31f56f0fe25422cdc70ad62a6b2f3687da9d2
  title: Multi-domain Distribution Learning for De Novo Drug Design
  title_normalized: multidomain_distribution_learning_for_de_novo_drug_design

================================================================================
Document #50 (ID: XehHWJoBclM7MZc3J5KN)
================================================================================
  abstract: In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects  spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios. We propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.
  abstract_embedding: [-0.2080078125, 0.1220703125, 0.515625]... (1536 items)
  authors: ['Akash Kumar', 'Zsolt Kira', 'Yogesh S Rawat']
  code_generated: True
  code_generated_at: 2025-11-06T08:28:09.833689
  code_metadata_s3_key: XehHWJoBclM7MZc3J5KN/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: XehHWJoBclM7MZc3J5KN/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417649538
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Contextual_Self-paced_Learning_for_Weakly_Supervised_Spatio-Temporal_Video_Grounding.pdf
  sha_abstract: f0ef99df8e38574f2f50aa9ed6a0bb51cef9b41af8cda2245698d6a5d3c13bb8
  title: Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding
  title_normalized: contextual_selfpaced_learning_for_weakly_supervised_spatiotemporal_video_grounding

================================================================================
Document #51 (ID: duhHWJoBclM7MZc3wpKf)
================================================================================
  abstract: This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal image restoration pre-training. Unlike the existing self-supervised pre-training methods, DCPT utilizes the degradation type of the input image as an extremely weak supervision, which can be effortlessly obtained, even intrinsic in all image restoration datasets. DCPT comprises two primary stages. Initially, image features are extracted from the encoder. Subsequently, a lightweight decoder, such as ResNet18, is leveraged to classify the degradation type of the input image solely based on the features extracted in the first stage, without utilizing the input image. The encoder is pre-trained with a straightforward yet potent DCPT, which is used to address universal image restoration and achieve outstanding performance. Following DCPT, both convolutional neural networks (CNNs) and transformers demonstrate performance improvements, with gains of up to 2.55 dB in the 10D all-in-one restoration task and 6.53 dB in the mixed degradation scenarios. Moreover, previous self-supervised pretraining methods, such as masked image modeling, discard the decoder after pre-training, while our DCPT utilizes the pre-trained parameters more effectively. This superiority arises from the degradation classifier acquired during DCPT, which facilitates transfer learning between models of identical architecture trained on diverse degradation types. Source code and models are available at \url{https://github.com/MILab-PKU/dcpt}.
  abstract_embedding: [-0.07373046875, 0.0306396484375, -0.08349609375]... (1536 items)
  authors: ['JiaKui Hu', 'Lujia Jin', 'Zhengjian Yao']... (4 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417689238
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: Universal_Image_Restoration_Pre-training_via_Degradation_Classification.pdf
  sha_abstract: 0ffa4f4e1c77c9dfa3cd11b517bf448061d610b00c411635b5103a0d6b234c86
  title: Universal Image Restoration Pre-training via Degradation Classification
  title_normalized: universal_image_restoration_pretraining_via_degradation_classification

================================================================================
Document #52 (ID: cuhHWJoBclM7MZc3sZKF)
================================================================================
  abstract: First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or handcrafted templates, making it difficult to achieve the necessary complexity, scalability, and diversity for robust evaluation. To address these limitations, we propose a novel framework called ProverGen that synergizes the generative strengths of Large Language Models (LLMs) with the rigor and precision of symbolic provers, enabling the creation of a scalable, diverse, and high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by its inclusion of accessible and logically coherent intermediate reasoning steps for each problem. Our evaluation shows that state-of-the-art LLMs struggle to solve ProverQA problems, even with CoT prompting, highlighting the dataset's challenging nature. We also finetune Llama3.1-8B-Instruct on a separate training set generated by our framework.
The finetuned model demonstrates consistent improvements on both in-distribution and out-of-distribution test sets, suggesting the value of our proposed data generation framework. Code available at: \url{https://github.com/opendatalab/ProverGen}
  abstract_embedding: [0.23828125, 0.443359375, 0.07080078125]... (1536 items)
  authors: ['Chengwen Qi', 'Ren Ma', 'Bowen Li']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417684858
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Large_Language_Models_Meet_Symbolic_Provers_for_Logical_Reasoning_Evaluation.pdf
  sha_abstract: b644c34c9c0e4766fc8f7b20535638636e876085f3ad3e0f2db94e88c30a7b7e
  title: Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation
  title_normalized: large_language_models_meet_symbolic_provers_for_logical_reasoning_evaluation

================================================================================
Document #53 (ID: cehHWJoBclM7MZc3rJLp)
================================================================================
  abstract: The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents---which use external tools and can execute multi-stage tasks---may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly complaint with malicious agent requests without jailbreaking, (2) simple universal jailbreak strings can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.
  abstract_embedding: [1.203125, 0.357421875, 0.02392578125]... (1536 items)
  authors: ['Maksym Andriushchenko', 'Alexandra Souly', 'Mateusz Dziemian']... (12 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417683678
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: AgentHarm__A_Benchmark_for_Measuring_Harmfulness_of_LLM_Agents.pdf
  sha_abstract: 112e83a7b152a913153f288f3bee4ae334b3671856467caec6ca6ee3eaa1b397
  title: AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents
  title_normalized: agentharm_a_benchmark_for_measuring_harmfulness_of_llm_agents

================================================================================
Document #54 (ID: dehHWJoBclM7MZc3v5Le)
================================================================================
  abstract: Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding & reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth of software and code comprehension in LLMs. CodeMMLU includes nearly 20,000 questions spanning diverse domains, including code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks that emphasize code generation, CodeMMLU assesses a model’s ability to reason about programs across a wide-range of tasks such as code repair, execution reasoning, and fill-in-the-blank challenges. Our extensive evaluation reveals that even state-of-the-art models struggle with CodeMMLU, highlighting significant gaps in comprehension beyond generation. By emphasizing the essential connection between code understanding and effective AI-assisted development, CodeMMLU provides a critical resource for advancing more reliable and capable coding assistants.
  abstract_embedding: [0.4375, 0.0291748046875, 0.390625]... (1536 items)
  authors: ['Dung Manh Nguyen', 'Thang Chau Phan', 'Nam Le Hai']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417688503
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: CodeMMLU__A_Multi-Task_Benchmark_for_Assessing_Code_Understanding___Reasoning_Capabilities_of_CodeLLMs.pdf
  sha_abstract: 7c84a259afc65efa679d9499ff6a900f1586d356dd47469613a886e159221043
  title: CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs
  title_normalized: codemmlu_a_multitask_benchmark_for_assessing_code_understanding__reasoning_capabilities_of_codellms

================================================================================
Document #55 (ID: dOhHWJoBclM7MZc3u5JF)
================================================================================
  abstract: Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors, performance remains suboptimal. In this work, we introduce DiTTo-TTS, a Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based TTS can achieve state-of-the-art performance without domain-specific factors. Through rigorous analysis and empirical exploration, we find that (1) DiT with minimal modifications outperforms U-Net, (2) variable-length modeling with a speech length predictor significantly improves results over fixed-length approaches, and (3) conditions like semantic alignment in speech latent representations are key to further enhancement. By scaling our training data to 82K hours and the model size to 790M parameters, we achieve superior or comparable zero-shot performance to state-of-the-art TTS models in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific factors. Speech samples are available at https://ditto-tts.github.io.
  abstract_embedding: [0.3125, 0.392578125, 0.1328125]... (1536 items)
  authors: ['Keon Lee', 'Dong Won Kim', 'Jaehyeon Kim']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417687319
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: DiTTo-TTS__Diffusion_Transformers_for_Scalable_Text-to-Speech_without_Domain-Specific_Factors.pdf
  sha_abstract: 73fd9e3269f52eca5ac6eca6d24e70d4e8dec5781900c25a45bf24bc58a4a45d
  title: DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors
  title_normalized: dittotts_diffusion_transformers_for_scalable_texttospeech_without_domainspecific_factors

================================================================================
Document #56 (ID: c-hHWJoBclM7MZc3tpJY)
================================================================================
  abstract: Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map---namely, the negative entropy---which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. Using evolutionary strategies, we identify more efficient mirror maps that enhance the performance of PMD. We first focus on a tabular environment, i.e.\ Grid-World, where we relate existing theoretical bounds with the performance of PMD for a few standard mirror maps and the learned one. We then show that it is possible to learn a mirror map that outperforms the negative entropy in more complex environments, such as the MinAtar suite. Additionally, we demonstrate that the learned mirror maps generalize effectively to different tasks by testing each map across various other environments.
  abstract_embedding: [0.9375, 0.5625, 0.05908203125]... (1536 items)
  authors: ['Carlo Alfano', 'Sebastian Rene Towers', 'Silvia Sapora']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417686097
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Learning_mirror_maps_in_policy_mirror_descent.pdf
  sha_abstract: 46b3d71542c2cb31f9802e0047593be6abe7594059d25028a51bca0493ee2e29
  title: Learning mirror maps in policy mirror descent
  title_normalized: learning_mirror_maps_in_policy_mirror_descent

================================================================================
Document #57 (ID: eOhHWJoBclM7MZc3x5KH)
================================================================================
  abstract: Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. 
Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method.
  abstract_embedding: [0.296875, 0.30078125, -0.0693359375]... (1536 items)
  authors: ['Ruiyi Fang', 'Bingheng Li', 'zhao kang']... (8 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417690491
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: On_the_Benefits_of_Attribute-Driven_Graph_Domain_Adaptation.pdf
  sha_abstract: 32639fb627fececb4968bd2121248d5f9edebfae77eceb9bca62788b1b5a51ef
  title: On the Benefits of Attribute-Driven Graph Domain Adaptation
  title_normalized: on_the_benefits_of_attributedriven_graph_domain_adaptation

================================================================================
Document #58 (ID: iehHWJoBclM7MZc39JI5)
================================================================================
  abstract: We study beyond worst-case dimensionality reduction for $s$-sparse vectors (vectors with at most $s$ non-zero coordinates). Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis:

\noindent (a)  We first consider average-case guarantees for embedding $s$-sparse vectors. Here, a well-known folklore upper bound based on the birthday-paradox states: For any collection $X$ of $s$-sparse vectors in $\mathbb{R}^d$, there exists a linear map $A: \mathbb{R}^d \rightarrow \mathbb{R}^{O(s^2)}$ which \emph{exactly} preserves the norm of $99\%$ of the vectors in $X$ in any $\ell_p$ norm (as opposed to the usual setting where guarantees hold for all vectors). We provide novel lower bounds showing that this is indeed optimal in many settings. Specifically, any oblivious linear map satisfying similar average-case guarantees must map to $\Omega(s^2)$ dimensions. The same lower bound also holds for a wider class of sufficiently smooth maps, including `encoder-decoder schemes', where we compare the norm of the original vector to that of a smooth function of the embedding. These lower bounds reveal a surprising separation result for smooth embeddings of sparse vectors, as an upper bound of $O(s \log(d))$ is possible if we instead use arbitrary functions, e.g., via compressed sensing algorithms.


 (b) Given these lower bounds, we specialize to sparse \emph{non-negative} vectors to hopes of improved upper bounds. For a dataset $X$ of non-negative $s$-sparse vectors and any $p \ge 1$, we can non-linearly embed $X$ to $O(s\log(|X|s)/\varepsilon^2)$ dimensions while preserving all pairwise distances in $\ell_p$ norm up to $1\pm \varepsilon$, with no dependence on $p$. Surprisingly, the non-negativity assumption enables much smaller embeddings than arbitrary sparse vectors, where the best known bound suffers an exponential $(\log |X|)^{O(p)}$ dependence. Our map also guarantees \emph{exact} dimensionality reduction for the $\ell_{\infty}$ norm by embedding $X$ into $O(s\log |X|)$ dimensions, which is tight. We further give separation results showing that both the non-linearity of $f$ and the non-negativity of $X$ are necessary, and provide downstream algorithmic improvements using our embedding.
  abstract_embedding: [0.17369219660758972, 0.17889337241649628, 0.41720280051231384]... (1536 items)
  authors: ['Sandeep Silwal', 'David Woodruff', 'Qiuyi Zhang']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417701934
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Beyond_Worst-Case_Dimensionality_Reduction_for_Sparse_Vectors.pdf
  sha_abstract: c617cdcdeff48204b886058fea7e8094a035f52e752bf42f1afbd9d0096f2cba
  title: Beyond Worst-Case Dimensionality Reduction for Sparse Vectors
  title_normalized: beyond_worstcase_dimensionality_reduction_for_sparse_vectors

================================================================================
Document #59 (ID: g-hHWJoBclM7MZc37JLf)
================================================================================
  abstract: Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. 
In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\times$ speedup. In addition, our investigation raises doubts about whether MDMs can truly beat ARMs in text generation. We identify, for the first time, an underlying numerical issue, even with the commonly used 32-bit floating-point precision, which results in inaccurate categorical sampling. 
We show that it lowers the effective temperature both theoretically and empirically, and the resulting decrease in token diversity makes previous evaluations, which assess the generation quality solely through the incomplete generative perplexity metric, somewhat unfair.
  abstract_embedding: [0.154296875, 0.7109375, 0.46875]... (1536 items)
  authors: ['Kaiwen Zheng', 'Yongxin Chen', 'Hanzi Mao']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417700054
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Masked_Diffusion_Models_are_Secretly_Time-Agnostic_Masked_Models_and_Exploit_Inaccurate_Categorical_Sampling.pdf
  sha_abstract: 0c27bc80e84183a0ab222964b4054fbc1ab68222bb7a744b09729da3407a2bc5
  title: Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling
  title_normalized: masked_diffusion_models_are_secretly_timeagnostic_masked_models_and_exploit_inaccurate_categorical_sampling

================================================================================
Document #60 (ID: iOhHWJoBclM7MZc38pJo)
================================================================================
  abstract: In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.

Our system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.
  abstract_embedding: [0.8515625, 0.06103515625, 0.447265625]... (1536 items)
  authors: ['Kaustubh Vyas', 'Damien Graux', 'Yijun Yang']... (11 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417701473
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: From_an_LLM_Swarm_to_a_PDDL-empowered_Hive__Planning_Self-executed_Instructions_in_a_Multi-modal_Jungle.pdf
  sha_abstract: b4a622954b6e7560e33006a2eb89c6174322b980169809f88b9b2a00cd3663dd
  title: From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle
  title_normalized: from_an_llm_swarm_to_a_pddlempowered_hive_planning_selfexecuted_instructions_in_a_multimodal_jungle

================================================================================
Document #61 (ID: hOhHWJoBclM7MZc37pIN)
================================================================================
  abstract: Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption \& instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations.
  abstract_embedding: [0.2314453125, -0.04052734375, 0.4375]... (1536 items)
  authors: ['Yuxin Guo', 'Shuailei Ma', 'Shijie Ma']... (10 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417700358
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: Aligned_Better__Listen_Better_for_Audio-Visual_Large_Language_Models.pdf
  sha_abstract: aea160d31c518a004e227ae3e8b13321f328b817bd3feeb19d57c6b24724e83c
  title: Aligned Better, Listen Better for Audio-Visual Large Language Models
  title_normalized: aligned_better_listen_better_for_audiovisual_large_language_models

================================================================================
Document #62 (ID: hehHWJoBclM7MZc375Im)
================================================================================
  abstract: Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. 
While effective in fine-tuning, KD during pre-training faces efficiency, flexibility, and effectiveness issues. 
Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data.
In this work, we propose **MiniPLM**, a KD framework for pre-training LMs by refining the training data distribution with the teacher LM's knowledge.
For efficiency, MiniPLM performs offline teacher inference, allowing KD for multiple student LMs without adding training costs.
For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families.
For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the training data difficulty and diversity, helping student LMs acquire versatile and sophisticated knowledge.
Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 common downstream tasks, improves language modeling capabilities, and reduces pre-training computation. 
The benefit of MiniPLM extends to larger training scales, evidenced by the scaling curve extrapolation.
Further analysis reveals that MiniPLM supports KD across model families and enhances the pre-training data utilization. Our code, data, and models can be found at https://github.com/thu-coai/MiniPLM.
  abstract_embedding: [0.4296875, 0.44921875, 0.392578125]... (1536 items)
  authors: ['Yuxian Gu', 'Hao Zhou', 'Fandong Meng']... (5 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417700638
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: MiniPLM__Knowledge_Distillation_for_Pre-training_Language_Models.pdf
  sha_abstract: 23c4a07f43020622c025e4aecec48dd5be474aeb05ee40c87ab7d2d8cc02dfee
  title: MiniPLM: Knowledge Distillation for Pre-training Language Models
  title_normalized: miniplm_knowledge_distillation_for_pretraining_language_models

================================================================================
Document #63 (ID: h-hHWJoBclM7MZc38ZI7)
================================================================================
  abstract: We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks.
  abstract_embedding: [0.02734375, 0.349609375, 0.3828125]... (1536 items)
  authors: ['Minyoung Kim', 'Timothy Hospedales']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417701136
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: LiFT__Learning_to_Fine-Tune_via_Bayesian_Parameter_Efficient_Meta_Fine-Tuning.pdf
  sha_abstract: acb24036a419b330039e747be336d223aa500add9cd4638723a38c45b0f9229a
  title: LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning
  title_normalized: lift_learning_to_finetune_via_bayesian_parameter_efficient_meta_finetuning

================================================================================
Document #64 (ID: guhHWJoBclM7MZc365Lz)
================================================================================
  abstract: Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity.  These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models.
  abstract_embedding: [0.5078125, -0.0234375, 0.40625]... (1536 items)
  authors: ['Xiaofeng Lin', 'Chenheng Xu', 'Matthew Yang']... (4 items)
  date: 2024-10-04
  decision: reject
  ingested_at: 1762417699817
  novelty: unknown
  reason: Exact duplicate by title_normalized or sha_abstract
  rejected_by: exact_duplicate
  relevance: unknown
  s3_bucket: llm-research-papers
  s3_key: CTSyn__A_Foundation_Model_for_Cross_Tabular_Data_Generation.pdf
  sha_abstract: 0e3f1b904ae346acd8fce51a3ae0d89546340f4927c6fa95bf92b06fe5b012d3
  title: CTSyn: A Foundation Model for Cross Tabular Data Generation
  title_normalized: ctsyn_a_foundation_model_for_cross_tabular_data_generation

================================================================================
Document #65 (ID: huhHWJoBclM7MZc38JIR)
================================================================================
  abstract: Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at \url{https://github.com/thu-ml/DiffusionBridge}.
  abstract_embedding: [0.1435546875, 0.6328125, 0.38671875]... (1536 items)
  authors: ['Kaiwen Zheng', 'Guande He', 'Jianfei Chen']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417700874
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Diffusion_Bridge_Implicit_Models.pdf
  sha_abstract: 02ebb6c664597ebfb273f873b506b1c97b12ea6ef9940f138a945d4be57ee4ae
  title: Diffusion Bridge Implicit Models
  title_normalized: diffusion_bridge_implicit_models

================================================================================
Document #66 (ID: lehIWJoBclM7MZc3MpJm)
================================================================================
  abstract: Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a constant learning rate to produce a main branch of iterates that can in principle continue indefinitely without a pre-specified compute budget. Then, given any compute budget, one can branch out from the main branch at a proper time with a rapidly decaying learning rate to produce a strong model. Empirically, WSD generates an intriguing, non-traditional loss curve: the loss remains elevated during the stable phase but sharply declines during the decay phase. Towards explaining this phenomenon, we conjecture that pretraining loss exhibits a river valley landscape, which resembles a deep valley with a river at its bottom. Under this assumption, we show that during the stable phase, the iterate undergoes large oscillations due to the high learning rate, yet it progresses swiftly along the river. During the decay phase, the rapidly dropping learning rate minimizes the iterate’s oscillations, moving it closer to the river and revealing true optimization progress. Therefore, the sustained high learning rate phase and fast decaying phase are responsible for progress in the river and the mountain directions, respectively, and are both critical. Our analysis predicts phenomenons consistent with empirical observations and shows that this landscape can naturally emerge from pretraining on a simple bi-gram dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that reuses previous checkpoints’ decay phases and keeps only one main branch, where we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and Cyclic-Cosine in obtaining multiple pretrained language model checkpoints across various compute budgets in a single run for parameters scaling from 0.1B to 1.2B.
  abstract_embedding: [0.2236328125, 0.337890625, -0.09521484375]... (1536 items)
  authors: ['Kaiyue Wen', 'Zhiyuan Li', 'Jason S. Wang']... (6 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417717854
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Understanding_Warmup-Stable-Decay_Learning_Rates__A_River_Valley_Loss_Landscape_View.pdf
  sha_abstract: b2804e5eb0d77fef57d645fb89c3e16a56fa916cc6ca278d712859f3142691b3
  title: Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View
  title_normalized: understanding_warmupstabledecay_learning_rates_a_river_valley_loss_landscape_view

================================================================================
Document #67 (ID: mehIWJoBclM7MZc3RJLp)
================================================================================
  abstract: Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.
  abstract_embedding: [0.5546875, -0.09716796875, 0.3203125]... (1536 items)
  authors: ['Nikunj Saunshi', 'Nishanth Dikkala', 'Zhiyuan Li']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417722555
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Reasoning_with_Latent_Thoughts__On_the_Power_of_Looped_Transformers.pdf
  sha_abstract: 117b4a8d098d1d18e19701927e217ce52282f3b874faf73c968c79e3a9335147
  title: Reasoning with Latent Thoughts: On the Power of Looped Transformers
  title_normalized: reasoning_with_latent_thoughts_on_the_power_of_looped_transformers

================================================================================
Document #68 (ID: lOhIWJoBclM7MZc3LZJ-)
================================================================================
  abstract: This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain solutions that are accurate to near floating point error, and (2) numerical generality, i.e. we want them to apply broadly across problem instances. We find that prior approaches using Transformers fail to meet these criteria, and identify limitations present in existing architectures and training procedures. First, we show that softmax Transformers struggle to perform high-precision multiplications, which prevents them from precisely learning numerical algorithms. Second, we identify an alternate class of architectures, comprised entirely of polynomials, that can efficiently represent high-precision gradient descent iterates. Finally, we investigate precision bottlenecks during training and address them via a high-precision training recipe that reduces stochastic gradient noise. Our recipe enables us to train two polynomial architectures, gated convolutions and linear attention, to perform gradient descent iterates on least squares problems. For the first time, we demonstrate the ability to train to near machine precision. Applied iteratively, our models obtain $100,000\times$ lower MSE than standard Transformers trained end-to-end and they incur a $10,000\times$ smaller generalization gap on out-of-distribution problems. We make progress towards end-to-end learning of numerical algorithms for least squares.
  abstract_embedding: [0.2470703125, 0.1494140625, 0.27734375]... (1536 items)
  authors: ['Jerry Weihong Liu', 'Jessica Grogan', 'Owen M Dugan']... (7 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417716594
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Towards_Learning_High-Precision_Least_Squares_Algorithms_with_Sequence_Models.pdf
  sha_abstract: a0aacaa6150ff4c3864629affca9d59c7d3fcd0f37d4a48bf46efd5ef51d0945
  title: Towards Learning High-Precision Least Squares Algorithms with Sequence Models
  title_normalized: towards_learning_highprecision_least_squares_algorithms_with_sequence_models

================================================================================
Document #69 (ID: k-hIWJoBclM7MZc3J5KQ)
================================================================================
  abstract: Modern language models have demonstrated remarkable reasoning capabilities by using chain-of-thought (CoT). One hypothesis about the inner workings of CoT is that it breaks down originally complex tasks into smaller subtasks that are more amenable to learning. We formalize this notion by showing possibility and impossibility results of learning from in-context demonstrations with and without CoT. In particular, with CoT, we examine a family of learning algorithms that learn a task step-by-step, capable of composing simpler functions from individual reasoning steps to form an overall complex function. This process reduces the difficulty of learning a task to that of the hardest reasoning step in the chain. Moreover, we prove Transformers can express this algorithm and thus they can efficiently in-context learn arbitrary tasks as long as these tasks can be decomposed into a finite number of subtasks, each of which are efficiently learnable. In contrast, without CoT, we demonstrate that there exist tasks that are inherently unlearnable by the same algorithm. Overall, our results suggest several provably effective ways for decomposing target problems to instantiate CoT. Empirically, we demonstrate our proposed CoT construction significantly enhances the reasoning capabilities of real-world LLMs in solving challenging arithmetic reasoning tasks, including learning polynomials and Boolean formulas.
  abstract_embedding: [0.5703125, -0.150390625, 0.224609375]... (1536 items)
  authors: ['Chenxiao Yang', 'Zhiyuan Li', 'David Wipf']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417715074
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Chain-of-Thought_Provably_Enables_Learning_the__Otherwise__Unlearnable.pdf
  sha_abstract: bfd8f4faa8a534eac651d564ba4e8c06e23f674cc628d719487541d41f4e6e89
  title: Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable
  title_normalized: chainofthought_provably_enables_learning_the_otherwise_unlearnable

================================================================================
Document #70 (ID: muhIWJoBclM7MZc3S5J7)
================================================================================
  abstract: Despite extensive research, recovering PDE expressions from experimental observations often involves symbolic regression. This method generally lacks the incorporation of meaningful physical insights, resulting in outcomes lacking clear physical interpretations. Recognizing that the primary interest of Machine Learning for Science (ML4Sci) often lies in understanding the underlying physical mechanisms or even discovering new physical laws rather than simply obtaining mathematical expressions, this paper introduces a novel ML4Sci task paradigm. This paradigm focuses on interpreting experimental data within the framework of prior physical hypotheses and theories, thereby guiding and constraining the discovery of PDE expressions. We have formulated this approach as a nonlinear mixed-integer programming (MIP) problem, addressed through an efficient search scheme developed for this purpose. Our experiments on newly designed Fluid Mechanics and Laser Fusion datasets demonstrate the interpretability and feasibility of this method.
  abstract_embedding: [0.337890625, 0.1181640625, -0.12060546875]... (1536 items)
  authors: ['Mingquan Feng', 'Yixin Huang', 'Yizhou Liu']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417724235
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: PhysPDE__Rethinking_PDE_Discovery_and_a_Physical_HYpothesis_Selection_Benchmark.pdf
  sha_abstract: 33c054c02e54d51c654da4c978d0c79c569da73345e9225f21cf57f089d47316
  title: PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark
  title_normalized: physpde_rethinking_pde_discovery_and_a_physical_hypothesis_selection_benchmark

================================================================================
Document #71 (ID: mOhIWJoBclM7MZc3QJJi)
================================================================================
  abstract: Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset.
  abstract_embedding: [0.376953125, 0.451171875, 0.30078125]... (1536 items)
  authors: ['Linus Bao', 'Emily Jin', 'Michael M. Bronstein']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417721420
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Homomorphism_Counts_as_Structural_Encodings_for_Graph_Learning.pdf
  sha_abstract: aabb8fb183e66f16ec0357e532f2beb59779f15090a469d651cfcfca639d9150
  title: Homomorphism Counts as Structural Encodings for Graph Learning
  title_normalized: homomorphism_counts_as_structural_encodings_for_graph_learning

================================================================================
Document #72 (ID: l-hIWJoBclM7MZc3PJIH)
================================================================================
  abstract: Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our empirical analysis finds that, for deeper neural networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\alpha$-SVRG. Our results show $\alpha$-SVRG better optimizes models, consistently reducing training loss compared to the baseline and standard SVRG across various model architectures and multiple image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at github.com/davidyyd/alpha-SVRG.
  abstract_embedding: [0.41015625, 0.197265625, -0.11083984375]... (1536 items)
  authors: ['Yida Yin', 'Zhiqiu Xu', 'Zhiyuan Li']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417720318
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: A_Coefficient_Makes_SVRG_Effective.pdf
  sha_abstract: 239e70df6a9d37c53d43744e790606c1a9a9cc55401e7c5e7658c9167d3edd5d
  title: A Coefficient Makes SVRG Effective
  title_normalized: a_coefficient_makes_svrg_effective

================================================================================
Document #73 (ID: m-hIWJoBclM7MZc3UJI_)
================================================================================
  abstract: Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically --  previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\ell_\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\ell_\infty$-geometry rather than the more common $\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\ell_\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions.
  abstract_embedding: [0.91796875, 0.142578125, 0.3125]... (1536 items)
  authors: ['Shuo Xie', 'Mohamad Amin Mohamadi', 'Zhiyuan Li']
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417725494
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Adam_Exploits___ell__infty_-geometry_of_Loss_Landscape_via_Coordinate-wise_Adaptivity.pdf
  sha_abstract: 2882a305d1a330b405e2214343e56b0bf3d6e093dcac7a434d7f79d153a7c913
  title: Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity
  title_normalized: adam_exploits_ellinftygeometry_of_loss_landscape_via_coordinatewise_adaptivity

================================================================================
Document #74 (ID: d-hHWJoBclM7MZc3xJI-)
================================================================================
  abstract: Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities --- popular text, speech, and video datasets --- from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.
  abstract_embedding: [0.419921875, 0.318359375, 0.263671875]... (1536 items)
  authors: ['Shayne Longpre', 'Nikhil Singh', 'Manuel Cherep']... (43 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:28:59.602254
  code_metadata_s3_key: d-hHWJoBclM7MZc3xJI-/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: d-hHWJoBclM7MZc3xJI-/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417689623
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Bridging_the_Data_Provenance_Gap_Across_Text__Speech__and_Video.pdf
  sha_abstract: 5b99f498a593207ff7578c6e1ca895af9d6fde5f1b679116650aa44c357d16f4
  title: Bridging the Data Provenance Gap Across Text, Speech, and Video
  title_normalized: bridging_the_data_provenance_gap_across_text_speech_and_video

================================================================================
Document #75 (ID: iuhHWJoBclM7MZc3-pI4)
================================================================================
  abstract: Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.
  abstract_embedding: [0.48828125, 0.40234375, 0.271484375]... (1536 items)
  authors: ['Kaiwen Zheng', 'Guande He', 'Jianfei Chen']... (5 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:28:54.406071
  code_metadata_s3_key: iuhHWJoBclM7MZc3-pI4/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: iuhHWJoBclM7MZc3-pI4/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417703435
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Elucidating_the_Preconditioning_in_Consistency_Distillation.pdf
  sha_abstract: 92ac9a9c96c0178e57c8869d44e27ffa094e424fa40fffc5b6a8a5555482ea3c
  title: Elucidating the Preconditioning in Consistency Distillation
  title_normalized: elucidating_the_preconditioning_in_consistency_distillation

================================================================================
Document #76 (ID: juhIWJoBclM7MZc3DZLI)
================================================================================
  abstract: Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. 
Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. 
Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models.
To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions.
Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. 
The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.
  abstract_embedding: [0.28125, 0.05712890625, 0.1884765625]... (1536 items)
  authors: ['Juyeon Heo', 'Miao Xiong', 'Christina Heinze-Deml']... (4 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:29:02.215763
  code_metadata_s3_key: juhIWJoBclM7MZc3DZLI/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: juhIWJoBclM7MZc3DZLI/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417708474
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Do_LLMs_estimate_uncertainty_well_in_instruction-following_.pdf
  sha_abstract: b76bd4af926c2820594d770badbcca82d202a958509c80a74e267c99e086fe7c
  title: Do LLMs estimate uncertainty well in instruction-following?
  title_normalized: do_llms_estimate_uncertainty_well_in_instructionfollowing

================================================================================
Document #77 (ID: nuhIWJoBclM7MZc3YJLc)
================================================================================
  abstract: Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.
  abstract_embedding: [-0.166015625, 0.09228515625, 0.07421875]... (1536 items)
  authors: ['Jaeseong Lee', 'Taewoong Kang', 'Marcel Buehler']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417729746
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: SurFhead__Affine_Rig_Blending_for_Geometrically_Accurate_2D_Gaussian_Surfel_Head_Avatars.pdf
  sha_abstract: 35510a1106bed9af79c5d43b4c845d70c609807d591f74717aa0757c90b1dc20
  title: SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars
  title_normalized: surfhead_affine_rig_blending_for_geometrically_accurate_2d_gaussian_surfel_head_avatars

================================================================================
Document #78 (ID: nehIWJoBclM7MZc3W5Kq)
================================================================================
  abstract: Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc. However, real-time portrait editing presents three significant challenges: identity preservation, fidelity to editing instructions, and fast model inference. Given that these aspects often present a trade-off, concurrently addressing them poses an even greater challenge. While diffusion-based image editing methods have shown promising capabilities in personalized image editing in recent years, they lack a dedicated focus on portrait editing and thus suffer from the aforementioned problems as well. To address the gap, this paper introduces an Instant-Portrait Network (IPNet), the first one-step diffusion-based model for portrait editing. We train the network in two stages. We first employ an annealing identity loss to train an Identity Enhancement Network (IDE-Net), to ensure robust identity preservation. We then train the IPNet using a novel diffusion Multi-Objective Distillation approach that integrates adversarial loss, identity distillation loss, and a novel Facial-Style Enhancing loss. The Diffusion Multi-Objective Distillation approach efficiently reduces inference steps, ensures identity consistency, and enhances the precision of instruction-based editing. Extensive comparison with prior models demonstrates IPNet as a superior model in terms of identity preservation, text fidelity, and inference speed.
  abstract_embedding: [0.031494140625, 0.60546875, 0.039306640625]... (1536 items)
  authors: ['Zhixin Lai', 'Keqiang Sun', 'Fu-Yun Wang']... (5 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417728414
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: InstantPortrait__One-Step_Portrait_Editing_via_Diffusion_Multi-Objective_Distillation.pdf
  sha_abstract: a96a8d7ce5169f7c148e3963356c95e372419b327c7c667c395ace49f4ed295b
  title: InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation
  title_normalized: instantportrait_onestep_portrait_editing_via_diffusion_multiobjective_distillation

================================================================================
Document #79 (ID: oOhIWJoBclM7MZc3a5IK)
================================================================================
  abstract: Designing a safe policy for uncertain environments is crucial in real-world control systems. However, this challenge remains inadequately addressed within the Markov decision process (MDP) framework. This paper presents the first algorithm guaranteed to identify a near-optimal policy in a robust constrained MDP (RCMDP), where an optimal policy minimizes cumulative cost while satisfying constraints in the worst-case scenario across a set of environments. We first prove that the conventional policy gradient approach to the Lagrangian max-min formulation can become trapped in suboptimal solutions. This occurs when its inner minimization encounters a sum of conflicting gradients from the objective and constraint functions. To address this, we leverage the epigraph form of the RCMDP problem, which resolves the conflict by selecting a single gradient from either the objective or the constraints. Building on the epigraph form, we propose a bisection search algorithm with a policy gradient subroutine and prove that it identifies an $\varepsilon$-optimal policy in an RCMDP with $\widetilde{\mathcal{O}}(\varepsilon^{-4})$ robust policy evaluations.
  abstract_embedding: [0.9765625, 0.51953125, 0.1728515625]... (1536 items)
  authors: ['Toshinori Kitamura', 'Tadashi Kozuno', 'Wataru Kumagai']... (9 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417732322
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Near-Optimal_Policy_Identification_in_Robust_Constrained_Markov_Decision_Processes_via_Epigraph_Form.pdf
  sha_abstract: ab656bc49cbf15df676779a7d6e0fc62a27051b83a0676233c69972e7358057e
  title: Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form
  title_normalized: nearoptimal_policy_identification_in_robust_constrained_markov_decision_processes_via_epigraph_form

================================================================================
Document #80 (ID: n-hIWJoBclM7MZc3ZpIj)
================================================================================
  abstract: Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks.
  abstract_embedding: [1.2578125, 0.490234375, 0.259765625]... (1536 items)
  authors: ['Xinran Wang', 'Qi Le', 'Ammar Ahmed']... (8 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417731094
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: MAP__Multi-Human-Value_Alignment_Palette.pdf
  sha_abstract: a65c1b15145123c0b86f1dbd80ffb6423b7733a839681a30f91c9c5deaebdce8
  title: MAP: Multi-Human-Value Alignment Palette
  title_normalized: map_multihumanvalue_alignment_palette

================================================================================
Document #81 (ID: luhIWJoBclM7MZc3NpLu)
================================================================================
  abstract: We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.
  abstract_embedding: [0.50390625, 0.016357421875, 0.515625]... (1536 items)
  authors: ['Ilya Loshchilov', 'Cheng-Ping Hsieh', 'Simeng Sun']... (4 items)
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417719014
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: nGPT__Normalized_Transformer_with_Representation_Learning_on_the_Hypersphere.pdf
  sha_abstract: 9c456b374f8847aab6ebe1d4ef3e2027b4eee0cdf458dc29c93701a128c4c5d9
  title: nGPT: Normalized Transformer with Representation Learning on the Hypersphere
  title_normalized: ngpt_normalized_transformer_with_representation_learning_on_the_hypersphere

================================================================================
Document #82 (ID: nOhIWJoBclM7MZc3VJKK)
================================================================================
  abstract: We present a novel framework, StochastIc Network Graph Evolving operatoR (SINGER), for learning the evolution operator of high-dimensional partial differential equations (PDEs). The framework uses a sub-network to approximate the solution at the initial time step and stochastically evolves the sub-network parameters over time by a graph neural network to approximate the solution at later time steps. The framework is designed to inherit the desirable properties of the parametric solution operator, including graph topology, semigroup, and stability, with a theoretical guarantee. Numerical experiments on 8 evolution PDEs of 5,10,15,20-dimensions show that our method outperforms existing baselines in almost all cases (31 out of 32), and that our method generalizes well to unseen initial conditions, equation dimensions, sub-network width, and time steps.
  abstract_embedding: [0.291015625, 0.4453125, -0.212890625]... (1536 items)
  authors: ['Mingquan Feng', 'Yixin Huang', 'Weixin Liao']... (6 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:29:20.462359
  code_metadata_s3_key: nOhIWJoBclM7MZc3VJKK/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: nOhIWJoBclM7MZc3VJKK/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417726594
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: SINGER__Stochastic_Network_Graph_Evolving_Operator_for_High_Dimensional_PDEs.pdf
  sha_abstract: b04c6c4799ed42a5aae0b811473dfe01f0a9bac59403e97dd062a9b07ba0afa5
  title: SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs
  title_normalized: singer_stochastic_network_graph_evolving_operator_for_high_dimensional_pdes

================================================================================
Document #83 (ID: oehIWJoBclM7MZc3b5Jv)
================================================================================
  abstract: Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, 
generating time series of tabular data, where each element of the series depends on the others,
remains a largely unexplored domain. 
This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series.
In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. 
Using extensive experiments on six datasets, we show that the proposed approach  outperforms previous work by a large margin.
  abstract_embedding: [0.48046875, -0.037109375, 0.7421875]... (1536 items)
  authors: ['Fabrizio Garuti', 'Enver Sangineto', 'Simone Luetto']... (5 items)
  code_generated: True
  code_generated_at: 2025-11-06T08:29:31.412586
  code_metadata_s3_key: oehIWJoBclM7MZc3b5Jv/metadata.json
  code_s3_bucket: papers-code-artifacts
  code_s3_key: oehIWJoBclM7MZc3b5Jv/code.py
  date: 2024-10-04
  decision: accept
  ingested_at: 1762417733436
  novelty: yes
  reason: Auto-accepted for testing purposes only.
  relevance: yes
  s3_bucket: llm-research-papers
  s3_key: Diffusion_Transformers_for_Tabular_Data_Time_Series_Generation.pdf
  sha_abstract: c31a6a36017ee539d9da44cb5efb4bdad2194eefe1b74aee98440ede84fb62d4
  title: Diffusion Transformers for Tabular Data Time Series Generation
  title_normalized: diffusion_transformers_for_tabular_data_time_series_generation
